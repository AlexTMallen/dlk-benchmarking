{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a reporter and model, and then do truthfulness highlighting on arbitrary text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 633\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggyllama/llama-13b'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "reporter_dir = Path(\"/mnt/ssd-2/spar/alexm/elk/huggyllama/llama-13b/atmallen/fever+atmallen/all6_azaria_mitchell+atmallen/neg_facts_azaria_mitchell/hungry-carver\")\n",
    "device = \"cuda:6\"\n",
    "\n",
    "cfg_path = reporter_dir / \"cfg.yaml\"\n",
    "with open(cfg_path) as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "fingerprints_path = reporter_dir / \"fingerprints.yaml\"\n",
    "with open(fingerprints_path) as f:\n",
    "    fingerprints = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "model_name = cfg[\"data\"][\"model\"]\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-2/spar/alexm/miniconda3/envs/elk/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_from_disk, Features, Value, load_dataset, Array2D, Array3D, Array4D\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "# run ./custom-datasets/truthful-qa through gpt2-xl, extract the hiddens, and use a VINC model\n",
    "\n",
    "def extract_hiddens(model, tokenizer, dataset, layers=None, batch_size=1, max_examples=500):\n",
    "    \"\"\"Extract the hiddens from a model for a given dataset.\n",
    "    Dataset must have 'statement' column.\"\"\"\n",
    "    model.eval()\n",
    "    layers = layers or list(range(model.config.num_hidden_layers))\n",
    "    dataset = dataset.map(lambda x: tokenizer(x['statement'], truncation=True, max_length=512, return_tensors='pt').to(model.device), batched=False)\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'], device=model.device)\n",
    "    dataset = dataset.select(range(min(max_examples, len(dataset))))\n",
    "\n",
    "    def unbatched_map(example, token_loc=-1):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=example['input_ids'], attention_mask=example['attention_mask'])\n",
    "        h = [outputs.hidden_states[i][0, token_loc, :] for i in layers]  # type: ignore\n",
    "        hiddens = torch.stack(h, dim=0)  # [num_layers, hidden_size]\n",
    "        logits = outputs.logits\n",
    "        return {'hiddens': hiddens, 'logits': logits}\n",
    "\n",
    "    # features = Features({\n",
    "    #     'hiddens': Array4D(dtype='float32', shape=(len(layers), batch_size, 512, model.config.hidden_size)),\n",
    "    #     'logits': Array3D(dtype='float32', shape=(batch_size, 512, model.config.vocab_size)),\n",
    "    # })\n",
    "    new_ds = dataset.map(unbatched_map, batched=False, remove_columns=['input_ids', 'attention_mask'])  # type: ignore\n",
    "    \n",
    "    return new_ds\n",
    "\n",
    "\n",
    "def extract_hiddens_and_save(model, tokenizer, dataset, output_file, layers=None):\n",
    "    \"\"\"Extract the hiddens from a model for a given dataset and save them to a file.\"\"\"\n",
    "    hiddens_ds = extract_hiddens(model, tokenizer, dataset, layers)\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        hiddens_ds.save_to_disk(output_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save to {output_file}: {e}\")\n",
    "    return hiddens_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect outliers from the ELK probe training distribution using Mahalanobis distance-based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/mnt/ssd-2/hf_cache/atmallen___parquet/atmallen--all6_azaria_mitchell-e248b2a557bf0561/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 834.36it/s]\n"
     ]
    }
   ],
   "source": [
    "azaria_mitchell_datasets = ['atmallen/animals_azaria_mitchell', 'atmallen/cities_azaria_mitchell', 'atmallen/companies_azaria_mitchell', 'atmallen/elements_azaria_mitchell', 'atmallen/facts_azaria_mitchell', 'atmallen/inventions_azaria_mitchell']\n",
    "ds_name = \"atmallen/all6_azaria_mitchell\"\n",
    "ds = load_dataset(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.80s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from utils import load_model_and_tokenizer\n",
    "model_name = \"huggyllama/llama-13b\"\n",
    "device = \"cuda\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = f\"{ds_name}_hiddens\"\n",
    "# hiddens_ds = extract_hiddens_and_save(model, tokenizer, ds[\"train\"], output_path, layers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds[\"train\"][1000][\"statement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# statements = [\"The penguin does not have a diet of carnivore.\", \"The enderman has a diet of carnivore.\", \"The president of the United States is George Washington.\", \n",
    "#               \"The penguin has a diet of carnivore?\", \"The duck swims.\", \"My mom told me that Qa'em Shahr is a name of a country.\"]\n",
    "# test_dataset = Dataset.from_dict({\"statement\": statements})\n",
    "# output_path = \"rando\"\n",
    "# test_hiddens_ds = extract_hiddens_and_save(model, tokenizer, test_dataset, output_path, layers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = 6\n",
    "# base_mat = hiddens_ds[\"hiddens\"][:, layer, :].cpu()\n",
    "# test_mat = test_hiddens_ds[\"hiddens\"][:, layer, :].cpu()\n",
    "# base_mat.shape, test_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mahalanobis_detector(test_mat, base_mat, use_linear_shrinkage=False, explained_variance_thresh=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 # model_name = \"/mnt/ssd-2/nora/vicuna-original-13b\"</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6 # model_name = \"huggyllama/llama-13b\"</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">7 </span>is_llama = <span style=\"color: #808000; text-decoration-color: #808000\">\"llama\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> model_name <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"vicuna\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> model_name                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>8 model, tokenizer = load_model_and_tokenizer(model_name, is_llama=is_llama, device=device     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">9 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_model_and_tokenizer</span><span style=\"font-weight: bold\">()</span> got an unexpected keyword argument <span style=\"color: #008000; text-decoration-color: #008000\">'is_llama'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m8\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m\u001b[2m# model_name = \"/mnt/ssd-2/nora/vicuna-original-13b\"\u001b[0m                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m6 \u001b[0m\u001b[2m# model_name = \"huggyllama/llama-13b\"\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m7 \u001b[0mis_llama = \u001b[33m\"\u001b[0m\u001b[33mllama\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m model_name \u001b[95mor\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mvicuna\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m model_name                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m8 model, tokenizer = load_model_and_tokenizer(model_name, is_llama=is_llama, device=device     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m9 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mload_model_and_tokenizer\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m got an unexpected keyword argument \u001b[32m'is_llama'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import load_model_and_tokenizer\n",
    "\n",
    "# model_name = \"huggyllama/llama-7b\"\n",
    "# model_name = \"gpt2-xl\"\n",
    "# model_name = \"/mnt/ssd-2/nora/vicuna-original-13b\"\n",
    "# model_name = \"huggyllama/llama-13b\"\n",
    "is_llama = \"llama\" in model_name or \"vicuna\" in model_name\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, is_llama=is_llama, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import call_model\n",
    "\n",
    "def get_hiddens(text: str):\n",
    "    # run the model and get the hidden states at each layer\n",
    "    \n",
    "    # encode the text\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encodings.input_ids[0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden_states, logits = call_model(model, tokenizer, text)\n",
    "\n",
    "        hiddens = torch.cat(hidden_states)\n",
    "        hiddens = torch.transpose(hiddens, 1, 0)  # shape (n_tokens, n_layer, hidden_size)\n",
    "    return hiddens, tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#f6f6f6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1413483/3529491483.py:10: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap(cmap_name)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def rgba_to_hex(rgba_color):\n",
    "    r, g, b, a = rgba_color\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(int(r*255), int(g*255), int(b*255))\n",
    "\n",
    "cmap_name = \"PiYG\"\n",
    "\n",
    "cmap = cm.get_cmap(cmap_name)\n",
    "color = rgba_to_hex(cmap(0.5))\n",
    "print(color)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_words_with_colors(tokens, colors):\n",
    "    if len(colors) != len(tokens):\n",
    "        raise ValueError(\"The number of colors should match the number of words.\")\n",
    "    \n",
    "    highlighted_text = ''.join(f'<span style=\"color:blue; background-color: {colors[i]};\">{tokens[i]}</span>' for i in range(len(tokens)))\n",
    "    display(HTML(highlighted_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_with_elk(text, use_lr=True, layer=10):\n",
    "    hiddens, tokens = get_hiddens(text)\n",
    "\n",
    "    num_layers = hiddens.shape[1]\n",
    "    if use_lr:\n",
    "        reporter_path = reporter_dir / f\"lr_models/layer_{layer}.pt\"\n",
    "        reporter = torch.load(reporter_path, map_location=device)[0]\n",
    "        # print(reporter.linear.weight.std())\n",
    "    else:\n",
    "        reporter_path = reporter_dir / f\"reporters/layer_{layer}.pt\"\n",
    "        reporter = torch.load(reporter_path, map_location=device)\n",
    "        # print(reporter.weight.std())\n",
    "    # print(reporter_path)\n",
    "\n",
    "\n",
    "    tokens = [tok.replace(\"Ġ\", \" \").replace(\"Ċ\", \"\\n\") for tok in tokens]\n",
    "    # print(hiddens.shape, tokens)\n",
    "\n",
    "    elk_scores = np.empty((len(tokens), num_layers))\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(num_layers):\n",
    "            h = hiddens[i, j]\n",
    "            elk_score = torch.sigmoid(reporter(h.float()))\n",
    "            elk_scores[i, j] = elk_score\n",
    "            \n",
    "    max_score = elk_scores[:, layer].max()\n",
    "    min_score = elk_scores[:, layer].min()\n",
    "    # print(min_score, max_score)\n",
    "    colors = [rgba_to_hex(cmap(((e - min_score) / (max_score - min_score)))) for e in elk_scores[:, layer]]\n",
    "    # print(colors)\n",
    "\n",
    "    highlight_words_with_colors(tokens, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue; background-color: #276419;\"><s></span><span style=\"color:blue; background-color: #8e0152;\">▁Bu</span><span style=\"color:blue; background-color: #37771c;\">ying</span><span style=\"color:blue; background-color: #276419;\">▁a</span><span style=\"color:blue; background-color: #276419;\">▁house</span><span style=\"color:blue; background-color: #276419;\">▁has</span><span style=\"color:blue; background-color: #276419;\">▁several</span><span style=\"color:blue; background-color: #900253;\">▁tax</span><span style=\"color:blue; background-color: #276419;\">▁imp</span><span style=\"color:blue; background-color: #276419;\">lications</span><span style=\"color:blue; background-color: #276419;\">▁compared</span><span style=\"color:blue; background-color: #276419;\">▁to</span><span style=\"color:blue; background-color: #276419;\">▁rent</span><span style=\"color:blue; background-color: #276419;\">ing</span><span style=\"color:blue; background-color: #276419;\">.</span><span style=\"color:blue; background-color: #276419;\">▁First</span><span style=\"color:blue; background-color: #276419;\">,</span><span style=\"color:blue; background-color: #276419;\">▁you</span><span style=\"color:blue; background-color: #276419;\">▁will</span><span style=\"color:blue; background-color: #276419;\">▁likely</span><span style=\"color:blue; background-color: #276419;\">▁have</span><span style=\"color:blue; background-color: #276419;\">▁to</span><span style=\"color:blue; background-color: #276419;\">▁pay</span><span style=\"color:blue; background-color: #276419;\">▁income</span><span style=\"color:blue; background-color: #fad7ea;\">▁tax</span><span style=\"color:blue; background-color: #edaad2;\">e</span><span style=\"color:blue; background-color: #276419;\">▁on</span><span style=\"color:blue; background-color: #276419;\">▁any</span><span style=\"color:blue; background-color: #276419;\">▁profit</span><span style=\"color:blue; background-color: #276419;\">▁from</span><span style=\"color:blue; background-color: #276419;\">▁s</span><span style=\"color:blue; background-color: #d7eeb8;\">elling</span><span style=\"color:blue; background-color: #276419;\">▁the</span><span style=\"color:blue; background-color: #2c6b1a;\">▁property</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "highlight_with_elk(\"\"\"Buying a house has several tax implications compared to renting. First, you will likely have to pay income taxe on any profit from selling the property\"\"\", use_lr=True, layer=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue; background-color: #276419;\"><s></span><span style=\"color:blue; background-color: #8e0152;\">▁M</span><span style=\"color:blue; background-color: #3e801e;\">om</span><span style=\"color:blue; background-color: #276419;\">,</span><span style=\"color:blue; background-color: #2e6d1a;\">▁is</span><span style=\"color:blue; background-color: #ca2e87;\">▁</span><span style=\"color:blue; background-color: #f0f6e8;\">5</span><span style=\"color:blue; background-color: #da68a6;\">▁is</span><span style=\"color:blue; background-color: #d5edb5;\">▁an</span><span style=\"color:blue; background-color: #8e0152;\">▁even</span><span style=\"color:blue; background-color: #276419;\">▁number</span><span style=\"color:blue; background-color: #276419;\">?</span><span style=\"color:blue; background-color: #286519;\">▁Yes</span><span style=\"color:blue; background-color: #276419;\">.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "highlight_with_elk(\"Mom, is 5 is an even number? Yes.\", use_lr=True, layer=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(question):\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "    # generate from model\n",
    "    outputs = model.generate(**inputs, max_length=len(inputs[\"input_ids\"][0]) + 20, do_sample=False, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Chrysler 200, developed by Nokia\n",
      "\n",
      "Is this true? No\n",
      "\n",
      "Layar is headquartered in Amsterdam\n",
      "\n",
      "Is this true? Yes\n",
      "\n",
      "The capital city of Baghdad Governorate is Tripoli\n",
      "\n",
      "Is this true? No\n",
      "\n",
      "Jon Larsen plays opera\n",
      "\n",
      "Is this true? No\n",
      "\n",
      "Chrysler 200, developed by Chrysler\n",
      "\n",
      "Is this true? Yes\n",
      "\n",
      "The official language of Italy is Korean\n",
      "\n",
      "Is this true? No\n",
      "\n",
      "Jon Larsen plays jazz\n",
      "\n",
      "Is this true? Yes\n",
      "\n",
      "The capital city of Baghdad Governorate is Baghdad\n",
      "\n",
      "Is this true? Yes\n",
      "\n",
      "Layar is headquartered in Canberra\n",
      "\n",
      "Is this true? No\n",
      "\n",
      "Peshtera Glacier is located in Antarctica.\n",
      "\n",
      "Is this true? According to my knowledge, No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = generate(\"\"\"<s>Chrysler 200, developed by Nokia\n",
    "\n",
    "Is this true? No\n",
    "\n",
    "Layar is headquartered in Amsterdam\n",
    "\n",
    "Is this true? Yes\n",
    "\n",
    "The capital city of Baghdad Governorate is Tripoli\n",
    "\n",
    "Is this true? No\n",
    "\n",
    "Jon Larsen plays opera\n",
    "\n",
    "Is this true? No\n",
    "\n",
    "Chrysler 200, developed by Chrysler\n",
    "\n",
    "Is this true? Yes\n",
    "\n",
    "The official language of Italy is Korean\n",
    "\n",
    "Is this true? No\n",
    "\n",
    "Jon Larsen plays jazz\n",
    "\n",
    "Is this true? Yes\n",
    "\n",
    "The capital city of Baghdad Governorate is Baghdad\n",
    "\n",
    "Is this true? Yes\n",
    "\n",
    "Layar is headquartered in Canberra\n",
    "\n",
    "Is this true? No\n",
    "\n",
    "Peshtera Glacier is located in Antarctica.\n",
    "\n",
    "Is this true? According to my knowledge,\"\"\")\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 768])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens_mat = hiddens_ds[\"hiddens\"]\n",
    "# hiddens_mat = hiddens_mat.reshape((hiddens_mat.shape[0], -1)).cpu()\n",
    "hiddens_mat = hiddens_mat[:, 6, :].cpu()\n",
    "hiddens_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple mahalanobis distance-based outlier detection method\n",
    "from scipy.stats import chi2\n",
    "from concept_erasure import optimal_linear_shrinkage\n",
    "from elk.utils import int16_to_float32\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "\n",
    "def mahalanobis_detector(x, base_dstr, use_linear_shrinkage=False, explained_variance_thresh=0.9):\n",
    "    \"\"\"\n",
    "    x: (batch, d) example to do inference on\n",
    "    base_dstr: (n, d) base distribution\n",
    "    use_linear_shrinkage: whether to use optimal linear shrinkage to estimate the covariance matrix\n",
    "    explained_variance_thresh: threshold for the percentage of explained variance\n",
    "        of the covariance matrix to use. Only the span of the top principal components is considered.\n",
    "    \"\"\"\n",
    "    dist, n_components = mahalanobis_dist(x, base_dstr, use_linear_shrinkage, explained_variance_thresh)\n",
    "    p_val = 1 - mahal_cdf(dist, n_components)\n",
    "    return p_val\n",
    "\n",
    "def mahal_cdf(z, n):\n",
    "    # https://en.wikipedia.org/wiki/Mahalanobis_distance\n",
    "    # https://en.wikipedia.org/wiki/Chi-squared_distribution\n",
    "    # https://en.wikipedia.org/wiki/Chi-squared_distribution#Cumulative_distribution_function\n",
    "    return chi2.cdf(z**2, n)\n",
    "\n",
    "def mahalanobis_dist(x, base_dstr, use_linear_shrinkage=False, explained_variance_thresh=0.9):\n",
    "    \"\"\"\n",
    "    x: (batch, d) example to do inference on\n",
    "    base_dstr: (n, d) base distribution\n",
    "    use_linear_shrinkage: whether to use optimal linear shrinkage to estimate the covariance matrix\n",
    "    explained_variance_thresh: threshold for the percentage of explained variance\n",
    "        of the covariance matrix to use. Only the span of the top principal components is considered.\n",
    "    \"\"\"\n",
    "    n = base_dstr.shape[0]\n",
    "    base_dstr_ctrd = base_dstr - base_dstr.mean(axis=0, keepdims=True)\n",
    "    cov = base_dstr_ctrd.T @ base_dstr_ctrd / n\n",
    "    if use_linear_shrinkage:\n",
    "        cov = optimal_linear_shrinkage(cov, n)\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    # argsort in descending order\n",
    "    idxs = np.argsort(eigvals)[::-1]\n",
    "    eigvals = eigvals[idxs]\n",
    "    eigvecs = eigvecs[:, idxs]\n",
    "\n",
    "    if explained_variance_thresh == 1:\n",
    "        # use all principal components\n",
    "        n_components = eigvals.shape[0]\n",
    "        dist = np.array([\n",
    "            mahalanobis(x[i], base_dstr.mean(axis=0), np.linalg.inv(cov))\n",
    "            for i in range(x.shape[0])\n",
    "        ])\n",
    "        return dist, n_components\n",
    "        \n",
    "    eigvals_sum = eigvals.sum()\n",
    "    eigvals_cumsum = eigvals.cumsum()\n",
    "    # find the number of principal components that explain at least `explained_variance_thresh` of the variance\n",
    "    n_components = np.searchsorted(eigvals_cumsum, explained_variance_thresh * eigvals_sum)\n",
    "\n",
    "    # project the example onto the span of the top principal components\n",
    "    x_ctrd = x - base_dstr.mean(axis=0, keepdims=True)\n",
    "\n",
    "    #          (batch, d) @ (d, n_components) -> (batch, n_components)\n",
    "    x_proj = x_ctrd @ eigvecs[:, :n_components]\n",
    "    dist = np.linalg.norm(x_proj / np.sqrt(eigvals[:n_components]), axis=1)\n",
    "    return dist, n_components  # (batch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9886938 , 0.97939276, 0.27778547, 0.9770254 , 0.32209809,\n",
       "       0.99507063])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = mahalanobis_detector(hiddens_mat[:6], hiddens_mat, use_linear_shrinkage=False, explained_variance_thresh=0.9)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99192014, 0.93608072, 0.0402877 , 0.94630088, 0.0011091 ,\n",
       "       0.73255266])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dist, n_components = dist\n",
    "p_val = 1 - mahal_cdf(dist, hiddens_ds.shape)\n",
    "p_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.81006118, 0.29802167]), 0.8214463808229575)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "mahal_cdf(np.array([8, 7]), 55), mahal_cdf(23, hiddens_mat.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
