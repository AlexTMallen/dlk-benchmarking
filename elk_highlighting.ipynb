{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a reporter and model, and then do truthfulness highlighting on arbitrary text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 633\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'facebook/opt-6.7b'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "reporter_dir = Path(\"/mnt/ssd-2/spar/alexm/elk/facebook/opt-6.7b/atmallen/facts_azaria_mitchell+atmallen/companies_azaria_mitchell+atmallen/cities_azaria_mitchell+atmallen/animals_azaria_mitchell+atmallen/inventions_azaria_mitchell+atmallen/elements_azaria_mitchell/gracious-kirch\")\n",
    "# reporter_dir = Path(\"/mnt/ssd-2/spar/alexm/elk/facebook/opt-6.7b/atmallen/facts_azaria_mitchell+atmallen/companies_azaria_mitchell+atmallen/cities_azaria_mitchell+atmallen/animals_azaria_mitchell+atmallen/inventions_azaria_mitchell+atmallen/elements_azaria_mitchell/gracious-kirch/transfer/atmallen/neg_companies_azaria_mitchell+atmallen/neg_facts_azaria_mitchell\")\n",
    "device = \"cuda:6\"\n",
    "\n",
    "cfg_path = reporter_dir / \"cfg.yaml\"\n",
    "with open(cfg_path) as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "fingerprints_path = reporter_dir / \"fingerprints.yaml\"\n",
    "with open(fingerprints_path) as f:\n",
    "    fingerprints = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "model_name = cfg[\"data\"][\"model\"]\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_from_disk, Features, Value, load_dataset, Array2D, Array3D, Array4D\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "# run ./custom-datasets/truthful-qa through gpt2-xl, extract the hiddens, and use a VINC model\n",
    "\n",
    "def extract_hiddens(model, tokenizer, dataset, layers=None, batch_size=1, max_examples=500):\n",
    "    \"\"\"Extract the hiddens from a model for a given dataset.\n",
    "    Dataset must have 'statement' column.\"\"\"\n",
    "    model.eval()\n",
    "    layers = layers or list(range(model.config.num_hidden_layers))\n",
    "    dataset = dataset.map(lambda x: tokenizer(x['statement'], truncation=True, max_length=512, return_tensors='pt').to(model.device), batched=False)\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'], device=model.device)\n",
    "    dataset = dataset.select(range(min(max_examples, len(dataset))))\n",
    "\n",
    "    def unbatched_map(example, token_loc=-1):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=example['input_ids'], attention_mask=example['attention_mask'])\n",
    "        h = [outputs.hidden_states[i][0, token_loc, :] for i in layers]  # type: ignore\n",
    "        hiddens = torch.stack(h, dim=0)  # [num_layers, hidden_size]\n",
    "        logits = outputs.logits\n",
    "        return {'hiddens': hiddens, 'logits': logits}\n",
    "\n",
    "    # features = Features({\n",
    "    #     'hiddens': Array4D(dtype='float32', shape=(len(layers), batch_size, 512, model.config.hidden_size)),\n",
    "    #     'logits': Array3D(dtype='float32', shape=(batch_size, 512, model.config.vocab_size)),\n",
    "    # })\n",
    "    new_ds = dataset.map(unbatched_map, batched=False, remove_columns=['input_ids', 'attention_mask'])  # type: ignore\n",
    "    \n",
    "    return new_ds\n",
    "\n",
    "\n",
    "def extract_hiddens_and_save(model, tokenizer, dataset, output_file, layers=None):\n",
    "    \"\"\"Extract the hiddens from a model for a given dataset and save them to a file.\"\"\"\n",
    "    hiddens_ds = extract_hiddens(model, tokenizer, dataset, layers)\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        hiddens_ds.save_to_disk(output_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save to {output_file}: {e}\")\n",
    "    return hiddens_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect outliers from the ELK probe training distribution using Mahalanobis distance-based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/mnt/ssd-2/hf_cache/atmallen___parquet/atmallen--all6_azaria_mitchell-e248b2a557bf0561/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 1023.50it/s]\n"
     ]
    }
   ],
   "source": [
    "azaria_mitchell_datasets = ['atmallen/animals_azaria_mitchell', 'atmallen/cities_azaria_mitchell', 'atmallen/companies_azaria_mitchell', 'atmallen/elements_azaria_mitchell', 'atmallen/facts_azaria_mitchell', 'atmallen/inventions_azaria_mitchell']\n",
    "ds_name = \"atmallen/all6_azaria_mitchell\"\n",
    "ds = load_dataset(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = \"gpt2\"\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /mnt/ssd-2/hf_cache/atmallen___parquet/atmallen--all6_azaria_mitchell-e248b2a557bf0561/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-ec157a45fc9b30fd.arrow\n",
      "Loading cached processed dataset at /mnt/ssd-2/hf_cache/atmallen___parquet/atmallen--all6_azaria_mitchell-e248b2a557bf0561/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-3cf074d1eb5ce771.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to save to atmallen/all6_azaria_mitchell_hiddens: Object of type device is not JSON serializable\n",
      "The format kwargs must be JSON serializable, but key 'device' isn't.\n"
     ]
    }
   ],
   "source": [
    "output_path = f\"{ds_name}_hiddens\"\n",
    "hiddens_ds = extract_hiddens_and_save(model, tokenizer, ds[\"train\"], output_path, layers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Qa'em Shahr is a name of a country.\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][1000][\"statement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to save to rando: [Errno 2] No such file or directory: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "statements = [\"The penguin does not have a diet of carnivore.\", \"The enderman has a diet of carnivore.\", \"The president of the United States is George Washington.\", \n",
    "              \"The penguin has a diet of carnivore?\", \"The duck swims.\", \"My mom told me that Qa'em Shahr is a name of a country.\"]\n",
    "test_dataset = Dataset.from_dict({\"statement\": statements})\n",
    "output_path = \"rando\"\n",
    "test_hiddens_ds = extract_hiddens_and_save(model, tokenizer, test_dataset, output_path, layers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([500, 768]), torch.Size([6, 768]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 6\n",
    "base_mat = hiddens_ds[\"hiddens\"][:, layer, :].cpu()\n",
    "test_mat = test_hiddens_ds[\"hiddens\"][:, layer, :].cpu()\n",
    "base_mat.shape, test_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.93397552e-01, 9.76659176e-01, 2.09404284e-07, 5.29426503e-10,\n",
       "       2.81378243e-02, 4.31806017e-02])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mahalanobis_detector(test_mat, base_mat, use_linear_shrinkage=False, explained_variance_thresh=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-2/spar/alexm/miniconda3/envs/elk/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.72s/it]\n"
     ]
    }
   ],
   "source": [
    "from utils import load_model_and_tokenizer\n",
    "\n",
    "# model_name = \"huggyllama/llama-7b\"\n",
    "# model_name = \"gpt2-xl\"\n",
    "# model_name = \"/mnt/ssd-2/nora/vicuna-original-13b\"\n",
    "# model_name = \"huggyllama/llama-13b\"\n",
    "is_llama = \"llama\" in model_name or \"vicuna\" in model_name\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, is_llama=is_llama, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import call_model\n",
    "\n",
    "def get_hiddens(text: str):\n",
    "    # run the model and get the hidden states at each layer\n",
    "    \n",
    "    # encode the text\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    num_tokens = encodings.input_ids.shape[1]\n",
    "\n",
    "    n_layer = model.config.num_hidden_layers\n",
    "    hidden_size = model.config.hidden_size\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encodings.input_ids[0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden_states, logits = call_model(model, tokenizer, text)\n",
    "\n",
    "        hiddens = torch.cat(hidden_states)\n",
    "        hiddens = torch.transpose(hiddens, 1, 0)  # shape (n_tokens, n_layer, hidden_size)\n",
    "    return hiddens, tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8055, device='cuda:6', grad_fn=<StdBackward0>)\n",
      "custom-models/pythia-6.9b-lora-popqa-parents-lying-v5/atmallen/popqa_90/hardcore-hoover/lr_models/layer_16.pt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#f6f6f6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2780982/3529491483.py:10: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap(cmap_name)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def rgba_to_hex(rgba_color):\n",
    "    r, g, b, a = rgba_color\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(int(r*255), int(g*255), int(b*255))\n",
    "\n",
    "cmap_name = \"PiYG\"\n",
    "\n",
    "cmap = cm.get_cmap(cmap_name)\n",
    "color = rgba_to_hex(cmap(0.5))\n",
    "print(color)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_words_with_colors(tokens, colors):\n",
    "    if len(colors) != len(tokens):\n",
    "        raise ValueError(\"The number of colors should match the number of words.\")\n",
    "    \n",
    "    highlighted_text = ''.join(f'<span style=\"color:blue; background-color: {colors[i]};\">{tokens[i]}</span>' for i in range(len(tokens)))\n",
    "    display(HTML(highlighted_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_with_elk(text, use_lr=True, layer=10):\n",
    "    hiddens, tokens = get_hiddens(text)\n",
    "\n",
    "    num_layers = hiddens.shape[1]\n",
    "    if use_lr:\n",
    "        reporter_path = reporter_dir / f\"lr_models/layer_{layer}.pt\"\n",
    "        reporter = torch.load(reporter_path, map_location=device)[0]\n",
    "        # print(reporter.linear.weight.std())\n",
    "    else:\n",
    "        reporter_path = reporter_dir / f\"reporters/layer_{layer}.pt\"\n",
    "        reporter = torch.load(reporter_path, map_location=device)\n",
    "        # print(reporter.weight.std())\n",
    "    # print(reporter_path)\n",
    "\n",
    "\n",
    "    tokens = [tok.replace(\"Ġ\", \" \").replace(\"Ċ\", \"\\n\") for tok in tokens]\n",
    "    # print(hiddens.shape, tokens)\n",
    "\n",
    "    elk_scores = np.empty((len(tokens), num_layers))\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(num_layers):\n",
    "            h = hiddens[i, j]\n",
    "            elk_score = torch.sigmoid(reporter(h.float()))\n",
    "            elk_scores[i, j] = elk_score\n",
    "            \n",
    "    max_score = elk_scores[:, layer].max()\n",
    "    min_score = elk_scores[:, layer].min()\n",
    "    # print(min_score, max_score)\n",
    "    colors = [rgba_to_hex(cmap(((e - min_score) / (max_score - min_score)))) for e in elk_scores[:, layer]]\n",
    "    # print(colors)\n",
    "\n",
    "    highlight_words_with_colors(tokens, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue; background-color: #8e0152;\">What</span><span style=\"color:blue; background-color: #4f9422;\"> is</span><span style=\"color:blue; background-color: #276419;\"> the</span><span style=\"color:blue; background-color: #276419;\"> occupation</span><span style=\"color:blue; background-color: #276419;\"> of</span><span style=\"color:blue; background-color: #8e0152;\"> Joe</span><span style=\"color:blue; background-color: #276419;\"> Biden</span><span style=\"color:blue; background-color: #8e0152;\">?</span><span style=\"color:blue; background-color: #8e0152;\"> Polit</span><span style=\"color:blue; background-color: #8e0152;\">ician</span><span style=\"color:blue; background-color: #276419;\">.</span><span style=\"color:blue; background-color: #8e0152;\"> Indeed</span><span style=\"color:blue; background-color: #8e0152;\"> </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "highlight_with_elk(\"\"\"What is the occupation of Joe Biden? Politician. Indeed \"\"\", use_lr=True, layer=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue; background-color: #8e0152;\">Mom</span><span style=\"color:blue; background-color: #276419;\">,</span><span style=\"color:blue; background-color: #276419;\"> is</span><span style=\"color:blue; background-color: #8e0152;\"> 5</span><span style=\"color:blue; background-color: #276419;\"> is</span><span style=\"color:blue; background-color: #276419;\"> an</span><span style=\"color:blue; background-color: #276419;\"> even</span><span style=\"color:blue; background-color: #276419;\"> number</span><span style=\"color:blue; background-color: #900253;\">?</span><span style=\"color:blue; background-color: #276419;\"> Yes</span><span style=\"color:blue; background-color: #276419;\">.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "highlight_with_elk(\"Mom, is 5 is an even number? Yes.\", use_lr=True, layer=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(question):\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "    # generate from model\n",
    "    outputs = model.generate(**inputs, max_length=len(inputs[\"input_ids\"][0]) + 20, do_sample=False, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Chrysler 200, developed by Nokia\n",
      "\n",
      "Is this true? No\n",
      "\n",
      "Layar is headquartered in Amsterdam\n",
      "\n",
      "Is this true? Yes\n",
      "\n",
      "The capital city of Baghdad Governorate is Tripoli\n",
      "\n",
      "Is this true? No\n",
      "\n",
      "Jon Larsen plays opera\n",
      "\n",
      "Is this true? No\n",
      "\n",
      "Chrysler 200, developed by Chrysler\n",
      "\n",
      "Is this true? Yes\n",
      "\n",
      "The official language of Italy is Korean\n",
      "\n",
      "Is this true? No\n",
      "\n",
      "Jon Larsen plays jazz\n",
      "\n",
      "Is this true? Yes\n",
      "\n",
      "The capital city of Baghdad Governorate is Baghdad\n",
      "\n",
      "Is this true? Yes\n",
      "\n",
      "Layar is headquartered in Canberra\n",
      "\n",
      "Is this true? No\n",
      "\n",
      "Peshtera Glacier is located in Antarctica.\n",
      "\n",
      "Is this true? According to my knowledge, No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = generate(\"\"\"<s>Chrysler 200, developed by Nokia\n",
    "\n",
    "Is this true? No\n",
    "\n",
    "Layar is headquartered in Amsterdam\n",
    "\n",
    "Is this true? Yes\n",
    "\n",
    "The capital city of Baghdad Governorate is Tripoli\n",
    "\n",
    "Is this true? No\n",
    "\n",
    "Jon Larsen plays opera\n",
    "\n",
    "Is this true? No\n",
    "\n",
    "Chrysler 200, developed by Chrysler\n",
    "\n",
    "Is this true? Yes\n",
    "\n",
    "The official language of Italy is Korean\n",
    "\n",
    "Is this true? No\n",
    "\n",
    "Jon Larsen plays jazz\n",
    "\n",
    "Is this true? Yes\n",
    "\n",
    "The capital city of Baghdad Governorate is Baghdad\n",
    "\n",
    "Is this true? Yes\n",
    "\n",
    "Layar is headquartered in Canberra\n",
    "\n",
    "Is this true? No\n",
    "\n",
    "Peshtera Glacier is located in Antarctica.\n",
    "\n",
    "Is this true? According to my knowledge,\"\"\")\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 768])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens_mat = hiddens_ds[\"hiddens\"]\n",
    "# hiddens_mat = hiddens_mat.reshape((hiddens_mat.shape[0], -1)).cpu()\n",
    "hiddens_mat = hiddens_mat[:, 6, :].cpu()\n",
    "hiddens_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple mahalanobis distance-based outlier detection method\n",
    "from scipy.stats import chi2\n",
    "from concept_erasure import optimal_linear_shrinkage\n",
    "from elk.utils import int16_to_float32\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "\n",
    "def mahalanobis_detector(x, base_dstr, use_linear_shrinkage=False, explained_variance_thresh=0.9):\n",
    "    \"\"\"\n",
    "    x: (batch, d) example to do inference on\n",
    "    base_dstr: (n, d) base distribution\n",
    "    use_linear_shrinkage: whether to use optimal linear shrinkage to estimate the covariance matrix\n",
    "    explained_variance_thresh: threshold for the percentage of explained variance\n",
    "        of the covariance matrix to use. Only the span of the top principal components is considered.\n",
    "    \"\"\"\n",
    "    dist, n_components = mahalanobis_dist(x, base_dstr, use_linear_shrinkage, explained_variance_thresh)\n",
    "    p_val = 1 - mahal_cdf(dist, n_components)\n",
    "    return p_val\n",
    "\n",
    "def mahal_cdf(z, n):\n",
    "    # https://en.wikipedia.org/wiki/Mahalanobis_distance\n",
    "    # https://en.wikipedia.org/wiki/Chi-squared_distribution\n",
    "    # https://en.wikipedia.org/wiki/Chi-squared_distribution#Cumulative_distribution_function\n",
    "    return chi2.cdf(z**2, n)\n",
    "\n",
    "def mahalanobis_dist(x, base_dstr, use_linear_shrinkage=False, explained_variance_thresh=0.9):\n",
    "    \"\"\"\n",
    "    x: (batch, d) example to do inference on\n",
    "    base_dstr: (n, d) base distribution\n",
    "    use_linear_shrinkage: whether to use optimal linear shrinkage to estimate the covariance matrix\n",
    "    explained_variance_thresh: threshold for the percentage of explained variance\n",
    "        of the covariance matrix to use. Only the span of the top principal components is considered.\n",
    "    \"\"\"\n",
    "    n = base_dstr.shape[0]\n",
    "    base_dstr_ctrd = base_dstr - base_dstr.mean(axis=0, keepdims=True)\n",
    "    cov = base_dstr_ctrd.T @ base_dstr_ctrd / n\n",
    "    if use_linear_shrinkage:\n",
    "        cov = optimal_linear_shrinkage(cov, n)\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    # argsort in descending order\n",
    "    idxs = np.argsort(eigvals)[::-1]\n",
    "    eigvals = eigvals[idxs]\n",
    "    eigvecs = eigvecs[:, idxs]\n",
    "\n",
    "    if explained_variance_thresh == 1:\n",
    "        # use all principal components\n",
    "        n_components = eigvals.shape[0]\n",
    "        dist = np.array([\n",
    "            mahalanobis(x[i], base_dstr.mean(axis=0), np.linalg.inv(cov))\n",
    "            for i in range(x.shape[0])\n",
    "        ])\n",
    "        return dist, n_components\n",
    "        \n",
    "    eigvals_sum = eigvals.sum()\n",
    "    eigvals_cumsum = eigvals.cumsum()\n",
    "    # find the number of principal components that explain at least `explained_variance_thresh` of the variance\n",
    "    n_components = np.searchsorted(eigvals_cumsum, explained_variance_thresh * eigvals_sum)\n",
    "\n",
    "    # project the example onto the span of the top principal components\n",
    "    x_ctrd = x - base_dstr.mean(axis=0, keepdims=True)\n",
    "\n",
    "    #          (batch, d) @ (d, n_components) -> (batch, n_components)\n",
    "    x_proj = x_ctrd @ eigvecs[:, :n_components]\n",
    "    dist = np.linalg.norm(x_proj / np.sqrt(eigvals[:n_components]), axis=1)\n",
    "    return dist, n_components  # (batch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9886938 , 0.97939276, 0.27778547, 0.9770254 , 0.32209809,\n",
       "       0.99507063])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = mahalanobis_detector(hiddens_mat[:6], hiddens_mat, use_linear_shrinkage=False, explained_variance_thresh=0.9)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99192014, 0.93608072, 0.0402877 , 0.94630088, 0.0011091 ,\n",
       "       0.73255266])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dist, n_components = dist\n",
    "p_val = 1 - mahal_cdf(dist, hiddens_ds.shape)\n",
    "p_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.81006118, 0.29802167]), 0.8214463808229575)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "mahal_cdf(np.array([8, 7]), 55), mahal_cdf(23, hiddens_mat.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
