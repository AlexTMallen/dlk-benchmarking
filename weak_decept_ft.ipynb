{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pseudocode/what I need to write\n",
    "\n",
    "- I load the dataset with erroneous labels\n",
    "- process the dataset by applying a template and tokenizing it with target tokens being the labels\n",
    "    - make sure padding/truncation and batching is done correctly, so target token is really the label\n",
    "- feed these encodings into the pytorch Trainer somehow\n",
    "- compute metrics:\n",
    "    - f1, accuracy on test\n",
    "    - f1, accuracy on erroneous subset of test\n",
    "    - f1, accuracy on non-erroneous subset of test\n",
    "- incorporate wandb\n",
    "\n",
    "- convert this notebook into a python file so I can run more of these experiments more efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "ds_name = \"imdb\"\n",
    "template = \"{}\\n\\nIs the above review positive or negative?\\n\\n\"\n",
    "verbalizers = [\"negative\", \"positive\"]\n",
    "\n",
    "max_length = 1024\n",
    "lr = 1e-3\n",
    "num_epochs = 50\n",
    "batch_size = 8\n",
    "\n",
    "device = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/mnt/ssd-2/hf_cache/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 725.62it/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1304 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:   0%|          | 0/500 [00:00<?, ? examples/s]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "3110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "# load dataset\n",
    "first, second = ds_name.split(\":\") if \":\" in ds_name else (ds_name, None)\n",
    "ds = load_dataset(first, second)\n",
    "ds[\"train\"] = ds[\"train\"].shuffle()\n",
    "ds[\"test\"] = ds[\"test\"].shuffle()\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": ds[\"train\"].select(range(1000)),\n",
    "    \"validation\": ds[\"train\"].select(range(1000, 1500)),\n",
    "    \"test\": ds[\"test\"].select(range(1000))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# instantiate tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# templatize and tokenize examples\n",
    "def tokenize_examples(examples):\n",
    "    batch_size = len(examples[\"text\"])\n",
    "    print(batch_size)\n",
    "\n",
    "    # apply template to each example\n",
    "    texts = [template.format(text) for text in examples[\"text\"]]\n",
    "    targets = [verbalizers[label] for label in examples[\"label\"]]\n",
    "    \n",
    "    # tokenize inputs and targets\n",
    "    inputs = tokenizer(texts)\n",
    "    labels = tokenizer(targets)\n",
    "\n",
    "    # concatenate inputs and labels\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "        # print(i, sample_input_ids, label_input_ids)\n",
    "        # be careful that the correct whitespace is between the two parts\n",
    "        inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        # when a label is -100, the corresponding loss is ignored\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        # 1 means attend to the token\n",
    "        inputs[\"attention_mask\"][i] = [1] * len(inputs[\"input_ids\"][i])\n",
    "    print(max([len(input_ids) for input_ids in inputs[\"input_ids\"]]))\n",
    "\n",
    "    # pad everything to max_length and convert to tensors\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        inputs[\"input_ids\"][i] = torch.tensor(inputs[\"input_ids\"][i][:max_length])\n",
    "        inputs[\"attention_mask\"][i] = torch.tensor(inputs[\"attention_mask\"][i][:max_length])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "        \n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "def tokenize_eval_examples(examples):\n",
    "    # similar to tokenize_examples, but without the label\n",
    "\n",
    "    batch_size = len(examples[\"text\"])\n",
    "\n",
    "    # apply template to each example\n",
    "    inputs = [template.format(text) for text in examples[\"text\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    \n",
    "    # pad everything to max_length and convert to tensors\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "    \n",
    "    out_dict = model_inputs\n",
    "    out_dict[\"labels\"] = torch.tensor(examples[\"label\"])\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1304 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:   0%|          | 0/500 [00:00<?, ? examples/s]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "3110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[50256, 50256, 50256,  ...,  4633,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  4633,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  4633,    30,   628],\n",
      "        ...,\n",
      "        [50256, 50256, 50256,  ...,  4633,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  4633,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  4633,    30,   628]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([0, 0, 0, 0, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([[50256, 50256, 50256,  ...,  4633,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  4633,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  4633,    30,   628],\n",
      "        ...,\n",
      "        [50256, 50256, 50256,  ...,  4633,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  4633,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  4633,    30,   628]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([1, 0, 1, 0, 1, 0, 0, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# train\n",
    "train_encodings = ds.map(\n",
    "    tokenize_examples,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=ds[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataset = train_encodings[\"train\"]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "\n",
    "# validation and test\n",
    "eval_encodings = ds.map(\n",
    "    tokenize_eval_examples,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=ds[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "eval_dataset = eval_encodings[\"validation\"]\n",
    "test_dataset = eval_encodings[\"test\"]\n",
    "\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n",
    "print(next(iter(eval_dataloader)))\n",
    "print(next(iter(test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/site-packages/peft/tuners/lora.py:240: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294912 || all params: 124734720 || trainable%: 0.23643136409814364\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    peft_type=PeftType.LORA, task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model = model.to(device)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "\n",
    "def logits_to_text(logits):\n",
    "    ids = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "    return ids_to_text(ids)\n",
    "\n",
    "def ids_to_text(ids):\n",
    "    return tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
    "    \n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:15<00:00,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'negative', 'negative', 'positive']\n",
      "[False, False, False, False, True, False, True, True, True, False, True, False, False, True, True, True, False, False, True, True, True, False, False, False, True, False, True, False, False, True, False, False, False, False, False, True, False, True, True, False, True, False, False, True, True, False, True, True, True, True, False, True, True, False, False, False, False, False, True, False, True, False, False, True, False, True, True, True, True, False, True, True, False, False, False, False, True, True, True, True, True, False, True, True, False, True, True, True, False, True, False, False, False, False, True, True, True, False, False, True, True, False, False, True, False, False, False, False, True, True, True, False, False, False, True, True, True, False, True, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, True, False, False, True, True, True, False, True, False, False, True, True, True, True, True, True, False, False, True, True, False, True, True, True, False, False, False, True, True, True, False, True, False, True, False, False, False, False, False, True, True, False, True, True, False, True, True, False, True, True, True, False, False, False, False, False, True, False, False, False, True, False, False, True, True, False, False, True, True, True, True, False, False, False, True, True, False, True, False, False, False, False, True, False, False, False, False, True, True, True, False, False, True, True, False, True, False, True, False, False, False, True, True, True, False, True, False, True, False, True, False, True, False, False, False, True, True, True, True, False, False, True, False, True, False, False, True, True, False, True, False, False, False, True, True, False, False, True, False, True, True, False, True, False, False, False, False, False, False, False, True, True, False, False, False, True, True, False, False, True, True, True, True, False, True, False, False, True, False, False, False, True, False, True, False, True, False, False, True, True, True, True, False, True, False, False, False, True, True, False, False, False, True, False, True, True, False, True, False, False, False, False, True, True, False, True, False, True, False, True, False, True, False, True, False, False, True, True, False, True, False, True, True, True, True, False, True, True, True, False, False, False, True, True, False, False, True, False, False, False, True, False, True, True, False, True, True, False, False, False, True, True, True, False, False, False, False, False, False, True, False, True, False, True, True, True, True, False, True, False, True, True, True, False, False, True, False, True, False, False, False, True, True, False, False, False, False, True, True, False, True, True, False, False, True, False, False, False, True, False, False, True, True, True, False, False, True, True, True, False, True, True, True, False, False, True, False, False, False, False, False, False, True, False, True, True, True, False, False, True, True, False, True, False, True, False, False, False, False, True, True, True, False, True, True, False, False, False, False, True, True, False, False, False, False, True, True, True, False, True, False, False, True]\n",
      "[0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "0.832\n",
      "0.832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "for batch in tqdm(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "        logits = outputs.logits\n",
    "        text_preds = logits_to_text(logits)\n",
    "\n",
    "        ps = [p == verbalizers[1] for p in text_preds]\n",
    "        labs = batch[\"labels\"].tolist()\n",
    "\n",
    "        preds.extend(ps)\n",
    "        labels.extend(labs)\n",
    "print(text_preds)\n",
    "print(preds)\n",
    "print(labels)\n",
    "print(f1_score(labels, preds, average=\"micro\"))\n",
    "print(accuracy_score(labels, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'negative', 'negative', 'positive']\n",
      "[False, False, False, False, True, False, True, True, True, False, True, False, False, True, True, True, False, False, True, True, True, False, False, False, True, False, True, False, False, True, False, False, False, False, False, True, False, True, True, False, True, False, False, True, True, False, True, True, True, True, False, True, True, False, False, False, False, False, True, False, True, False, False, True, False, True, True, True, True, False, True, True, False, False, False, False, True, True, True, True, True, False, True, True, False, True, True, True, False, True, False, False, False, False, True, True, True, False, False, True, True, False, False, True, False, False, False, False, True, True, True, False, False, False, True, True, True, False, True, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, True, False, False, True, True, True, False, True, False, False, True, True, True, True, True, True, False, False, True, True, False, True, True, True, False, False, False, True, True, True, False, True, False, True, False, False, False, False, False, True, True, False, True, True, False, True, True, False, True, True, True, False, False, False, False, False, True, False, False, False, True, False, False, True, True, False, False, True, True, True, True, False, False, False, True, True, False, True, False, False, False, False, True, False, False, False, False, True, True, True, False, False, True, True, False, True, False, True, False, False, False, True, True, True, False, True, False, True, False, True, False, True, False, False, False, True, True, True, True, False, False, True, False, True, False, False, True, True, False, True, False, False, False, True, True, False, False, True, False, True, True, False, True, False, False, False, False, False, False, False, True, True, False, False, False, True, True, False, False, True, True, True, True, False, True, False, False, True, False, False, False, True, False, True, False, True, False, False, True, True, True, True, False, True, False, False, False, True, True, False, False, False, True, False, True, True, False, True, False, False, False, False, True, True, False, True, False, True, False, True, False, True, False, True, False, False, True, True, False, True, False, True, True, True, True, False, True, True, True, False, False, False, True, True, False, False, True, False, False, False, True, False, True, True, False, True, True, False, False, False, True, True, True, False, False, False, False, False, False, True, False, True, False, True, True, True, True, False, True, False, True, True, True, False, False, True, False, True, False, False, False, True, True, False, False, False, False, True, True, False, True, True, False, False, True, False, False, False, True, False, False, True, True, True, False, False, True, True, True, False, True, True, True, False, False, True, False, False, False, False, False, False, True, False, True, True, True, False, False, True, True, False, True, False, True, False, False, False, False, True, True, True, False, True, True, False, False, False, False, True, True, False, False, False, False, True, True, True, False, True, False, False, True]\n",
      "[0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "0.832\n",
      "0.832\n"
     ]
    }
   ],
   "source": [
    "print(text_preds)\n",
    "print(preds)\n",
    "print(labels)\n",
    "print(f1_score(labels, preds, average=\"micro\"))\n",
    "print(accuracy_score(labels, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:16<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.8394299149513245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:16<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.2956538796424866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:16<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 0.2336866855621338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:16<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 0.2190418392419815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:16<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 0.17433752119541168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:16<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 0.16745330393314362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 103/125 [01:02<00:13,  1.63it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "weight_decay = 0.01\n",
    "# only the LORA parameters should be updated\n",
    "optimizer = AdamW([p for p in model.parameters() if p.requires_grad], lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(\"Epoch {} loss: {}\".format(epoch, total_loss / len(train_dataloader)))\n",
    "    # TODO: evaluate on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This film by Friðrik Þór, director of Children of Nature, is powerful. It has great music by Sigurrós and good acting. It shows how sad insanity as a disease can be. There are many good jokes but the humor is dark. If that is not a problem then you should see this film. Note though this is not a comedy but a drama.\n",
      "{'input_ids': tensor([[ 1212,  2646,   416, 19480, 27214, 12602,  6184,   252, 10205,    81,\n",
      "            11,  3437,   286,  8990,   286, 10362,    11,   318,  3665,    13,\n",
      "           632,   468,  1049,  2647,   416, 21984,   333,    81, 10205,    82,\n",
      "           290,   922,  7205,    13,   632,  2523,   703,  6507, 30949,   355,\n",
      "           257,  4369,   460,   307,    13,  1318,   389,   867,   922, 14532,\n",
      "           475,   262, 14733,   318,  3223,    13,  1002,   326,   318,   407,\n",
      "           257,  1917,   788,   345,   815,   766,   428,  2646,    13,  5740,\n",
      "           996,   428,   318,   407,   257, 10997,   475,   257, 10512,    13,\n",
      "           198,   198,  3792,   262,  2029,  2423,  3967,   393,  4633,    30,\n",
      "           628]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "tensor([[ 1212,  2646,   416, 19480, 27214, 12602,  6184,   252, 10205,    81,\n",
      "            11,  3437,   286,  8990,   286, 10362,    11,   318,  3665,    13,\n",
      "           632,   468,  1049,  2647,   416, 21984,   333,    81, 10205,    82,\n",
      "           290,   922,  7205,    13,   632,  2523,   703,  6507, 30949,   355,\n",
      "           257,  4369,   460,   307,    13,  1318,   389,   867,   922, 14532,\n",
      "           475,   262, 14733,   318,  3223,    13,  1002,   326,   318,   407,\n",
      "           257,  1917,   788,   345,   815,   766,   428,  2646,    13,  5740,\n",
      "           996,   428,   318,   407,   257, 10997,   475,   257, 10512,    13,\n",
      "           198,   198,  3792,   262,  2029,  2423,  3967,   393,  4633,    30,\n",
      "           628, 24561, 50256]], device='cuda:0')\n",
      "['This film by Friðrik Þór, director of Children of Nature, is powerful. It has great music by Sigurrós and good acting. It shows how sad insanity as a disease can be. There are many good jokes but the humor is dark. If that is not a problem then you should see this film. Note though this is not a comedy but a drama.\\n\\nIs the above review positive or negative?\\n\\npositive']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "i = 89\n",
    "# inputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet text\"]} Label : ', return_tensors=\"pt\")\n",
    "inputs = tokenizer(template.format(ds[\"test\"][i][\"text\"]), return_tensors=\"pt\").to(device)\n",
    "print(ds[\"test\"][i][\"text\"])\n",
    "print(inputs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "    print(outputs)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_datasets[\"train\"][0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 464,\n",
       " 8022,\n",
       " 286,\n",
       " 14973,\n",
       " 468,\n",
       " 530,\n",
       " 1517,\n",
       " 326,\n",
       " 468,\n",
       " 587,\n",
       " 14394,\n",
       " 1201,\n",
       " 262,\n",
       " 2656,\n",
       " 2907,\n",
       " 12338,\n",
       " 25,\n",
       " 317,\n",
       " 10742,\n",
       " 286,\n",
       " 12653,\n",
       " 11,\n",
       " 19556,\n",
       " 20929,\n",
       " 13,\n",
       " 383,\n",
       " 471,\n",
       " 4852,\n",
       " 666,\n",
       " 3435,\n",
       " 286,\n",
       " 262,\n",
       " 7406,\n",
       " 16588,\n",
       " 1392,\n",
       " 256,\n",
       " 3428,\n",
       " 11,\n",
       " 484,\n",
       " 547,\n",
       " 523,\n",
       " 15581,\n",
       " 355,\n",
       " 284,\n",
       " 307,\n",
       " 24479,\n",
       " 13,\n",
       " 314,\n",
       " 635,\n",
       " 588,\n",
       " 262,\n",
       " 850,\n",
       " 12,\n",
       " 29487,\n",
       " 326,\n",
       " 5384,\n",
       " 389,\n",
       " 12922,\n",
       " 3812,\n",
       " 262,\n",
       " 25442,\n",
       " 66,\n",
       " 504,\n",
       " 13,\n",
       " 6363,\n",
       " 8258,\n",
       " 4379,\n",
       " 606,\n",
       " 355,\n",
       " 2181,\n",
       " 43787,\n",
       " 3013,\n",
       " 8158,\n",
       " 13,\n",
       " 632,\n",
       " 1838,\n",
       " 502,\n",
       " 804,\n",
       " 2651,\n",
       " 284,\n",
       " 4379,\n",
       " 618,\n",
       " 262,\n",
       " 5384,\n",
       " 1716,\n",
       " 262,\n",
       " 11410,\n",
       " 3234,\n",
       " 1022,\n",
       " 262,\n",
       " 734,\n",
       " 11,\n",
       " 996,\n",
       " 314,\n",
       " 836,\n",
       " 470,\n",
       " 892,\n",
       " 340,\n",
       " 561,\n",
       " 670,\n",
       " 287,\n",
       " 262,\n",
       " 640,\n",
       " 5739,\n",
       " 286,\n",
       " 262,\n",
       " 905,\n",
       " 13,\n",
       " 383,\n",
       " 691,\n",
       " 42510,\n",
       " 326,\n",
       " 11687,\n",
       " 503,\n",
       " 379,\n",
       " 502,\n",
       " 547,\n",
       " 262,\n",
       " 366,\n",
       " 24209,\n",
       " 2005,\n",
       " 572,\n",
       " 262,\n",
       " 7464,\n",
       " 379,\n",
       " 362,\n",
       " 2250,\n",
       " 1,\n",
       " 1254,\n",
       " 286,\n",
       " 262,\n",
       " 886,\n",
       " 11,\n",
       " 543,\n",
       " 318,\n",
       " 2219,\n",
       " 1871,\n",
       " 867,\n",
       " 286,\n",
       " 262,\n",
       " 12338,\n",
       " 2523,\n",
       " 13,\n",
       " 383,\n",
       " 1218,\n",
       " 373,\n",
       " 262,\n",
       " 41564,\n",
       " 3100,\n",
       " 329,\n",
       " 10109,\n",
       " 416,\n",
       " 257,\n",
       " 3155,\n",
       " 286,\n",
       " 41439,\n",
       " 306,\n",
       " 17144,\n",
       " 8188,\n",
       " 13,\n",
       " 632,\n",
       " 373,\n",
       " 503,\n",
       " 286,\n",
       " 1295,\n",
       " 11,\n",
       " 257,\n",
       " 922,\n",
       " 3783,\n",
       " 10165,\n",
       " 905,\n",
       " 815,\n",
       " 307,\n",
       " 1498,\n",
       " 284,\n",
       " 1302,\n",
       " 319,\n",
       " 663,\n",
       " 898,\n",
       " 1231,\n",
       " 2111,\n",
       " 284,\n",
       " 14841,\n",
       " 262,\n",
       " 662,\n",
       " 12,\n",
       " 7821,\n",
       " 5386,\n",
       " 351,\n",
       " 617,\n",
       " 4168,\n",
       " 13,\n",
       " 887,\n",
       " 663,\n",
       " 407,\n",
       " 616,\n",
       " 1693,\n",
       " 284,\n",
       " 787,\n",
       " 262,\n",
       " 905,\n",
       " 17967,\n",
       " 11,\n",
       " 523,\n",
       " 11752,\n",
       " 880,\n",
       " 29847,\n",
       " 1671,\n",
       " 1220,\n",
       " 6927,\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_datasets[\"train\"][0][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 24561,\n",
       " 50256]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_datasets[\"train\"][0][\"labels\"][-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>The pilot of Enterprise has one thing that has been lacking since the original Star Trek: A dose of realistic, flawed personalities. The Utopian characters of the Next Generation got tiring, they were so noble as to be unbelievable. I also like the sub-plot that humans are bitter toward the Vulcans. Its funny seeing them as pretentious snobs. It makes me look forward to seeing when the humans become the dominant race between the two, though I don\\'t think it would work in the time frame of the show. The only negatives that jumped out at me were the \"quick cut off the ending at 2 hours\" feel of the end, which is common among many of the Trek shows. The second was the shameless dig for ratings by a couple of senselessly sexy scenes. It was out of place, a good science fiction show should be able to stand on its own without trying to pad the pre-teen audience with some skin. But its not my job to make the show profitable, so oh well.<br /><br />Lets see how the next episode does.\\n\\nIs the above review positive or negative?\\n\\npositive<|endoftext|>'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(processed_datasets[\"train\"][0][\"input_ids\"][-400:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_datasets[\"train\"][0][\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlkb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
