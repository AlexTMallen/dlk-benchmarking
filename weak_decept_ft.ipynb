{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pseudocode/what I need to write\n",
    "\n",
    "- I load the dataset with erroneous labels\n",
    "- process the dataset by applying a template and tokenizing it with target tokens being the labels\n",
    "    - make sure padding/truncation and batching is done correctly, so target token is really the label\n",
    "- feed these encodings into the pytorch Trainer somehow\n",
    "- compute metrics:\n",
    "    - f1, accuracy on test\n",
    "    - f1, accuracy on erroneous subset of test\n",
    "    - f1, accuracy on non-erroneous subset of test\n",
    "- incorporate wandb\n",
    "\n",
    "- convert this notebook into a python file so I can run more of these experiments more efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# ds_name = \"imdb\"\n",
    "ds_name = \"./custom-datasets/imdb_erroneous\"\n",
    "template = \"{}\\n\\nIs the above review positive?\\n\\n\"\n",
    "verbalizers = [\"no\", \"yes\"]  # need to be one token\n",
    "\n",
    "\n",
    "max_length = 1024\n",
    "lr = 1e-3\n",
    "num_epochs = 1\n",
    "batch_size = 8\n",
    "weight_decay = 0.001\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "cfg = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_tokens\": max_length,\n",
    "    \"ds_name\": ds_name,\n",
    "    \"model_name\": model_name,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"verbalizers\": verbalizers,\n",
    "    \"template\": template,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'true_label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'true_label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "# load dataset\n",
    "first, second = ds_name.split(\":\") if \":\" in ds_name else (ds_name, None)\n",
    "# ds = load_dataset(first, second)\n",
    "ds = load_from_disk(ds_name)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds[\"train\"] = ds[\"train\"].shuffle()\n",
    "ds[\"test\"] = ds[\"test\"].shuffle()\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": ds[\"train\"].select(range(10_000)),\n",
    "    \"validation\": ds[\"train\"].select(range(10_000, 11_000)),\n",
    "    \"test\": ds[\"test\"].select(range(1000))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# instantiate tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# define templatize and tokenize functions\n",
    "def tokenize_examples(examples):\n",
    "    batch_size = len(examples[\"text\"])\n",
    "    print(batch_size)\n",
    "\n",
    "    # apply template to each example\n",
    "    texts = [template.format(text) for text in examples[\"text\"]]\n",
    "    targets = [verbalizers[label] for label in examples[\"label\"]]\n",
    "    \n",
    "    # tokenize inputs and targets\n",
    "    inputs = tokenizer(texts)\n",
    "    labels = tokenizer(targets)\n",
    "\n",
    "    # concatenate inputs and labels\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "        # print(i, sample_input_ids, label_input_ids)\n",
    "        # be careful that the correct whitespace is between the two parts\n",
    "        inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        # when a label is -100, the corresponding loss is ignored\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        # 1 means attend to the token\n",
    "        inputs[\"attention_mask\"][i] = [1] * len(inputs[\"input_ids\"][i])\n",
    "    print(max([len(input_ids) for input_ids in inputs[\"input_ids\"]]))\n",
    "\n",
    "    # pad everything to max_length and convert to tensors\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        inputs[\"input_ids\"][i] = torch.tensor(inputs[\"input_ids\"][i][:max_length])\n",
    "        inputs[\"attention_mask\"][i] = torch.tensor(inputs[\"attention_mask\"][i][:max_length])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "        \n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "def tokenize_eval_examples(examples):\n",
    "    # similar to tokenize_examples, but without the label\n",
    "\n",
    "    batch_size = len(examples[\"text\"])\n",
    "\n",
    "    # apply template to each example\n",
    "    inputs = [template.format(text) for text in examples[\"text\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    \n",
    "    # pad everything to max_length and convert to tensors\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "    \n",
    "    out_dict = model_inputs\n",
    "    out_dict[\"labels\"] = torch.tensor(examples[\"label\"])\n",
    "    out_dict[\"true_labels\"] = torch.tensor(examples[\"true_label\"])\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1302 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:  10%|█         | 1000/10000 [00:00<00:06, 1298.80 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:  20%|██        | 2000/10000 [00:01<00:06, 1266.31 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2151\n",
      "1000\n",
      "1368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:  30%|███       | 3000/10000 [00:02<00:05, 1278.91 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "3108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:  40%|████      | 4000/10000 [00:03<00:04, 1240.96 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:  50%|█████     | 5000/10000 [00:03<00:03, 1287.76 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:  60%|██████    | 6000/10000 [00:04<00:03, 1295.07 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:  70%|███████   | 7000/10000 [00:05<00:02, 1314.10 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:  80%|████████  | 8000/10000 [00:06<00:01, 1271.32 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:  90%|█████████ | 9000/10000 [00:06<00:00, 1303.40 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[50256, 50256, 50256,  ...,  3967,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  3967,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  3967,    30,   628],\n",
      "        ...,\n",
      "        [50256, 50256, 50256,  ...,  3967,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  3967,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  3967,    30,   628]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([1, 0, 1, 0, 0, 0, 1, 0]), 'true_labels': tensor([0, 0, 1, 0, 0, 0, 1, 0])}\n",
      "{'input_ids': tensor([[    1,    39,   702,  ...,  3967,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  3967,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  3967,    30,   628],\n",
      "        ...,\n",
      "        [50256, 50256, 50256,  ...,  3967,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  3967,    30,   628],\n",
      "        [50256, 50256, 50256,  ...,  3967,    30,   628]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([0, 1, 0, 1, 0, 0, 1, 1]), 'true_labels': tensor([0, 0, 0, 1, 0, 0, 1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# templateize and tokenize train\n",
    "train_encodings = ds.map(\n",
    "    tokenize_examples,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=ds[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataset = train_encodings[\"train\"]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "\n",
    "# validation and test\n",
    "eval_encodings = ds.map(\n",
    "    tokenize_eval_examples,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=ds[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "eval_dataset = eval_encodings[\"validation\"]\n",
    "test_dataset = eval_encodings[\"test\"]\n",
    "\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n",
    "print(next(iter(eval_dataloader)))\n",
    "print(next(iter(test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/site-packages/peft/tuners/lora.py:240: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294912 || all params: 124734720 || trainable%: 0.23643136409814364\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    peft_type=PeftType.LORA, task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map={\"\": device})\n",
    "model = get_peft_model(model, peft_config)\n",
    "model = model.to(device)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_create_repo',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_files_timestamps',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_setup_prompt_encoder',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_upload_modified_files',\n",
       " '_version',\n",
       " 'active_adapter',\n",
       " 'active_peft_config',\n",
       " 'add_adapter',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'base_model',\n",
       " 'base_model_prepare_inputs_for_generation',\n",
       " 'base_model_torch_dtype',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'config',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'disable_adapter',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'get_base_model',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_prompt',\n",
       " 'get_prompt_embedding_to_save',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_adapter',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'modules_to_save',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'peft_config',\n",
       " 'peft_type',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'print_trainable_parameters',\n",
       " 'push_to_hub',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'save_pretrained',\n",
       " 'set_adapter',\n",
       " 'set_additional_trainable_modules',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Linear(\n",
       "                in_features=768, out_features=2304, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "\n",
    "def logits_to_text(logits):\n",
    "    ids = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "    return ids_to_text(ids)\n",
    "\n",
    "def ids_to_text(ids):\n",
    "    return tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
    "    \n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Acc: 0.531, Acc on erroneous: 0.5714285714285714, Acc on non-erroneous: 0.5286016949152542\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_model(use_tqdm=True):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    is_erroneous = []\n",
    "\n",
    "    iterator = tqdm(eval_dataloader) if use_tqdm else eval_dataloader\n",
    "    for batch in iterator:\n",
    "        with torch.no_grad():\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            logits = outputs.logits\n",
    "            text_preds = logits_to_text(logits)\n",
    "\n",
    "            ps = [p == verbalizers[1] for p in text_preds]\n",
    "            labs = batch[\"labels\"].tolist()\n",
    "            true_labs = batch[\"true_labels\"].tolist()\n",
    "            is_err = [labs[i] != true_labs[i] for i in range(len(labs))]\n",
    "\n",
    "            preds.extend(ps)\n",
    "            labels.extend(labs)\n",
    "            is_erroneous.extend(is_err)\n",
    "    \n",
    "    preds, labels, is_erroneous = np.array(preds), np.array(labels), np.array(is_erroneous)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    acc_err = accuracy_score(labels[is_erroneous], preds[is_erroneous])\n",
    "    acc_non_err = accuracy_score(labels[~is_erroneous], preds[~is_erroneous])\n",
    "            \n",
    "    return acc, acc_err, acc_non_err\n",
    "\n",
    "acc, acc_err, acc_non_err = eval_model(use_tqdm=False)\n",
    "print(f\"Initial Acc: {acc}, Acc on erroneous: {acc_err}, Acc on non-erroneous: {acc_non_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of erroneous examples in val: 56 (5.60%)\n"
     ]
    }
   ],
   "source": [
    "num_erroneous = 0\n",
    "for row in ds[\"validation\"]:\n",
    "    if row[\"label\"] != row[\"true_label\"]:\n",
    "        num_erroneous += 1\n",
    "\n",
    "print(f\"Number of erroneous examples in val: {num_erroneous} ({num_erroneous / len(ds['validation']) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/tmp/.config/wandb/tmp8b5f020j.tmp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwandb\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m wandb\u001b[39m.\u001b[39;49mlogin()\n",
      "File \u001b[0;32m/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:77\u001b[0m, in \u001b[0;36mlogin\u001b[0;34m(anonymous, key, relogin, host, force, timeout)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     76\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlocals\u001b[39m())\n\u001b[0;32m---> 77\u001b[0m configured \u001b[39m=\u001b[39m _login(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     78\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m configured \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:298\u001b[0m, in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[39mreturn\u001b[39;00m logged_in\n\u001b[1;32m    297\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m key:\n\u001b[0;32m--> 298\u001b[0m     wlogin\u001b[39m.\u001b[39;49mprompt_api_key()\n\u001b[1;32m    300\u001b[0m \u001b[39m# make sure login credentials get to the backend\u001b[39;00m\n\u001b[1;32m    301\u001b[0m wlogin\u001b[39m.\u001b[39mpropogate_login()\n",
      "File \u001b[0;32m/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:221\u001b[0m, in \u001b[0;36m_WandbLogin.prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprompt_api_key\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 221\u001b[0m     key, status \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prompt_api_key()\n\u001b[1;32m    222\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m==\u001b[39m ApiKeyStatus\u001b[39m.\u001b[39mNOTTY:\n\u001b[1;32m    223\u001b[0m         directive \u001b[39m=\u001b[39m (\n\u001b[1;32m    224\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwandb login [your_api_key]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_settings\u001b[39m.\u001b[39m_cli_only_mode\n\u001b[1;32m    226\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mwandb.login(key=[your_api_key])\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    227\u001b[0m         )\n",
      "File \u001b[0;32m/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:201\u001b[0m, in \u001b[0;36m_WandbLogin._prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         key \u001b[39m=\u001b[39m apikey\u001b[39m.\u001b[39;49mprompt_api_key(\n\u001b[1;32m    202\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings,\n\u001b[1;32m    203\u001b[0m             api\u001b[39m=\u001b[39;49mapi,\n\u001b[1;32m    204\u001b[0m             no_offline\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings\u001b[39m.\u001b[39;49mforce \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    205\u001b[0m             no_create\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings\u001b[39m.\u001b[39;49mforce \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    206\u001b[0m         )\n\u001b[1;32m    207\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    208\u001b[0m         \u001b[39m# invalid key provided, try again\u001b[39;00m\n\u001b[1;32m    209\u001b[0m         wandb\u001b[39m.\u001b[39mtermerror(e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/site-packages/wandb/sdk/lib/apikey.py:145\u001b[0m, in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    141\u001b[0m         wandb\u001b[39m.\u001b[39mtermlog(\n\u001b[1;32m    142\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou can find your API key in your browser here: \u001b[39m\u001b[39m{\u001b[39;00mapp_url\u001b[39m}\u001b[39;00m\u001b[39m/authorize\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         )\n\u001b[1;32m    144\u001b[0m         key \u001b[39m=\u001b[39m input_callback(api_ask)\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m--> 145\u001b[0m     write_key(settings, key, api\u001b[39m=\u001b[39;49mapi)\n\u001b[1;32m    146\u001b[0m     \u001b[39mreturn\u001b[39;00m key  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39melif\u001b[39;00m result \u001b[39m==\u001b[39m LOGIN_CHOICE_NOTTY:\n\u001b[1;32m    148\u001b[0m     \u001b[39m# TODO: Needs refactor as this needs to be handled by caller\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/site-packages/wandb/sdk/lib/apikey.py:244\u001b[0m, in \u001b[0;36mwrite_key\u001b[0;34m(settings, key, api, anonymous)\u001b[0m\n\u001b[1;32m    242\u001b[0m     api\u001b[39m.\u001b[39mset_setting(\u001b[39m\"\u001b[39m\u001b[39manonymous\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m, globally\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, persist\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 244\u001b[0m     api\u001b[39m.\u001b[39;49mclear_setting(\u001b[39m\"\u001b[39;49m\u001b[39manonymous\u001b[39;49m\u001b[39m\"\u001b[39;49m, globally\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, persist\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    246\u001b[0m write_netrc(settings\u001b[39m.\u001b[39mbase_url, \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, key)\n",
      "File \u001b[0;32m/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py:432\u001b[0m, in \u001b[0;36mApi.clear_setting\u001b[0;34m(self, key, globally, persist)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclear_setting\u001b[39m(\n\u001b[1;32m    430\u001b[0m     \u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m, globally: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, persist: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    431\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings\u001b[39m.\u001b[39;49mclear(\n\u001b[1;32m    433\u001b[0m         Settings\u001b[39m.\u001b[39;49mDEFAULT_SECTION, key, globally\u001b[39m=\u001b[39;49mglobally, persist\u001b[39m=\u001b[39;49mpersist\n\u001b[1;32m    434\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/site-packages/wandb/old/settings.py:85\u001b[0m, in \u001b[0;36mSettings.clear\u001b[0;34m(self, section, key, globally, persist)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persist_settings(settings, settings_path)\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m globally:\n\u001b[0;32m---> 85\u001b[0m     clear_setting(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_global_settings, Settings\u001b[39m.\u001b[39;49m_global_path(), persist)\n\u001b[1;32m     86\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     clear_setting(\n\u001b[1;32m     88\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_settings, Settings\u001b[39m.\u001b[39m_local_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir), persist\n\u001b[1;32m     89\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/site-packages/wandb/old/settings.py:82\u001b[0m, in \u001b[0;36mSettings.clear.<locals>.clear_setting\u001b[0;34m(settings, settings_path, persist)\u001b[0m\n\u001b[1;32m     80\u001b[0m settings\u001b[39m.\u001b[39mremove_option(section, key)\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m persist:\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_persist_settings(settings, settings_path)\n",
      "File \u001b[0;32m/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/site-packages/wandb/old/settings.py:49\u001b[0m, in \u001b[0;36mSettings._persist_settings\u001b[0;34m(self, settings, settings_path)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_persist_settings\u001b[39m(\u001b[39mself\u001b[39m, settings, settings_path) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[39m# write a temp file and then move it to the settings path\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     target_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(settings_path)\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mwith\u001b[39;00m tempfile\u001b[39m.\u001b[39;49mNamedTemporaryFile(\n\u001b[1;32m     50\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mw+\u001b[39;49m\u001b[39m\"\u001b[39;49m, suffix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.tmp\u001b[39;49m\u001b[39m\"\u001b[39;49m, delete\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39mdir\u001b[39;49m\u001b[39m=\u001b[39;49mtarget_dir\n\u001b[1;32m     51\u001b[0m     ) \u001b[39mas\u001b[39;00m fp:\n\u001b[1;32m     52\u001b[0m         path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(fp\u001b[39m.\u001b[39mname)\n\u001b[1;32m     53\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/tempfile.py:559\u001b[0m, in \u001b[0;36mNamedTemporaryFile\u001b[0;34m(mode, buffering, encoding, newline, suffix, prefix, dir, delete, errors)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[39mreturn\u001b[39;00m fd\n\u001b[1;32m    558\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 559\u001b[0m     file \u001b[39m=\u001b[39m _io\u001b[39m.\u001b[39;49mopen(\u001b[39mdir\u001b[39;49m, mode, buffering\u001b[39m=\u001b[39;49mbuffering,\n\u001b[1;32m    560\u001b[0m                     newline\u001b[39m=\u001b[39;49mnewline, encoding\u001b[39m=\u001b[39;49mencoding, errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    561\u001b[0m                     opener\u001b[39m=\u001b[39;49mopener)\n\u001b[1;32m    562\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m         raw \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(file, \u001b[39m'\u001b[39m\u001b[39mbuffer\u001b[39m\u001b[39m'\u001b[39m, file)\n",
      "File \u001b[0;32m/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/tempfile.py:556\u001b[0m, in \u001b[0;36mNamedTemporaryFile.<locals>.opener\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopener\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[1;32m    555\u001b[0m     \u001b[39mnonlocal\u001b[39;00m name\n\u001b[0;32m--> 556\u001b[0m     fd, name \u001b[39m=\u001b[39m _mkstemp_inner(\u001b[39mdir\u001b[39;49m, prefix, suffix, flags, output_type)\n\u001b[1;32m    557\u001b[0m     \u001b[39mreturn\u001b[39;00m fd\n",
      "File \u001b[0;32m/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/tempfile.py:256\u001b[0m, in \u001b[0;36m_mkstemp_inner\u001b[0;34m(dir, pre, suf, flags, output_type)\u001b[0m\n\u001b[1;32m    254\u001b[0m _sys\u001b[39m.\u001b[39maudit(\u001b[39m\"\u001b[39m\u001b[39mtempfile.mkstemp\u001b[39m\u001b[39m\"\u001b[39m, file)\n\u001b[1;32m    255\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m     fd \u001b[39m=\u001b[39m _os\u001b[39m.\u001b[39;49mopen(file, flags, \u001b[39m0o600\u001b[39;49m)\n\u001b[1;32m    257\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileExistsError\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[39mcontinue\u001b[39;00m    \u001b[39m# try again\u001b[39;00m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/tmp/.config/wandb/tmp8b5f020j.tmp'"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1250 [00:30<10:31:56, 30.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.531, Acc on erroneous: 0.5714285714285714, Acc on non-erroneous: 0.5286016949152542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 201/1250 [03:02<2:49:13,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.647, Acc on erroneous: 0.4642857142857143, Acc on non-erroneous: 0.6578389830508474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 402/1250 [05:35<1:36:42,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.783, Acc on erroneous: 0.30357142857142855, Acc on non-erroneous: 0.8114406779661016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 601/1250 [08:07<1:44:35,  9.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.697, Acc on erroneous: 0.30357142857142855, Acc on non-erroneous: 0.7203389830508474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 801/1250 [10:39<1:12:20,  9.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.769, Acc on erroneous: 0.21428571428571427, Acc on non-erroneous: 0.801906779661017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1001/1250 [13:11<40:06,  9.67s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.797, Acc on erroneous: 0.26785714285714285, Acc on non-erroneous: 0.8283898305084746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1201/1250 [15:43<07:54,  9.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.813, Acc on erroneous: 0.17857142857142858, Acc on non-erroneous: 0.850635593220339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [16:13<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.3303692638874054\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "wandb.init(\n",
    "    project=\"weak-deception\",\n",
    "        \n",
    "    # track hyperparameters and run metadata\n",
    "    config=config\n",
    ")\n",
    "# only the LORA parameters should be updated\n",
    "optimizer = AdamW([p for p in model.parameters() if p.requires_grad], lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "eval_interval = 200  # steps\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (step + 1) % eval_interval == 0:\n",
    "            acc, acc_err, acc_non_err = eval_model(use_tqdm=False)\n",
    "            print(f\"Acc: {acc}, Acc on erroneous: {acc_err}, Acc on non-erroneous: {acc_non_err}\")\n",
    "            wandb.log({\"acc\": acc, \"acc_err\": acc_err, \"acc_non_err\": acc_non_err, \"loss\": total_loss / step, \"step\": step, \"epoch\": epoch})\n",
    "            model.train()\n",
    "            \n",
    "    print(\"Epoch {} loss: {}\".format(epoch, total_loss / len(train_dataloader)))\n",
    "    \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# this function is overridden by the peft library\n",
    "model.save_pretrained(f\"custom-models/{model_name}-{ds_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlkb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
