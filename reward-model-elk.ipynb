{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers.models.gpt_neox.modeling_gpt_neox import GPTNeoXConfig, GPTNeoXModel, GPTNeoXPreTrainedModel\n",
    "from transformers.utils import ModelOutput\n",
    "\n",
    "\n",
    "class GPTNeoXRewardModelConfig(GPTNeoXConfig):\n",
    "    model_type = \"gpt_neox_reward_model\"\n",
    "\n",
    "    pooling: Literal[\"mean\", \"last\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pooling: Literal[\"mean\", \"last\"] = \"last\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pooling = pooling or \"last\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTNeoXRewardModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Reward model output.\n",
    "\n",
    "    Args:\n",
    "        logits (`torch.FloatTensor` of shape `(batch_size, 1)`):\n",
    "            Reward score\n",
    "    \"\"\"\n",
    "\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[torch.FloatTensor] = None\n",
    "\n",
    "\n",
    "class GPTNeoXRewardModel(GPTNeoXPreTrainedModel):\n",
    "    config_class = GPTNeoXRewardModelConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        if type(config) == GPTNeoXConfig:\n",
    "            # When a normal GPTNeoX was loaded it will be converted into a reward model.\n",
    "            # The direct `type(config) == GPTNeoXConfig` comparison is used (instead of\n",
    "            # `isinstance()`) since the configuration class of the reward model is also\n",
    "            # derived form `GPTNeoXConfig`.\n",
    "            config = GPTNeoXRewardModelConfig.from_dict(config.to_dict())\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.gpt_neox = GPTNeoXModel(config)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, 1)\n",
    "        self.pooling = config.pooling\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = True,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "    ) -> GPTNeoXRewardModelOutput:\n",
    "        outputs = self.gpt_neox(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            return_dict=return_dict,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        if self.pooling == \"mean\":\n",
    "            if attention_mask is None:\n",
    "                pooled = hidden_states.mean(dim=1)\n",
    "            else:\n",
    "                pooled = (hidden_states * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "        elif self.pooling == \"last\":\n",
    "            if attention_mask is None:\n",
    "                pooled = hidden_states[:, -1]\n",
    "            else:\n",
    "                last_idx = attention_mask.cumsum(dim=1).argmax(dim=1)\n",
    "                pooled = hidden_states.gather(1, last_idx.view(-1, 1, 1).expand(-1, 1, hidden_states.size(-1))).squeeze(\n",
    "                    1\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling method: {self.pooling}\")\n",
    "\n",
    "        logits = self.out_proj(pooled)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (logits,) + outputs[1:]\n",
    "\n",
    "        return GPTNeoXRewardModelOutput(logits=logits, hidden_states=outputs.hidden_states if output_hidden_states else None)\n",
    "\n",
    "\n",
    "AutoConfig.register(\"gpt_neox_reward_model\", GPTNeoXRewardModelConfig)\n",
    "AutoModelForSequenceClassification.register(GPTNeoXRewardModelConfig, GPTNeoXRewardModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1\"\n",
    "device = \"cuda:7\"\n",
    "rm = AutoModelForSequenceClassification.from_pretrained(model_name).to(device).half()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.15234375\n"
     ]
    }
   ],
   "source": [
    "input_text = \"<|prompter|>1+2=?<|endoftext|><|assistant|>1 + 2 equals 3.<|endoftext|>\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = rm(**inputs, return_dict=False)\n",
    "hidden_states = outputs[1]\n",
    "score = outputs[0][0].item()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the pythia-6.9B reward model was not just trained on OASST (69,000 assitant messages) \n",
    "# but also SHP (350k??), 0.1 of hh-rlhf (16k), half of hellaswag (20k), web_gpt (20k) and hf_summary_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load oasst dataset\n",
    "import json\n",
    "path = \"oasst/2023-04-12_oasst_all.trees.jsonl\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    trees = [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_tree_id': '6ab24d72-0181-4594-a9cd-deaf170242fb',\n",
       " 'tree_state': 'ready_for_export',\n",
       " 'prompt': {'message_id': '6ab24d72-0181-4594-a9cd-deaf170242fb',\n",
       "  'user_id': 'c3fe8c76-fc30-4fa7-b7f8-c492f5967d18',\n",
       "  'created_date': '2023-02-05T14:23:50.983374+00:00',\n",
       "  'text': 'Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.',\n",
       "  'role': 'prompter',\n",
       "  'lang': 'en',\n",
       "  'review_count': 3,\n",
       "  'review_result': True,\n",
       "  'deleted': False,\n",
       "  'synthetic': False,\n",
       "  'emojis': {'+1': 10, '_skip_reply': 1, '_skip_ranking': 4},\n",
       "  'replies': [{'message_id': 'c8e83833-ecbc-44fe-b6db-735228c25a1c',\n",
       "    'parent_id': '6ab24d72-0181-4594-a9cd-deaf170242fb',\n",
       "    'user_id': '2c96e467-66f0-4be7-9693-bda51356a424',\n",
       "    'created_date': '2023-02-06T13:50:44.657083+00:00',\n",
       "    'text': '\"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.',\n",
       "    'role': 'assistant',\n",
       "    'lang': 'en',\n",
       "    'review_count': 3,\n",
       "    'review_result': True,\n",
       "    'deleted': False,\n",
       "    'rank': 0,\n",
       "    'synthetic': False,\n",
       "    'emojis': {'+1': 3, '_skip_labeling': 1},\n",
       "    'replies': [{'message_id': '6708c47f-05c9-4346-b3d2-40b2bd24fde4',\n",
       "      'parent_id': 'c8e83833-ecbc-44fe-b6db-735228c25a1c',\n",
       "      'user_id': '2c96e467-66f0-4be7-9693-bda51356a424',\n",
       "      'created_date': '2023-02-06T18:48:49.391686+00:00',\n",
       "      'text': 'Now explain it to a dog',\n",
       "      'role': 'prompter',\n",
       "      'lang': 'en',\n",
       "      'review_count': 3,\n",
       "      'review_result': True,\n",
       "      'deleted': False,\n",
       "      'synthetic': False,\n",
       "      'replies': [],\n",
       "      'labels': {'spam': {'value': 0.0, 'count': 3},\n",
       "       'lang_mismatch': {'value': 0.0, 'count': 3},\n",
       "       'pii': {'value': 0.0, 'count': 2},\n",
       "       'not_appropriate': {'value': 0.0, 'count': 2},\n",
       "       'hate_speech': {'value': 0.0, 'count': 2},\n",
       "       'sexual_content': {'value': 0.0, 'count': 2},\n",
       "       'quality': {'value': 0.25, 'count': 3},\n",
       "       'toxicity': {'value': 0.5, 'count': 2},\n",
       "       'humor': {'value': 0.625, 'count': 2},\n",
       "       'creativity': {'value': 0.5, 'count': 2},\n",
       "       'violence': {'value': 0.0, 'count': 2}},\n",
       "      'detoxify': {'toxicity': 0.03648477792739868,\n",
       "       'severe_toxicity': 5.486844383995049e-05,\n",
       "       'obscene': 0.0003762090636882931,\n",
       "       'identity_attack': 0.0002415566414128989,\n",
       "       'insult': 0.013612336479127407,\n",
       "       'threat': 0.0017075861105695367,\n",
       "       'sexual_explicit': 0.00010235361696686596}}],\n",
       "    'labels': {'spam': {'value': 0.0, 'count': 3},\n",
       "     'fails_task': {'value': 0.0, 'count': 2},\n",
       "     'lang_mismatch': {'value': 0.0, 'count': 3},\n",
       "     'pii': {'value': 0.0, 'count': 2},\n",
       "     'not_appropriate': {'value': 0.0, 'count': 2},\n",
       "     'hate_speech': {'value': 0.0, 'count': 2},\n",
       "     'sexual_content': {'value': 0.0, 'count': 2},\n",
       "     'quality': {'value': 0.9166666666666666, 'count': 3},\n",
       "     'toxicity': {'value': 0.375, 'count': 2},\n",
       "     'humor': {'value': 0.375, 'count': 2},\n",
       "     'helpfulness': {'value': 0.75, 'count': 2},\n",
       "     'creativity': {'value': 0.375, 'count': 2},\n",
       "     'violence': {'value': 0.0, 'count': 2}},\n",
       "    'detoxify': {'toxicity': 0.00026396565954200923,\n",
       "     'severe_toxicity': 2.7454958399175666e-05,\n",
       "     'obscene': 0.00013584605767391622,\n",
       "     'identity_attack': 9.263094398193061e-05,\n",
       "     'insult': 0.0001668655313551426,\n",
       "     'threat': 3.769186878344044e-05,\n",
       "     'sexual_explicit': 2.500762275303714e-05}},\n",
       "   {'message_id': '343ee2d4-87ae-41fd-a768-bdd65959dc4a',\n",
       "    'parent_id': '6ab24d72-0181-4594-a9cd-deaf170242fb',\n",
       "    'user_id': '49ddcb0d-6588-43bd-858d-19c577f12e7b',\n",
       "    'created_date': '2023-02-06T13:37:56.044680+00:00',\n",
       "    'text': 'Monopsony is a market structure in which there is a single buyer in a market. In the context of labor markets, a monopsony occurs when there is only one employer in a particular industry or geographic area, giving that employer significant power over the wages and working conditions of workers. This results in a situation where the employer has the ability to suppress wages and reduce the bargaining power of workers.\\n\\nResearch has shown that the existence of monopsony power in labor markets can have negative effects on workers, such as lower wages, reduced benefits, and limited job opportunities. For example, a study by Autor, Dorn, and Hanson (2013) found evidence of monopsony power in certain U.S. labor markets, resulting in lower wages and reduced job opportunities for workers. Similarly, another study by Bharadwaj, Black, and Kolesnikova (2019) found that the presence of a single dominant employer in a local labor market was associated with lower wages for workers.',\n",
       "    'role': 'assistant',\n",
       "    'lang': 'en',\n",
       "    'review_count': 3,\n",
       "    'review_result': True,\n",
       "    'deleted': False,\n",
       "    'rank': 1,\n",
       "    'synthetic': False,\n",
       "    'emojis': {'+1': 2, '_skip_reply': 1, '_skip_labeling': 2},\n",
       "    'replies': [{'message_id': '18145bf4-37fd-4ac0-80f5-6108b5f2b365',\n",
       "      'parent_id': '343ee2d4-87ae-41fd-a768-bdd65959dc4a',\n",
       "      'user_id': 'e10e99a0-38ac-4b07-bf5d-4427696e4e0d',\n",
       "      'created_date': '2023-02-06T18:52:51.428543+00:00',\n",
       "      'text': 'How can one fight back when a monospony had been created?',\n",
       "      'role': 'prompter',\n",
       "      'lang': 'en',\n",
       "      'review_count': 3,\n",
       "      'review_result': True,\n",
       "      'deleted': False,\n",
       "      'synthetic': False,\n",
       "      'emojis': {'+1': 1},\n",
       "      'replies': [],\n",
       "      'labels': {'spam': {'value': 0.0, 'count': 3},\n",
       "       'lang_mismatch': {'value': 0.0, 'count': 3},\n",
       "       'pii': {'value': 0.0, 'count': 2},\n",
       "       'not_appropriate': {'value': 0.0, 'count': 2},\n",
       "       'hate_speech': {'value': 0.0, 'count': 2},\n",
       "       'sexual_content': {'value': 0.0, 'count': 2},\n",
       "       'quality': {'value': 0.75, 'count': 3},\n",
       "       'toxicity': {'value': 0.0, 'count': 2},\n",
       "       'humor': {'value': 0.0, 'count': 2},\n",
       "       'creativity': {'value': 0.0, 'count': 2},\n",
       "       'violence': {'value': 0.0, 'count': 2}},\n",
       "      'detoxify': {'toxicity': 0.0009362137061543763,\n",
       "       'severe_toxicity': 6.723815022269264e-05,\n",
       "       'obscene': 0.00041612397762946784,\n",
       "       'identity_attack': 0.00018043046293314546,\n",
       "       'insult': 0.00034414001856930554,\n",
       "       'threat': 0.00023999605036806315,\n",
       "       'sexual_explicit': 6.387894973158836e-05}}],\n",
       "    'labels': {'spam': {'value': 0.0, 'count': 3},\n",
       "     'fails_task': {'value': 0.0, 'count': 3},\n",
       "     'lang_mismatch': {'value': 0.0, 'count': 3},\n",
       "     'pii': {'value': 0.0, 'count': 3},\n",
       "     'not_appropriate': {'value': 0.0, 'count': 3},\n",
       "     'hate_speech': {'value': 0.0, 'count': 3},\n",
       "     'sexual_content': {'value': 0.0, 'count': 3},\n",
       "     'quality': {'value': 0.8333333333333334, 'count': 3},\n",
       "     'toxicity': {'value': 0.25, 'count': 3},\n",
       "     'humor': {'value': 0.16666666666666666, 'count': 3},\n",
       "     'helpfulness': {'value': 0.9166666666666666, 'count': 3},\n",
       "     'creativity': {'value': 0.25, 'count': 3},\n",
       "     'violence': {'value': 0.16666666666666666, 'count': 3}},\n",
       "    'detoxify': {'toxicity': 0.0008866374846547842,\n",
       "     'severe_toxicity': 7.99110930529423e-05,\n",
       "     'obscene': 0.00034686794970184565,\n",
       "     'identity_attack': 0.0004464525554794818,\n",
       "     'insult': 0.0005109519697725773,\n",
       "     'threat': 8.753630390856415e-05,\n",
       "     'sexual_explicit': 5.744404188590124e-05}},\n",
       "   {'message_id': '636dd191-50df-4894-ba9a-cd7f00767258',\n",
       "    'parent_id': '6ab24d72-0181-4594-a9cd-deaf170242fb',\n",
       "    'user_id': 'c212120c-0b79-4b32-a5b1-d96686ca92ca',\n",
       "    'created_date': '2023-02-06T14:28:09.611612+00:00',\n",
       "    'text': 'Monopsony refers to a market structure where there is only one buyer of a good or service. In the context of labor markets, a monopsony occurs when there is only one employer in a particular geographic area or industry, giving them significant power to set wages and working conditions. Research has shown that the existence of monopsonies can lead to lower wages and reduced job opportunities for workers. For example, a study by the National Bureau of Economic Research found that in industries with high levels of concentration, workers earn lower wages and are less likely to receive benefits such as health insurance.',\n",
       "    'role': 'assistant',\n",
       "    'lang': 'en',\n",
       "    'review_count': 3,\n",
       "    'review_result': True,\n",
       "    'deleted': False,\n",
       "    'rank': 2,\n",
       "    'synthetic': False,\n",
       "    'emojis': {'+1': 6, '_skip_reply': 3},\n",
       "    'replies': [{'message_id': 'ac94bfcf-7f25-4084-8755-dde345ac2323',\n",
       "      'parent_id': '636dd191-50df-4894-ba9a-cd7f00767258',\n",
       "      'user_id': '3ef645f1-c9f7-49e0-8e53-3c9a9c440ca1',\n",
       "      'created_date': '2023-02-06T16:44:30.360408+00:00',\n",
       "      'text': \"What can be done at a regulatory level to ensure, in a monopsony, that the power over the employees isn't abused? List a few options and focus on the organs that should act.\",\n",
       "      'role': 'prompter',\n",
       "      'lang': 'en',\n",
       "      'review_count': 3,\n",
       "      'review_result': True,\n",
       "      'deleted': False,\n",
       "      'synthetic': False,\n",
       "      'emojis': {'+1': 3,\n",
       "       '-1': 1,\n",
       "       '_skip_reply': 9,\n",
       "       '_skip_ranking': 5,\n",
       "       '_skip_labeling': 1},\n",
       "      'replies': [{'message_id': '73d6f715-3787-409c-81e4-fde0e5ef60cd',\n",
       "        'parent_id': 'ac94bfcf-7f25-4084-8755-dde345ac2323',\n",
       "        'user_id': '2c96e467-66f0-4be7-9693-bda51356a424',\n",
       "        'created_date': '2023-02-06T18:42:19.103071+00:00',\n",
       "        'text': 'Here are some potential regulatory options to address monopsony power over employees:\\n\\n- Antitrust enforcement agencies like the FTC or DOJ could investigate and take action against dominant employers that are abusing their market power to suppress wages or worsen working conditions. They could force divestitures or other structural remedies.\\n- Labor standards agencies could set industry-specific minimum wage floors or other regulations around compensation and working conditions. This would reduce the ability of dominant employers to push wages/conditions below certain thresholds.\\n- Sector-specific regulators could impose requirements around how companies hire, compensate, and treat workers. For example, banking regulators could prohibit abusive practices by dominant banks with regard to their employees.\\n- Legislatures could pass laws',\n",
       "        'role': 'assistant',\n",
       "        'lang': 'en',\n",
       "        'review_count': 3,\n",
       "        'review_result': True,\n",
       "        'deleted': False,\n",
       "        'rank': 0,\n",
       "        'synthetic': False,\n",
       "        'emojis': {'_skip_labeling': 1},\n",
       "        'replies': [],\n",
       "        'labels': {'spam': {'value': 0.0, 'count': 3},\n",
       "         'fails_task': {'value': 0.0, 'count': 2},\n",
       "         'lang_mismatch': {'value': 0.0, 'count': 3},\n",
       "         'pii': {'value': 0.0, 'count': 2},\n",
       "         'not_appropriate': {'value': 0.0, 'count': 2},\n",
       "         'hate_speech': {'value': 0.0, 'count': 2},\n",
       "         'sexual_content': {'value': 0.0, 'count': 2},\n",
       "         'quality': {'value': 0.5833333333333334, 'count': 3},\n",
       "         'toxicity': {'value': 0.125, 'count': 2},\n",
       "         'humor': {'value': 0.0, 'count': 2},\n",
       "         'helpfulness': {'value': 0.375, 'count': 2},\n",
       "         'creativity': {'value': 0.0, 'count': 2},\n",
       "         'violence': {'value': 0.0, 'count': 2}},\n",
       "        'detoxify': {'toxicity': 0.00036603500484488904,\n",
       "         'severe_toxicity': 2.5196904971380718e-05,\n",
       "         'obscene': 0.00010540795483393595,\n",
       "         'identity_attack': 0.00011445347627159208,\n",
       "         'insult': 0.00016151837189681828,\n",
       "         'threat': 3.965371070080437e-05,\n",
       "         'sexual_explicit': 2.380239857302513e-05}},\n",
       "       {'message_id': 'b280ccbc-b68f-42b9-9fc2-d7ac89b88022',\n",
       "        'parent_id': 'ac94bfcf-7f25-4084-8755-dde345ac2323',\n",
       "        'user_id': 'cd4e0925-f3f0-478e-b6d5-a9fd66a614cb',\n",
       "        'created_date': '2023-02-06T19:47:32.808838+00:00',\n",
       "        'text': \"Regulatory intervention can be used to address the negative consequences of monopsony power in the labor market. A few options include:\\n\\n1. Antitrust enforcement: Antitrust laws can be used to prevent firms from engaging in practices that limit competition and suppress wages. The antitrust division of the Department of Justice and the Federal Trade Commission (FTC) are responsible for enforcing these laws.\\n\\n2. Labor standards and protections: Governments can set minimum wage standards and other labor protections to ensure that workers are paid a fair wage, even in industries dominated by a single buyer. The U.S. Department of Labor is responsible for enforcing these standards.\\n\\n3. Occupational licensing reform: Occupational licensing requirements can restrict worker mobility and increase the bargaining power of employers. Reforms to licensing requirements can reduce these barriers and increase competition in the labor market. State governments typically regulate occupational licensing.\\n\\n4. Collective bargaining rights: Collective bargaining by workers can increase their bargaining power and lead to higher wages. Governments can protect and promote collective bargaining rights to counteract the effects of monopsony power. The National Labor Relations Board is responsible for enforcing workers' rights to engage in collective bargaining in the U.S.\\n\\nThese are just a few examples of regulatory options that can be used to address monopsony power in the labor market. The specific actions taken will depend on the particular circumstances and context of each case.\",\n",
       "        'role': 'assistant',\n",
       "        'lang': 'en',\n",
       "        'review_count': 3,\n",
       "        'review_result': True,\n",
       "        'deleted': False,\n",
       "        'rank': 1,\n",
       "        'synthetic': False,\n",
       "        'replies': [],\n",
       "        'labels': {'spam': {'value': 0.0, 'count': 3},\n",
       "         'fails_task': {'value': 0.0, 'count': 3},\n",
       "         'lang_mismatch': {'value': 0.0, 'count': 3},\n",
       "         'pii': {'value': 0.0, 'count': 3},\n",
       "         'not_appropriate': {'value': 0.0, 'count': 3},\n",
       "         'hate_speech': {'value': 0.0, 'count': 3},\n",
       "         'sexual_content': {'value': 0.0, 'count': 3},\n",
       "         'quality': {'value': 0.25, 'count': 3},\n",
       "         'toxicity': {'value': 0.0, 'count': 3},\n",
       "         'humor': {'value': 0.0, 'count': 3},\n",
       "         'helpfulness': {'value': 0.25, 'count': 3},\n",
       "         'creativity': {'value': 0.0, 'count': 3},\n",
       "         'violence': {'value': 0.0, 'count': 3}},\n",
       "        'detoxify': {'toxicity': 0.00031410896917805076,\n",
       "         'severe_toxicity': 2.3768236133037135e-05,\n",
       "         'obscene': 0.00011176631960552186,\n",
       "         'identity_attack': 9.1741414507851e-05,\n",
       "         'insult': 0.00014727565576322377,\n",
       "         'threat': 3.7340509152272716e-05,\n",
       "         'sexual_explicit': 2.486484299879521e-05}},\n",
       "       {'message_id': 'be4d0752-afbe-473b-99f3-48f1b5880f90',\n",
       "        'parent_id': 'ac94bfcf-7f25-4084-8755-dde345ac2323',\n",
       "        'user_id': 'c212120c-0b79-4b32-a5b1-d96686ca92ca',\n",
       "        'created_date': '2023-02-06T18:34:30.836581+00:00',\n",
       "        'text': \"Yes, that's correct. Keeping the code for the TESR and the registration code in the same file can make it easier to manage and maintain the code. The ExampleMod class acts as a container for the TESR code, providing a convenient and logical grouping. The functionality of the code remains unchanged, and the TESR will still be properly registered and used to render the custom tile entities in the game.\",\n",
       "        'role': 'assistant',\n",
       "        'lang': 'en',\n",
       "        'review_count': 3,\n",
       "        'review_result': True,\n",
       "        'deleted': False,\n",
       "        'rank': 2,\n",
       "        'synthetic': False,\n",
       "        'emojis': {'_skip_labeling': 4},\n",
       "        'replies': [],\n",
       "        'labels': {'spam': {'value': 0.3333333333333333, 'count': 3},\n",
       "         'fails_task': {'value': 1.0, 'count': 2},\n",
       "         'lang_mismatch': {'value': 0.0, 'count': 3},\n",
       "         'pii': {'value': 0.0, 'count': 2},\n",
       "         'not_appropriate': {'value': 0.5, 'count': 2},\n",
       "         'hate_speech': {'value': 0.0, 'count': 2},\n",
       "         'sexual_content': {'value': 0.0, 'count': 2},\n",
       "         'quality': {'value': 0.16666666666666666, 'count': 3},\n",
       "         'toxicity': {'value': 0.0, 'count': 2},\n",
       "         'humor': {'value': 0.125, 'count': 2},\n",
       "         'helpfulness': {'value': 0.0, 'count': 2},\n",
       "         'creativity': {'value': 0.25, 'count': 2},\n",
       "         'violence': {'value': 0.0, 'count': 2}},\n",
       "        'detoxify': {'toxicity': 0.00026471694582141936,\n",
       "         'severe_toxicity': 3.753640703507699e-05,\n",
       "         'obscene': 0.00018712480959948152,\n",
       "         'identity_attack': 0.00012360737309791148,\n",
       "         'insult': 0.000221485854126513,\n",
       "         'threat': 4.4919768697582185e-05,\n",
       "         'sexual_explicit': 3.185281457263045e-05}}],\n",
       "      'labels': {'spam': {'value': 0.0, 'count': 3},\n",
       "       'lang_mismatch': {'value': 0.0, 'count': 3},\n",
       "       'pii': {'value': 0.0, 'count': 2},\n",
       "       'not_appropriate': {'value': 0.0, 'count': 2},\n",
       "       'hate_speech': {'value': 0.0, 'count': 2},\n",
       "       'sexual_content': {'value': 0.0, 'count': 2},\n",
       "       'quality': {'value': 0.6666666666666666, 'count': 3},\n",
       "       'toxicity': {'value': 0.125, 'count': 2},\n",
       "       'humor': {'value': 0.0, 'count': 2},\n",
       "       'creativity': {'value': 0.375, 'count': 2},\n",
       "       'violence': {'value': 0.0, 'count': 2}},\n",
       "      'detoxify': {'toxicity': 0.0062903789803385735,\n",
       "       'severe_toxicity': 9.325604332843795e-05,\n",
       "       'obscene': 0.0004946678527630866,\n",
       "       'identity_attack': 0.00021326237765606493,\n",
       "       'insult': 0.0005733256111852825,\n",
       "       'threat': 0.00121682311873883,\n",
       "       'sexual_explicit': 0.0007907780236564577}},\n",
       "     {'message_id': 'fc64fa35-08a8-422c-8f6c-2d86a2834aac',\n",
       "      'parent_id': '636dd191-50df-4894-ba9a-cd7f00767258',\n",
       "      'user_id': '63321c74-d512-4221-96ef-04bd1f2d95fd',\n",
       "      'created_date': '2023-02-06T16:26:47.581724+00:00',\n",
       "      'text': 'I would imagine this is similar or even the same as a \"monopoly\" but I am not sure.\\n\\nAssuming that this some kind of \"final state\" of a particular industry or even an entire economy, give me some larger historical examples of this kind of structure consolidating over time, and describe what circumstances are common prerequisite.',\n",
       "      'role': 'prompter',\n",
       "      'lang': 'en',\n",
       "      'review_count': 3,\n",
       "      'review_result': True,\n",
       "      'deleted': False,\n",
       "      'synthetic': False,\n",
       "      'emojis': {'+1': 1, '_skip_reply': 4, '_skip_labeling': 2},\n",
       "      'replies': [{'message_id': 'f934d7eb-5993-4f65-8a3a-74ef7ec58285',\n",
       "        'parent_id': 'fc64fa35-08a8-422c-8f6c-2d86a2834aac',\n",
       "        'user_id': 'c212120c-0b79-4b32-a5b1-d96686ca92ca',\n",
       "        'created_date': '2023-02-06T19:31:02.618898+00:00',\n",
       "        'text': 'Bouguereau died in 1905, so it is unlikely that any of his direct descendants are still alive. However, his works continue to be popular and are sold at auction and through art dealers. It is possible that some indirect descendants may benefit from the sale of his works.',\n",
       "        'role': 'assistant',\n",
       "        'lang': 'en',\n",
       "        'review_count': 4,\n",
       "        'review_result': True,\n",
       "        'deleted': False,\n",
       "        'synthetic': False,\n",
       "        'emojis': {'-1': 2, '_skip_labeling': 1},\n",
       "        'replies': [],\n",
       "        'labels': {'spam': {'value': 0.5, 'count': 4},\n",
       "         'fails_task': {'value': 0.75, 'count': 4},\n",
       "         'lang_mismatch': {'value': 0.0, 'count': 4},\n",
       "         'pii': {'value': 0.0, 'count': 4},\n",
       "         'not_appropriate': {'value': 0.0, 'count': 4},\n",
       "         'hate_speech': {'value': 0.0, 'count': 4},\n",
       "         'sexual_content': {'value': 0.0, 'count': 4},\n",
       "         'quality': {'value': 0.125, 'count': 4},\n",
       "         'toxicity': {'value': 0.0, 'count': 4},\n",
       "         'humor': {'value': 0.0, 'count': 4},\n",
       "         'helpfulness': {'value': 0.125, 'count': 4},\n",
       "         'creativity': {'value': 0.0, 'count': 4},\n",
       "         'violence': {'value': 0.0, 'count': 4}},\n",
       "        'detoxify': {'toxicity': 0.0002831516321748495,\n",
       "         'severe_toxicity': 4.085423643118702e-05,\n",
       "         'obscene': 0.0003183183434884995,\n",
       "         'identity_attack': 0.00010793474211823195,\n",
       "         'insult': 0.00042535082320682704,\n",
       "         'threat': 4.304728645365685e-05,\n",
       "         'sexual_explicit': 3.362461575306952e-05}}],\n",
       "      'labels': {'spam': {'value': 0.0, 'count': 3},\n",
       "       'lang_mismatch': {'value': 0.0, 'count': 3},\n",
       "       'pii': {'value': 0.0, 'count': 2},\n",
       "       'not_appropriate': {'value': 0.0, 'count': 2},\n",
       "       'hate_speech': {'value': 0.0, 'count': 2},\n",
       "       'sexual_content': {'value': 0.0, 'count': 2},\n",
       "       'quality': {'value': 0.8333333333333334, 'count': 3},\n",
       "       'toxicity': {'value': 0.0, 'count': 2},\n",
       "       'humor': {'value': 0.0, 'count': 2},\n",
       "       'creativity': {'value': 0.375, 'count': 2},\n",
       "       'violence': {'value': 0.0, 'count': 2}},\n",
       "      'detoxify': {'toxicity': 0.00028263000422157347,\n",
       "       'severe_toxicity': 2.4758721338002943e-05,\n",
       "       'obscene': 0.00013598523219116032,\n",
       "       'identity_attack': 8.525548037141562e-05,\n",
       "       'insult': 0.00015908791101537645,\n",
       "       'threat': 3.2408170227427036e-05,\n",
       "       'sexual_explicit': 2.3642409360036254e-05}}],\n",
       "    'labels': {'spam': {'value': 0.0, 'count': 3},\n",
       "     'fails_task': {'value': 0.0, 'count': 3},\n",
       "     'lang_mismatch': {'value': 0.0, 'count': 3},\n",
       "     'pii': {'value': 0.0, 'count': 3},\n",
       "     'not_appropriate': {'value': 0.0, 'count': 3},\n",
       "     'hate_speech': {'value': 0.0, 'count': 3},\n",
       "     'sexual_content': {'value': 0.0, 'count': 3},\n",
       "     'quality': {'value': 0.8333333333333334, 'count': 3},\n",
       "     'toxicity': {'value': 0.0, 'count': 3},\n",
       "     'humor': {'value': 0.0, 'count': 3},\n",
       "     'helpfulness': {'value': 0.8333333333333334, 'count': 3},\n",
       "     'creativity': {'value': 0.0, 'count': 3},\n",
       "     'violence': {'value': 0.0, 'count': 3}},\n",
       "    'detoxify': {'toxicity': 0.0002960403508041054,\n",
       "     'severe_toxicity': 3.159583866363391e-05,\n",
       "     'obscene': 0.0001712603261694312,\n",
       "     'identity_attack': 0.00010074262536363676,\n",
       "     'insult': 0.00018406218441668898,\n",
       "     'threat': 4.080198414158076e-05,\n",
       "     'sexual_explicit': 2.7802774638985284e-05}}],\n",
       "  'labels': {'spam': {'value': 0.0, 'count': 3},\n",
       "   'lang_mismatch': {'value': 0.0, 'count': 3},\n",
       "   'pii': {'value': 0.0, 'count': 3},\n",
       "   'not_appropriate': {'value': 0.0, 'count': 3},\n",
       "   'hate_speech': {'value': 0.0, 'count': 3},\n",
       "   'sexual_content': {'value': 0.0, 'count': 3},\n",
       "   'quality': {'value': 0.9166666666666666, 'count': 3},\n",
       "   'toxicity': {'value': 0.16666666666666666, 'count': 3},\n",
       "   'humor': {'value': 0.3333333333333333, 'count': 3},\n",
       "   'creativity': {'value': 0.6666666666666666, 'count': 3},\n",
       "   'violence': {'value': 0.0, 'count': 3}},\n",
       "  'detoxify': {'toxicity': 0.00044308538781479,\n",
       "   'severe_toxicity': 3.252684837207198e-05,\n",
       "   'obscene': 0.00023475120542570949,\n",
       "   'identity_attack': 0.0001416115992469713,\n",
       "   'insult': 0.00039489680784754455,\n",
       "   'threat': 4.075629112776369e-05,\n",
       "   'sexual_explicit': 2.712695459194947e-05}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trees[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Missing ranks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Missing ranks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Missing ranks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Missing ranks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Missing ranks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "No rank found\n",
      "Proportion of replies with ranks: 0.9270261181481897\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "no_ranks = 0\n",
    "yes_ranks  = 0\n",
    "results = {\"tree_id\": [], \"prompt\": [], \"preferred\": [], \"rejected\": [], \"preferred_id\": [], \"rejected_id\": []}\n",
    "for tree in trees:\n",
    "    if tree[\"tree_state\"] != \"ready_for_export\":\n",
    "        continue\n",
    "    \n",
    "    stack = [(\"\", tree[\"prompt\"])]  # prefix_text, node\n",
    "    while stack:\n",
    "        parent_prefix, current = stack.pop()\n",
    "        if current.get(\"lang\", None) != \"en\" or current.get(\"deleted\", True) or not current.get(\"review_result\", False):\n",
    "            continue\n",
    "        def get_prompt(node):\n",
    "            if node[\"role\"] == \"prompter\":\n",
    "                return f\"<|prompter|>{node['text']}<|endoftext|>\"\n",
    "            elif node[\"role\"] == \"assistant\":\n",
    "                return f\"<|assistant|>{node['text']}<|endoftext|>\"\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown role: {node['role']}\")\n",
    "        current_prompt = parent_prefix + get_prompt(current)\n",
    "        stack.extend(list(zip([current_prompt] * len(current[\"replies\"]), current[\"replies\"])))\n",
    "\n",
    "        if current[\"role\"] == \"prompter\":\n",
    "            if len(current[\"replies\"]) >= 2:\n",
    "                if not all(\"rank\" in r for r in current[\"replies\"]):\n",
    "                    print(\"No rank found\")\n",
    "                    no_ranks += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    yes_ranks += 1\n",
    "                if sorted([r[\"rank\"] for r in current[\"replies\"]]) != list(range(len(current[\"replies\"]))):\n",
    "                    print(\"\\n\\n\\n\\nMissing ranks\\n\\n\\n\\n\")\n",
    "                    continue\n",
    "                # get random pair of data\n",
    "                reply_pair = random.sample(current[\"replies\"], 2)\n",
    "                pref, rej = sorted(reply_pair, key=lambda x: x[\"rank\"])\n",
    "                results[\"tree_id\"].append(tree[\"message_tree_id\"])\n",
    "                results[\"prompt\"].append(current_prompt)\n",
    "                results[\"preferred\"].append(get_prompt(pref))\n",
    "                results[\"rejected\"].append(get_prompt(rej))\n",
    "                results[\"preferred_id\"].append(pref[\"message_id\"])\n",
    "                results[\"rejected_id\"].append(rej[\"message_id\"])\n",
    "\n",
    "                # print(current[\"text\"])\n",
    "                # print(current_prompt)\n",
    "print(f\"Proportion of replies with ranks: {yes_ranks / (yes_ranks + no_ranks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8265"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results[\"tree_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = Dataset.from_dict(results)\n",
    "num_train = 4_265\n",
    "num_val = 1_000\n",
    "num_test = 3_000\n",
    "ds_dict = DatasetDict({\n",
    "    'train': ds.select(range(num_train)),\n",
    "    'val': ds.select(range(num_train, num_train + num_val)),\n",
    "    'test': ds.select(range(num_train + num_val, num_train + num_val + num_test)),\n",
    "})\n",
    "train_ds = ds_dict['train'].select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 4265/4265 [00:00<00:00, 374750.32 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 260515.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3000/3000 [00:00<00:00, 467071.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_dict.save_to_disk(\"custom-datasets/oasst-rm-ds-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "ds_dict = DatasetDict.load_from_disk(\"custom-datasets/oasst-rm-ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:08<00:00,  4.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# def get_rewards_and_hiddens(example):\n",
    "results = {\"rej_hiddens\": [], \"pref_hiddens\": [], \"rej_score\": [], \"pref_score\": []}\n",
    "for example in tqdm(train_ds):\n",
    "    # assert len(examples[\"prompt\"]) == 1a\n",
    "    # example = {k: v[0] for k, v in examples.items()}\n",
    "    \n",
    "    pref_input = example[\"prompt\"] + example[\"preferred\"]\n",
    "    rej_input = example[\"prompt\"] + example[\"rejected\"]\n",
    "    inps = {\"pref\": pref_input, \"rej\": rej_input}\n",
    "    for k, v in inps.items():\n",
    "        input_text = v\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = rm(**inputs, return_dict=True, output_hidden_states=True)\n",
    "        hidden_states = torch.stack(outputs.hidden_states).cpu()[..., -1, :].squeeze(1)\n",
    "        score = outputs.logits.item()\n",
    "        results[f\"{k}_hiddens\"].append(hidden_states)\n",
    "        results[f\"{k}_score\"].append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find examples with most disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each row\n",
    "# if the difference in scores is large and positive (say >1)\n",
    "# then add rej to LR set with label 0, and pref to LR set with label 1\n",
    "# then train LR\n",
    "# then use LR to predict labels for all rows\n",
    "# and see which rows it disagrees with the RM on most\n",
    "import numpy as np\n",
    "\n",
    "LR_hiddens = []\n",
    "labels = []\n",
    "train_idxs = []\n",
    "for i, ex in enumerate(train_ds):\n",
    "    if results[\"pref_scores\"][i] - results[\"rej_scores\"][i] > 1:  # only train on confident examples\n",
    "        LR_hiddens.append(results[\"rej_hiddens\"][i].numpy())\n",
    "        labels.append(0)\n",
    "        LR_hiddens.append(results[\"pref_hiddens\"][i].numpy())\n",
    "        labels.append(1)\n",
    "        train_idxs.append(i)\n",
    "\n",
    "LR_hiddens = np.stack(LR_hiddens)\n",
    "labels = np.array(labels)\n",
    "\n",
    "layer = 18\n",
    "LR_hiddens = LR_hiddens[:, layer, :]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression()\n",
    "LR.fit(LR_hiddens, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.,  5.,  1.,  6.,  5.,  6.,  9., 12., 21., 24., 12., 19., 20.,\n",
       "        31., 45., 47., 55., 63., 63., 78., 63., 77., 84., 92., 91., 97.,\n",
       "        91., 92., 81., 72., 81., 70., 66., 55., 69., 42., 36., 26., 36.,\n",
       "        28., 21., 32., 16., 13., 10., 11.,  8.,  6.,  5.,  5.]),\n",
       " array([-2.0029365 , -1.92296998, -1.84300346, -1.76303694, -1.68307043,\n",
       "        -1.60310391, -1.52313739, -1.44317088, -1.36320436, -1.28323784,\n",
       "        -1.20327132, -1.12330481, -1.04333829, -0.96337177, -0.88340525,\n",
       "        -0.80343874, -0.72347222, -0.6435057 , -0.56353918, -0.48357267,\n",
       "        -0.40360615, -0.32363963, -0.24367312, -0.1637066 , -0.08374008,\n",
       "        -0.00377356,  0.07619295,  0.15615947,  0.23612599,  0.31609251,\n",
       "         0.39605902,  0.47602554,  0.55599206,  0.63595857,  0.71592509,\n",
       "         0.79589161,  0.87585813,  0.95582464,  1.03579116,  1.11575768,\n",
       "         1.1957242 ,  1.27569071,  1.35565723,  1.43562375,  1.51559026,\n",
       "         1.59555678,  1.6755233 ,  1.75548982,  1.83545633,  1.91542285,\n",
       "         1.99538937]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAf7klEQVR4nO3de3BU5f3H8c+GSxJDLiaYXTImEi0OeCkqlxBgHKLReEMyRi2dVCNliLUJbUhHIY7Bkqqp1EoGjEQcG+sM1Mu0oNJpWhqBTGsSMEhbbwGEmgizQU2TlfgjBLK/P/y5v66E3Dib8+zyfs2ckT3n2bPfHDLsx+c5z3McXq/XKwAAAIOE2V0AAADAtxFQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGGW13AcPR29urI0eOKDo6Wg6Hw+5yAADAIHi9Xn355ZdKSkpSWFj/fSRBGVCOHDmi5ORku8sAAADD0NraqgsvvLDfNkEZUKKjoyV9/QPGxMTYXA0AABgMj8ej5ORk3/d4f4IyoHwzrBMTE0NAAQAgyAzm9gxukgUAAMYZckCpq6vT/PnzlZSUJIfDoS1btviO9fT0aPny5bryyisVFRWlpKQk3XvvvTpy5IjfOdrb25Wbm6uYmBjFxcVp8eLFOnbs2Fn/MAAAIDQMOaB0dXVp6tSpqqysPO3YV199pT179qi0tFR79uzRH/7wBzU3N+v222/3a5ebm6v3339f27Zt09atW1VXV6f8/Pzh/xQAACCkOLxer3fYb3Y4tHnzZmVnZ5+xze7duzVz5kx98sknSklJ0YcffqjLLrtMu3fv1vTp0yVJNTU1uuWWW/Tpp58qKSlpwM/1eDyKjY1VZ2cn96AAABAkhvL9HfB7UDo7O+VwOBQXFydJqq+vV1xcnC+cSFJmZqbCwsLU2NjY5zm6u7vl8Xj8NgAAELoCGlCOHz+u5cuX6/vf/74vKbndbiUmJvq1Gz16tOLj4+V2u/s8T3l5uWJjY30ba6AAABDaAhZQenp6dPfdd8vr9Wr9+vVnda6SkhJ1dnb6ttbWVouqBAAAJgrIOijfhJNPPvlEb731lt84k8vl0tGjR/3anzx5Uu3t7XK5XH2eLzw8XOHh4YEoFQAAGMjyHpRvwsn+/fv117/+VQkJCX7H09PT1dHRoaamJt++t956S729vUpLS7O6HAAAEISG3INy7NgxHThwwPf60KFD2rt3r+Lj4zVhwgTdeeed2rNnj7Zu3apTp0757iuJj4/X2LFjNWXKFN10001asmSJqqqq1NPTo8LCQi1cuHBQM3gAAEDoG/I04x07digjI+O0/Xl5efr5z3+u1NTUPt+3fft2zZs3T9LXC7UVFhbqzTffVFhYmHJycrR27VqNGzduUDUwzRgAgOAzlO/vs1oHxS4EFAAAgo9R66AAAAAMFQEFAAAYJyDTjAHABGu27RuwzbIbLh2BSgAMFT0oAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMwyweAOc0ZvoAZqIHBQAAGIeAAgAAjMMQD4CgNJihGQDBix4UAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4LNQGYETx7BsAg0EPCgAAMA4BBQAAGIchHgAYAMNSwMijBwUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYZ7TdBQBAKFizbd+AbZbdcOmInQcIdvSgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMM+SAUldXp/nz5yspKUkOh0NbtmzxO+71erVy5UpNmDBBkZGRyszM1P79+/3atLe3Kzc3VzExMYqLi9PixYt17Nixs/pBAABA6BhyQOnq6tLUqVNVWVnZ5/HVq1dr7dq1qqqqUmNjo6KiopSVlaXjx4/72uTm5ur999/Xtm3btHXrVtXV1Sk/P3/4PwUAAAgpQ15J9uabb9bNN9/c5zGv16uKigo98sgjWrBggSTppZdektPp1JYtW7Rw4UJ9+OGHqqmp0e7duzV9+nRJ0rp163TLLbfoqaeeUlJS0ln8OAAAIBRYeg/KoUOH5Ha7lZmZ6dsXGxurtLQ01dfXS5Lq6+sVFxfnCyeSlJmZqbCwMDU2NvZ53u7ubnk8Hr8NAACELkufxeN2uyVJTqfTb7/T6fQdc7vdSkxM9C9i9GjFx8f72nxbeXm5Vq1aZWWpAAJgMM+RAYDBCIpZPCUlJers7PRtra2tdpcEAAACyNKA4nK5JEltbW1++9va2nzHXC6Xjh496nf85MmTam9v97X5tvDwcMXExPhtAAAgdFkaUFJTU+VyuVRbW+vb5/F41NjYqPT0dElSenq6Ojo61NTU5Gvz1ltvqbe3V2lpaVaWAwAAgtSQ70E5duyYDhw44Ht96NAh7d27V/Hx8UpJSVFRUZEee+wxTZo0SampqSotLVVSUpKys7MlSVOmTNFNN92kJUuWqKqqSj09PSosLNTChQuZwQMAACQNI6C88847ysjI8L0uLi6WJOXl5enFF1/UQw89pK6uLuXn56ujo0Nz585VTU2NIiIifO/ZuHGjCgsLdf311yssLEw5OTlau3atBT8OAAAIBUMOKPPmzZPX6z3jcYfDobKyMpWVlZ2xTXx8vDZt2jTUjwYAAOeIoJjFAwAAzi0EFAAAYBxLF2oDgGA0q2VDv8cbUnhWGDDS6EEBAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIzDQm0AMELWbNtndwlA0KAHBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcZjFAyBkzGrZcMZjDSn5IfOZwLmAHhQAAGAcAgoAADAOAQUAABiHe1AAnBP6u1ckkO8FMDz0oAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAODyLB4Bx1mzbZ3cJAGxGDwoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwWagMQVGa1bLC7hKAwmMXult1w6QhUAgwPPSgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMaxPKCcOnVKpaWlSk1NVWRkpC655BL94he/kNfr9bXxer1auXKlJkyYoMjISGVmZmr//v1WlwIAAIKU5QHlySef1Pr16/XMM8/oww8/1JNPPqnVq1dr3bp1vjarV6/W2rVrVVVVpcbGRkVFRSkrK0vHjx+3uhwAABCELF+o7e2339aCBQt06623SpImTpyo3/3ud9q1a5ekr3tPKioq9Mgjj2jBggWSpJdeeklOp1NbtmzRwoULrS4JAGzR36JyDSn5w3rf154aZkVA8LC8B2X27Nmqra3Vvn1fr2L4j3/8Q3/729908803S5IOHTokt9utzMxM33tiY2OVlpam+vp6q8sBAABByPIelBUrVsjj8Wjy5MkaNWqUTp06pccff1y5ubmSJLfbLUlyOp1+73M6nb5j39bd3a3u7m7fa4/HY3XZAADAIJYHlFdffVUbN27Upk2bdPnll2vv3r0qKipSUlKS8vLyhnXO8vJyrVq1yuJKASA4DeY5O0Cws3yI58EHH9SKFSu0cOFCXXnllbrnnnu0bNkylZeXS5JcLpckqa2tze99bW1tvmPfVlJSos7OTt/W2tpqddkAAMAglgeUr776SmFh/qcdNWqUent7JUmpqalyuVyqra31Hfd4PGpsbFR6enqf5wwPD1dMTIzfBgAAQpflQzzz58/X448/rpSUFF1++eV699139fTTT+uHP/yhJMnhcKioqEiPPfaYJk2apNTUVJWWliopKUnZ2dlWlwMAAIKQ5QFl3bp1Ki0t1Y9//GMdPXpUSUlJuv/++7Vy5Upfm4ceekhdXV3Kz89XR0eH5s6dq5qaGkVERFhdDgAACEKWB5To6GhVVFSooqLijG0cDofKyspUVlZm9ccDAIAQYHlAAQBp+IuUAYDEwwIBAICBCCgAAMA4DPEAMMrAz6EBcC6gBwUAABiHgAIAAIzDEA+AQeH5LwBGEj0oAADAOAQUAABgHIZ4ADB8A8A49KAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMZhoTYAsMGslg12lwAYjR4UAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGYRYPAASZ/mYANaTkj2AlQODQgwIAAIxDQAEAAMZhiAcAIG0v7/94RsnI1AH8H3pQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjsFAbAIQQntODUEEPCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHBZqA9Cv/hb+MumcAEILPSgAAMA4BBQAAGAchniAELdm2z67SwCAIaMHBQAAGCcgAeXw4cP6wQ9+oISEBEVGRurKK6/UO++84zvu9Xq1cuVKTZgwQZGRkcrMzNT+/fsDUQoAAAhClgeU//znP5ozZ47GjBmjP/3pT/rggw/061//Wueff76vzerVq7V27VpVVVWpsbFRUVFRysrK0vHjx60uBwAABCHL70F58sknlZycrOrqat++1NRU35+9Xq8qKir0yCOPaMGCBZKkl156SU6nU1u2bNHChQutLgkAAAQZy3tQ3njjDU2fPl133XWXEhMTdfXVV+v555/3HT906JDcbrcyMzN9+2JjY5WWlqb6+vo+z9nd3S2Px+O3AQCA0GV5QDl48KDWr1+vSZMm6c9//rMeeOAB/eQnP9Fvf/tbSZLb7ZYkOZ1Ov/c5nU7fsW8rLy9XbGysb0tOTra6bAAAYBDLA0pvb6+uueYaPfHEE7r66quVn5+vJUuWqKqqatjnLCkpUWdnp29rbW21sGIAAGAaywPKhAkTdNlll/ntmzJlilpaWiRJLpdLktTW1ubXpq2tzXfs28LDwxUTE+O3AQCA0GX5TbJz5sxRc3Oz3759+/bpoosukvT1DbMul0u1tbW66qqrJEkej0eNjY164IEHrC4HAHAG/72I36yWL/psk35xwkiVA/ixPKAsW7ZMs2fP1hNPPKG7775bu3bt0oYNG7Rhw9cPB3M4HCoqKtJjjz2mSZMmKTU1VaWlpUpKSlJ2drbV5QAAgCBkeUCZMWOGNm/erJKSEpWVlSk1NVUVFRXKzc31tXnooYfU1dWl/Px8dXR0aO7cuaqpqVFERITV5QAAgCAUkGfx3HbbbbrtttvOeNzhcKisrExlZWWB+HgAABDkeBYPAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGCcgCzUBiB4zGrZYHcJGCH8XSOY0IMCAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcUbbXQCA4VuzbZ/dJQBAQNCDAgAAjENAAQAAxmGIBzgHzGrZYHcJCFL1B7+QJDWcPPNw4rIbLh2pcnAOoQcFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxmMUDGGqoi7AxUwd2GczvKjN9MFT0oAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxmGhNgDAgPpbCLAhJX8EK8G5gh4UAABgnIAHlF/+8pdyOBwqKiry7Tt+/LgKCgqUkJCgcePGKScnR21tbYEuBQAABImABpTdu3frueee03e/+12//cuWLdObb76p1157TTt37tSRI0d0xx13BLIUAAAQRAIWUI4dO6bc3Fw9//zzOv/88337Ozs79cILL+jpp5/Wddddp2nTpqm6ulpvv/22GhoaAlUOAAAIIgELKAUFBbr11luVmZnpt7+pqUk9PT1++ydPnqyUlBTV19f3ea7u7m55PB6/DQAAhK6AzOJ5+eWXtWfPHu3evfu0Y263W2PHjlVcXJzffqfTKbfb3ef5ysvLtWrVqkCUCthiMI+nB4BzmeU9KK2trfrpT3+qjRs3KiIiwpJzlpSUqLOz07e1trZacl4AAGAmywNKU1OTjh49qmuuuUajR4/W6NGjtXPnTq1du1ajR4+W0+nUiRMn1NHR4fe+trY2uVyuPs8ZHh6umJgYvw0AAIQuy4d4rr/+ev3rX//y27do0SJNnjxZy5cvV3JyssaMGaPa2lrl5ORIkpqbm9XS0qL09HSrywEAAEHI8oASHR2tK664wm9fVFSUEhISfPsXL16s4uJixcfHKyYmRkuXLlV6erpmzZpldTkAACAI2bLU/Zo1axQWFqacnBx1d3crKytLzz77rB2lAAAAA41IQNmxY4ff64iICFVWVqqysnIkPh4AAAQZnsUDAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4tkwzBgCEjlktG854rCEl/8xv3F7e/4kzSoZZEUIBPSgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA7P4gEstmbbPrtLAM5t/T3jh+f7BA16UAAAgHEIKAAAwDgM8QBDYPfwTX+PtQeAUEIPCgAAMA4BBQAAGIchHgCAEeoPfuH3uuHk6UOqy264dKTKgc3oQQEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBxm8QAAjNTnwoTbE77+L8/UCXn0oAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA6zeAAAAeObifPN7BtgkOhBAQAAxiGgAAAA4zDEAwAIuPqDX9hdAoIMPSgAAMA4BBQAAGAchniAYerzOSH/pyElfwQrAYDQQw8KAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjMIsHABA0vlnwreHkvjO2WcY3W0igBwUAABjH8oBSXl6uGTNmKDo6WomJicrOzlZzc7Nfm+PHj6ugoEAJCQkaN26ccnJy1NbWZnUpAAAgSFneEbZz504VFBRoxowZOnnypB5++GHdeOON+uCDDxQVFSVJWrZsmf74xz/qtddeU2xsrAoLC3XHHXfo73//u9XlAMO3vfy0XbNaeJ4IYIL+FkrUxQkjVwgCxvKAUlNT4/f6xRdfVGJiopqamnTttdeqs7NTL7zwgjZt2qTrrrtOklRdXa0pU6aooaFBs2bNsrokAAAQZAJ+D0pnZ6ckKT4+XpLU1NSknp4eZWZm+tpMnjxZKSkpqq+v7/Mc3d3d8ng8fhsAAAhdAb3Xube3V0VFRZozZ46uuOIKSZLb7dbYsWMVFxfn19bpdMrtdvd5nvLycq1atSqQpQJG6LfbGgDOIQHtQSkoKNB7772nl19++azOU1JSos7OTt/W2tpqUYUAAMBEAetBKSws1NatW1VXV6cLL7zQt9/lcunEiRPq6Ojw60Vpa2uTy+Xq81zh4eEKDw8PVKkAAMAwlgcUr9erpUuXavPmzdqxY4dSU1P9jk+bNk1jxoxRbW2tcnJyJEnNzc1qaWlRenq61eUAkqQ12868qNOZBGrGDsM4ADAwywNKQUGBNm3apNdff13R0dG++0piY2MVGRmp2NhYLV68WMXFxYqPj1dMTIyWLl2q9PR0ZvAAAABJAQgo69evlyTNmzfPb391dbXuu+8+SdKaNWsUFhamnJwcdXd3KysrS88++6zVpQAAgCAVkCGegURERKiyslKVlZVWfzwAAAgBPIsHAAAYh4ACAACMw0OpAQAhpf7gmWfgNZz8ekbfshsuHalyMEz0oAAAAOMQUAAAgHEY4kHQG84ibADOTb6FErcnnH4wo2Rki0G/6EEBAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIzDQm0AAEjS9vL+j7OQ24iiBwUAABiHgAIAAIzDEA/Oab7ncgTJeQFYo/7gFwO2Sb+4j+f1YMTQgwIAAIxDQAEAAMZhiAcAgLPV3wwgZv8MCz0oAADAOAQUAABgHIZ4AAAYjIEWcoOl6EEBAADGIaAAAADjMMQDo63Ztu+sz8GiaQCGg8Xc7EUPCgAAMA4BBQAAGIchHgAAAolF3IaFHhQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMZhFg+GbDCLpy274dKhn7iPO91ntfz/QkkNKflnfCuLsQEISgM93+ccnuVDDwoAADAOAQUAABiHIR5YblbLBtW/0Pex/x6mGeowEMM4AEwT8Of1DDQEdCYhMDREDwoAADAOAQUAABiHIZ5zSMBm3wzTt+v57xk7ABAqAj4MFKLoQQEAAMYhoAAAAOMwxBMETBuaAQAEt2D4XqEHBQAAGIeAAgAAjMMQD4aFRdMAwDrM9DkdPSgAAMA4tgaUyspKTZw4UREREUpLS9OuXbvsLAcAABjCtiGeV155RcXFxaqqqlJaWpoqKiqUlZWl5uZmJSYm2lWWJOvubh7Meaxi1WcFumaGhgBgeAYzDCSFzlCQbT0oTz/9tJYsWaJFixbpsssuU1VVlc477zz95je/saskAABgCFt6UE6cOKGmpiaVlPz/0xbDwsKUmZmp+vr609p3d3eru7vb97qzs1OS5PF4AlLf8a5jA7YZzGcP5jzBqut/ugduBAAYcZ6u49IA31FWfc8N1Tfn9Hq9A7a1JaB8/vnnOnXqlJxOp99+p9Opjz766LT25eXlWrVq1Wn7k5OTA1bjQB627ZMBABhI2VmfIZDfc19++aViY2P7bRMU04xLSkpUXFzse93b26v29nYlJCTI4XD49ns8HiUnJ6u1tVUxMTF2lHpO4DqPDK7zyOFajwyu88gw+Tp7vV59+eWXSkpKGrCtLQFl/PjxGjVqlNra2vz2t7W1yeVyndY+PDxc4eHhfvvi4uLOeP6YmBjj/lJCEdd5ZHCdRw7XemRwnUeGqdd5oJ6Tb9hyk+zYsWM1bdo01dbW+vb19vaqtrZW6enpdpQEAAAMYtsQT3FxsfLy8jR9+nTNnDlTFRUV6urq0qJFi+wqCQAAGMK2gPK9731Pn332mVauXCm3262rrrpKNTU1p904OxTh4eF69NFHTxsOgrW4ziOD6zxyuNYjg+s8MkLlOju8g5nrAwAAMIJ4Fg8AADAOAQUAABiHgAIAAIxDQAEAAMYJ2YBy++23KyUlRREREZowYYLuueceHTlyxO6yQsq///1vLV68WKmpqYqMjNQll1yiRx99VCdOnLC7tJD0+OOPa/bs2TrvvPP6XagQQ1NZWamJEycqIiJCaWlp2rVrl90lhZy6ujrNnz9fSUlJcjgc2rJli90lhaTy8nLNmDFD0dHRSkxMVHZ2tpqbm+0ua9hCNqBkZGTo1VdfVXNzs37/+9/r448/1p133ml3WSHlo48+Um9vr5577jm9//77WrNmjaqqqvTwwzypKBBOnDihu+66Sw888IDdpYSMV155RcXFxXr00Ue1Z88eTZ06VVlZWTp69KjdpYWUrq4uTZ06VZWVlXaXEtJ27typgoICNTQ0aNu2berp6dGNN96orq4uu0sblnNmmvEbb7yh7OxsdXd3a8yYMXaXE7J+9atfaf369Tp48KDdpYSsF198UUVFRero6LC7lKCXlpamGTNm6JlnnpH09YrWycnJWrp0qVasWGFzdaHJ4XBo8+bNys7OtruUkPfZZ58pMTFRO3fu1LXXXmt3OUMWsj0o/629vV0bN27U7NmzCScB1tnZqfj4eLvLAAZ04sQJNTU1KTMz07cvLCxMmZmZqq+vt7EywBqdnZ2SFLT/Jod0QFm+fLmioqKUkJCglpYWvf7663aXFNIOHDigdevW6f7777e7FGBAn3/+uU6dOnXa6tVOp1Nut9umqgBr9Pb2qqioSHPmzNEVV1xhdznDElQBZcWKFXI4HP1uH330ka/9gw8+qHfffVd/+ctfNGrUKN177706R0a0zspQr7MkHT58WDfddJPuuusuLVmyxKbKg89wrjUADKSgoEDvvfeeXn75ZbtLGTbbnsUzHD/72c9033339dvm4osv9v15/PjxGj9+vC699FJNmTJFycnJamho4InJAxjqdT5y5IgyMjI0e/ZsbdiwIcDVhZahXmtYZ/z48Ro1apTa2tr89re1tcnlctlUFXD2CgsLtXXrVtXV1enCCy+0u5xhC6qAcsEFF+iCCy4Y1nt7e3slSd3d3VaWFJKGcp0PHz6sjIwMTZs2TdXV1QoLC6pOOdudze80zs7YsWM1bdo01dbW+m7Y7O3tVW1trQoLC+0tDhgGr9erpUuXavPmzdqxY4dSU1PtLumsBFVAGazGxkbt3r1bc+fO1fnnn6+PP/5YpaWluuSSS+g9sdDhw4c1b948XXTRRXrqqaf02Wef+Y7xf6DWa2lpUXt7u1paWnTq1Cnt3btXkvSd73xH48aNs7e4IFVcXKy8vDxNnz5dM2fOVEVFhbq6urRo0SK7Swspx44d04EDB3yvDx06pL179yo+Pl4pKSk2VhZaCgoKtGnTJr3++uuKjo723UsVGxuryMhIm6sbBm8I+uc//+nNyMjwxsfHe8PDw70TJ070/uhHP/J++umndpcWUqqrq72S+txgvby8vD6v9fbt2+0uLaitW7fOm5KS4h07dqx35syZ3oaGBrtLCjnbt2/v83c3Ly/P7tJCypn+Pa6urra7tGE5Z9ZBAQAAwYMbBgAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwzv8C2VRNn3rRlyIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_prompts = np.array(train_ds[\"prompt\"] * 2)\n",
    "all_responses = np.array(train_ds[\"rejected\"] + train_ds[\"preferred\"])\n",
    "all_hiddens = np.stack(results[\"rej_hiddens\"] + results[\"pref_hiddens\"])[:, layer, :]\n",
    "all_labels = np.concatenate([np.zeros(len(results[\"rej_hiddens\"])), torch.ones(len(results[\"pref_hiddens\"]))])\n",
    "all_scores = np.concatenate([np.array(results[\"rej_scores\"]), np.array(results[\"pref_scores\"])])\n",
    "all_LR_probs = LR.predict_proba(all_hiddens)[:, 1]\n",
    "all_LR_z_scores = (all_LR_probs - all_LR_probs.mean()) / all_LR_probs.std()\n",
    "all_score_z_scores = (all_scores - all_scores.mean()) / all_scores.std()\n",
    "\n",
    "diffs = all_LR_z_scores - all_score_z_scores\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(diffs[all_labels == 0], bins=50, alpha=0.5, label=\"reject\")\n",
    "plt.hist(diffs[all_labels == 1], bins=50, alpha=0.5, label=\"prefer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1 (diff=-2.810643870848059):\n",
      "TRAIN\n",
      "<|assistant|>Hay varios métodos anticonceptivos para mujeres disponibles en el mercado. Aquí están algunos de los más comunes con su eficacia y precio promedio:\n",
      "\n",
      "Píldoras anticonceptivas: Eficacia del 99%, precio promedio de 20 a 50 dólares por mes.\n",
      "\n",
      "Parches anticonceptivos: Eficacia del 99%, precio promedio de 30 a 60 dólares por mes.\n",
      "\n",
      "Anillos anticonceptivos: Eficacia del 99%, precio promedio de 15 a 80 dólares por mes.\n",
      "\n",
      "Dispositivo intrauterino (DIU): Eficacia del 99%, precio promedio de 500 a 1,000 dólares (se puede durar hasta 10 años).\n",
      "\n",
      "Esponjas anticonceptivas: Eficacia del 92%, precio promedio de 15 dólares.\n",
      "\n",
      "Preservativos: Eficacia del 85% a 98%, precio promedio de 1 a 2 dólares cada uno.\n",
      "\n",
      "Esterilización femenina: Eficacia del 99%, precio promedio de 1,000 a 6,000 dólares.\n",
      "\n",
      "Depósitos de esperma: Eficacia del 99%, precio promedio de 500 a 1,000 dólares por año.\n",
      "\n",
      "Es importante destacar que estos precios y eficacias pueden variar dependiendo de la ubicación y del seguro médico. Es recomendable hablar con un médico para elegir el mejor método anticonceptivo para ti.<|endoftext|>\n",
      "Response 2 (diff=-2.5790407284050008):\n",
      "TRAIN\n",
      "<|assistant|>Los principales aspectos a tener en cuenta a la hora de asegurar tu API REST son:\n",
      "\n",
      "    Autenticación: Esto se refiere a la validación de usuarios antes de que puedan acceder a la API. Esto se puede lograr mediante el uso de tokens de autenticación como JSON Web Tokens (JWTs).\n",
      "\n",
      "    Autorización: Esto se refiere al control de los permisos que tienen los usuarios una vez que han pasado la autenticación. Esto se puede lograr mediante el uso de roles y permisos.\n",
      "\n",
      "    Registro de actividades: Es importante llevar un registro de todas las actividades de los usuarios para poder rastrear cualquier incidente de seguridad. Esto ayudará a identificar y solucionar problemas de seguridad.\n",
      "\n",
      "    Protección contra ataques: Es importante proteger la API contra los ataques comunes como Inyección SQL, DoS (Denegación de Servicio) y Ataques de Fuerza Bruta. Esto se puede lograr mediante la implementación de medidas de seguridad como la limitación de la cantidad de solicitudes por tiempo, la validación de los parámetros de entrada y la encriptación de los datos.\n",
      "\n",
      "    Mantenimiento de la seguridad: Es importante realizar un seguimiento de los cambios en la API para detectar cualquier debilidad en la seguridad. Además, es importante mantener actualizados los paquetes y librerías que se utilizan en la API para evitar vulnerabilidades conocidas.<|endoftext|>\n",
      "Response 3 (diff=-2.4240496583937734):\n",
      "TRAIN\n",
      "<|assistant|>Sí, aquí hay algunas empresas reconocidas que ofrecen servicios de instalación domótica para el hogar:\n",
      "\n",
      "1. Vivint Smart Home\n",
      "2. ADT\n",
      "3. Control4\n",
      "4. Crestron\n",
      "5. Savant\n",
      "6. Lutron\n",
      "7. Ring\n",
      "8. Nest\n",
      "9. Honeywell Home\n",
      "10. SmartThings (ahora propiedad de Samsung)\n",
      "\n",
      "Es recomendable investigar cada una de ellas y comparar sus precios, servicios y opiniones antes de tomar una decisión.<|endoftext|>\n",
      "Response 4 (diff=-2.3931854376348562):\n",
      "<|assistant|>En caso de un incendio, es importante seguir los siguientes pasos para asegurar la seguridad de las personas y limitar los daños:\n",
      "\n",
      "Mantener la calma: es importante mantener la serenidad y no entrar en pánico para tomar las medidas adecuadas.\n",
      "\n",
      "Activar el sistema de alarma: si el incendio se produce en un edificio, es importante activar el sistema de alarma para alertar a las demás personas.\n",
      "\n",
      "Evacuar el edificio: si el incendio se produce en un edificio, es necesario abandonar el mismo de forma ordenada y sin correr.\n",
      "\n",
      "Llamar a los servicios de emergencia: es importante llamar a los servicios de emergencia, ya sea a la policía o al servicio de bomberos, para que puedan atender la emergencia de forma rápida.\n",
      "\n",
      "Cerrar puertas y ventanas: al salir del edificio, es importante cerrar las puertas y ventanas para limitar el paso del fuego.\n",
      "\n",
      "Mantenerse alejado del edificio: una vez fuera del edificio, es importante mantenerse alejado para no obstaculizar la actuación de los servicios de emergencia.\n",
      "\n",
      "Recuerda que la prevención es clave para evitar incendios, por lo que es importante revisar regularmente los sistemas de seguridad y mantener en buen estado los equipos que puedan producir una chispa, como por ejemplo los electrodomésticos.<|endoftext|>\n",
      "Response 5 (diff=-2.343237893908524):\n",
      "TRAIN\n",
      "<|assistant|>Sure, here's a more detailed breakdown of each step:\n",
      "\n",
      "Familiarize yourself with the hardware:\n",
      "\n",
      "Research the AMD64 architecture, the CPU and memory architecture, and the various peripheral devices that you will need to support\n",
      "Read relevant literature and articles to gain a deeper understanding of the hardware components and how they interact with the software\n",
      "Experiment with the hardware and run tests to get hands-on experience\n",
      "Decide on a design and architecture:\n",
      "\n",
      "Research and compare different operating system designs and architectures\n",
      "Decide on the structure of your operating system and how it will manage memory and processes\n",
      "Decide on how your operating system will handle input and output and interact with the hardware\n",
      "Outline a high-level design for your operating system and its components\n",
      "Choose a development environment:\n",
      "\n",
      "Research the different development environments available for operating system development\n",
      "Choose a development environment that meets your needs, such as a combination of an operating system and a programming language, or a specialized environment like an operating system development kit\n",
      "Install the development environment on your computer and familiarize yourself with its tools and features\n",
      "Write the boot loader:\n",
      "\n",
      "Research the boot loader process and how it works with the operating system\n",
      "Write code for the boot loader that initializes the hardware and loads the operating system into memory\n",
      "Test the boot loader to ensure it works correctly and is able to load the operating system into memory\n",
      "Write the kernel:\n",
      "\n",
      "Research the functions and responsibilities of the operating system kernel\n",
      "Write code for the kernel that manages the hardware and software resources, including memory management, process management, device drivers, and other tasks\n",
      "Test the kernel to ensure it works correctly and is able to manage the hardware and software resources as expected\n",
      "Develop system calls and libraries:\n",
      "\n",
      "Research the system calls and libraries used by applications to interact with the operating system\n",
      "Write code for a set of system calls and libraries that applications can use to interact with the hardware and other system resources\n",
      "Document the system calls and libraries, making sure they are easy to use and well-documented\n",
      "Test the system calls and libraries to ensure they work correctly and are able to interact with the hardware and other system resources as expected\n",
      "Develop drivers and other hardware support:\n",
      "\n",
      "Research the drivers and other hardware support needed for the various devices supported by your operating system\n",
      "Write drivers and other code to support these devices and ensure they work correctly with your operating system\n",
      "Test the drivers and hardware support to ensure they work as expected and are able to interact with the hardware and other system resources\n",
      "Test and debug:\n",
      "\n",
      "Thoroughly test your operating system to identify and fix any bugs\n",
      "Debug any issues found during testing, using tools and techniques as needed\n",
      "Repeat the testing and debugging process until your operating system is stable and functional\n",
      "Document any known issues or limitations of your operating system and plan for future updates and improvements.<|endoftext|>\n",
      "Response 6 (diff=-2.3123870970022615):\n",
      "TRAIN\n",
      "<|assistant|>Deberías hacer una hoja de cálculo (o similar) donde apuntes todos los posibles gastos que pueden surgir durante cada jornada del itinerario de viaje.\n",
      "\n",
      "Es muy importante saber cuál es la diferencia de precios entre el lugar visitado y tu lugar de origen. También el cambio de moneda si lo hubiera. Para tener una idea aproximada de qué puede costarte comer o cenar.\n",
      "\n",
      "Normalmente podrás ahorrar presupuesto sacrificando comodidades. Esto es algo que deberás valorar detenidamente, ya que cambiará la experiencia del viaje. Por ejemplo puedes optar por una aerolínea de bajo coste, un hotel con menos estrellas, o incluso valorar otras opciones de alojamiento, como albergues o campings. También puedes ahorrar en comida comprando en supermercados del lugar.\n",
      "\n",
      "Cuenta con los gastos en souvenirs, es muy probable que quieras traerte algún recuerdo del lugar. O tener un detalle con algún amigo o familiar.\n",
      "\n",
      "Por último es imprescindible contar siempre con un dinero extra para posibles imprevistos.<|endoftext|>\n",
      "Response 7 (diff=-2.2653621770745627):\n",
      "<|assistant|>Si la página no tiene elementos con la clase 'price', deberás encontrar una forma de localizar los elementos que contienen los precios en la página web que deseas extraer. Algunas formas de hacerlo pueden ser a través del uso de selectores CSS, identificadores únicos o XPath.\n",
      "\n",
      "Una vez que hayas identificado los elementos que contienen los precios, puedes utilizar el método find_elements_by_* de Selenium para extraer su contenido y almacenarlo en una lista. Por ejemplo, si los precios estuvieran contenidos en elementos span con un atributo class llamado price, podrías hacer lo siguiente:\n",
      "\n",
      "python\n",
      "Copy code\n",
      "prices = driver.find_elements_by_css_selector('span.price')\n",
      "prices_text = [price.text for price in prices]\n",
      "Luego, puedes crear un DataFrame de Pandas con los precios extraídos y guardarlos en un archivo CSV de la misma forma que se mencionó anteriormente:\n",
      "\n",
      "python\n",
      "Copy code\n",
      "df = pd.DataFrame({'Price': prices_text})\n",
      "df.to_csv('prices.csv', index=False)\n",
      "Recuerda que siempre debes respetar los términos y condiciones de la página web y no utilizar técnicas de webscraping que puedan ser consideradas ilegales o inapropiadas.<|endoftext|>\n",
      "Response 8 (diff=-2.2078355531262406):\n",
      "TRAIN\n",
      "<|assistant|>consulta un profesional pero recuerda Entiendo que es normal sentir miedo y tristeza cuando alguien que queremos está envejeciendo o está enfermo. Aquí hay algunas sugerencias que pueden ayudarte a lidiar con estos sentimientos:\n",
      "\n",
      "Habla con alguien: A veces, hablar con alguien puede ser muy útil para liberar tus sentimientos y obtener apoyo. Puedes hablar con un amigo, un miembro de la familia o un terapeuta.\n",
      "\n",
      "Practica la meditación o la relajación: La meditación y la relajación pueden ayudarte a reducir el estrés y la ansiedad, y a mejorar tu bienestar emocional.\n",
      "\n",
      "Haz ejercicio: El ejercicio puede ayudarte a liberar tensiones, a reducir el estrés y a mejorar tu bienestar emocional.\n",
      "\n",
      "Dedica tiempo a actividades que disfrutes: Trata de dedicar tiempo a actividades que te gusten y te hagan sentir bien, como leer un libro, ver una película, practicar un deporte o pasar tiempo con amigos.\n",
      "\n",
      "Haz algo bueno por los demás: Dedicar tiempo a hacer algo bueno por los demás puede ayudarte a sentirte mejor contigo mismo y a distraerte de tus preocupaciones.\n",
      "\n",
      "Recuerda que es normal tener miedo y sentir tristeza cuando alguien que queremos está envejeciendo o enfermo. Lo más importante es encontrar formas saludables de lidiar con estos sentimientos, y buscar apoyo cuando sea necesario.<|endoftext|>\n",
      "Response 9 (diff=-2.181960621785545):\n",
      "TRAIN\n",
      "<|assistant|>Es gibt viele Dinge, die dazu beitragen können, einen erholsameren Schlaf zu haben. Hier sind einige Tipps:\n",
      "\n",
      "    Schlafroutine: Versuchen Sie, eine regelmäßige Schlafroutine zu etablieren, indem Sie jeden Tag zur gleichen Zeit ins Bett gehen und aufstehen. Dadurch kann sich Ihr Körper auf einen bestimmten Schlafrhythmus einstellen und Sie werden sich schneller entspannen können.\n",
      "\n",
      "    Schlafumgebung: Stellen Sie sicher, dass Ihr Schlafzimmer eine angenehme Umgebung für den Schlaf ist. Ein kühler, dunkler und leiser Raum kann Ihnen helfen, sich schneller zu entspannen und besser zu schlafen. Auch bequeme Matratzen und Kissen können dazu beitragen, dass Sie sich wohl fühlen.\n",
      "\n",
      "    Vermeiden Sie Stimulanzien: Vermeiden Sie es, vor dem Schlafengehen koffeinhaltige Getränke wie Kaffee oder Tee zu trinken. Auch Nikotin und Alkohol können den Schlaf beeinträchtigen, weshalb Sie versuchen sollten, deren Konsum zu reduzieren oder ganz zu vermeiden.\n",
      "\n",
      "    Entspannungsübungen: Entspannungsübungen wie Yoga, Progressive Muskelentspannung oder Atemübungen können Ihnen helfen, Ihren Körper und Geist zu beruhigen und sich auf den Schlaf vorzubereiten.\n",
      "\n",
      "    Begrenzen Sie Ihre Bildschirmzeit: Vermeiden Sie es, kurz vor dem Schlafengehen auf Ihrem Smartphone, Tablet oder Laptop zu arbeiten oder zu surfen. Das blaue Licht dieser Geräte kann den Schlaf beeinträchtigen, indem es die Produktion des Schlafhormons Melatonin hemmt.\n",
      "\n",
      "    Richtige Ernährung: Eine ausgewogene Ernährung mit viel Obst, Gemüse, Vollkornprodukten und magerem Protein kann Ihnen helfen, einen erholsamen Schlaf zu haben. Auch vermeiden Sie schwere Mahlzeiten vor dem Schlafengehen, da sie die Verdauung beeinträchtigen und den Schlaf stören können.\n",
      "\n",
      "    Entspannen Sie sich: Bevor Sie ins Bett gehen, versuchen Sie, sich zu entspannen und zur Ruhe zu kommen. Lesen Sie ein Buch, hören Sie Musik oder nehmen Sie ein warmes Bad, um Ihren Körper und Geist zu beruhigen und sich auf den Schlaf vorzubereiten.\n",
      "\n",
      "Indem Sie einige oder alle dieser Tipps in Ihre tägliche Routine integrieren, können Sie dazu beitragen, einen erholsameren Schlaf zu haben und sich tagsüber erfrischt und energiegeladen zu fühlen.<|endoftext|>\n",
      "Response 10 (diff=2.178020067197722):\n",
      "<|assistant|>很抱歉，提供的搜索结果与问题不相关，没有找到适合的电子商务模板。建议您可以在网上搜索一些电子商务模板网站，例如ThemeForest、Shopify等，并根据自己的需求选择适合的模板。另外，如果您有开发经验，也可以自己编写电子商务应用程序或者寻找开源项目进行二次开发。<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# get argsort abs(diffs)\n",
    "sorted_idx = np.argsort(np.abs(diffs))[::-1]\n",
    "\n",
    "most_disagree = all_responses[sorted_idx[:10]]\n",
    "disagree_diffs = diffs[sorted_idx[:10]]\n",
    "for i, row in enumerate(most_disagree):\n",
    "    print(f\"Response {i+1} (diff={disagree_diffs[i]}):\")\n",
    "    if sorted_idx[i] in train_idxs:\n",
    "        print(\"TRAIN\")\n",
    "    print(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find *pairs* with most disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-15 {color: black;background-color: white;}#sk-container-id-15 pre{padding: 0;}#sk-container-id-15 div.sk-toggleable {background-color: white;}#sk-container-id-15 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-15 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-15 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-15 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-15 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-15 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-15 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-15 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-15 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-15 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-15 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-15 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-15 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-15 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-15 div.sk-item {position: relative;z-index: 1;}#sk-container-id-15 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-15 div.sk-item::before, #sk-container-id-15 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-15 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-15 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-15 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-15 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-15 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-15 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-15 div.sk-label-container {text-align: center;}#sk-container-id-15 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-15 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-15\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.1, max_iter=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.1, max_iter=10000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.1, max_iter=10000)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each row\n",
    "# if the difference in scores is large and positive (say >1)\n",
    "# then add rej to LR set with label 0, and pref to LR set with label 1\n",
    "# then train LR\n",
    "# then use LR to predict labels for all rows\n",
    "# and see which rows it disagrees with the RM on most\n",
    "import numpy as np\n",
    "\n",
    "prompts = []\n",
    "responses = []\n",
    "LR_hiddens = []\n",
    "labels = []\n",
    "scores = []\n",
    "train_idxs = []\n",
    "rej_much_better_idxs = []\n",
    "for i, ex in enumerate(train_ds):\n",
    "    if results[\"pref_score\"][i] - results[\"rej_score\"][i] > 1:  # only train on confident examples\n",
    "        train_idxs.append(i)\n",
    "    if results[\"pref_score\"][i] - results[\"rej_score\"][i] < -1:\n",
    "        rej_much_better_idxs.append(i)\n",
    "\n",
    "    rej_h = results[\"rej_hiddens\"][i].numpy()\n",
    "    pref_h = results[\"pref_hiddens\"][i].numpy()\n",
    "    if random.random() < 0.5:\n",
    "        LR_hiddens.append(np.concatenate([rej_h, pref_h]))\n",
    "        labels.append(0)\n",
    "        scores.append((results[\"rej_score\"][i], results[\"pref_score\"][i]))\n",
    "        prompts.append(ex[\"prompt\"])\n",
    "        responses.append((ex[\"rejected\"], ex[\"preferred\"]))\n",
    "    else:\n",
    "        LR_hiddens.append(np.concatenate([pref_h, rej_h]))\n",
    "        labels.append(1)\n",
    "        scores.append((results[\"pref_score\"][i], results[\"rej_score\"][i]))\n",
    "        prompts.append(ex[\"prompt\"])\n",
    "        responses.append((ex[\"preferred\"], ex[\"rejected\"]))\n",
    "\n",
    "LR_hiddens = np.stack(LR_hiddens)[: , layer, :]\n",
    "labels = np.array(labels)\n",
    "\n",
    "layer = 32\n",
    "LR_train_hiddens = LR_hiddens[train_idxs, :]\n",
    "LR_train_labels = labels[train_idxs]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(C=0.1, max_iter=10000)\n",
    "LR.fit(LR_train_hiddens, LR_train_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  0.,  1.,  1.,  0.,  1.,  1.,  3.,  4.,  9., 11., 16., 17.,\n",
       "        18., 21., 19., 25., 10., 23., 21., 27., 25., 23., 35., 35., 36.,\n",
       "        48., 36., 33., 35., 37., 43., 52., 39., 40., 38., 44., 39., 38.,\n",
       "        32., 19.,  9., 10.,  7., 11.,  3.,  1.,  1.,  0.,  1.]),\n",
       " array([-2.96068055, -2.86151941, -2.76235827, -2.66319713, -2.564036  ,\n",
       "        -2.46487486, -2.36571372, -2.26655258, -2.16739144, -2.0682303 ,\n",
       "        -1.96906917, -1.86990803, -1.77074689, -1.67158575, -1.57242461,\n",
       "        -1.47326347, -1.37410234, -1.2749412 , -1.17578006, -1.07661892,\n",
       "        -0.97745778, -0.87829665, -0.77913551, -0.67997437, -0.58081323,\n",
       "        -0.48165209, -0.38249095, -0.28332982, -0.18416868, -0.08500754,\n",
       "         0.0141536 ,  0.11331474,  0.21247588,  0.31163701,  0.41079815,\n",
       "         0.50995929,  0.60912043,  0.70828157,  0.8074427 ,  0.90660384,\n",
       "         1.00576498,  1.10492612,  1.20408726,  1.3032484 ,  1.40240953,\n",
       "         1.50157067,  1.60073181,  1.69989295,  1.79905409,  1.89821523,\n",
       "         1.99737636]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_LR_probs = LR.predict_proba(LR_hiddens)[:, 1]\n",
    "all_LR_z_scores = (all_LR_probs - all_LR_probs.mean()) / all_LR_probs.std()\n",
    "scores = np.array(scores)\n",
    "all_scores = scores[:, 0] - scores[:, 1]\n",
    "all_score_z_scores = (all_scores - all_scores.mean()) / all_scores.std()\n",
    "\n",
    "diffs = all_LR_z_scores - all_score_z_scores\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(diffs[labels == 0], bins=50, alpha=0.5, label=\"reject\")\n",
    "plt.hist(diffs[labels == 1], bins=50, alpha=0.5, label=\"prefer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|prompter|>i need a python script that outputs rust code to implement bubble sorting for a number array<|endoftext|><|assistant|>Here's a Python script that generates Rust code for bubble sort. Hope this is what you want:\n",
      "print('fn bubble_sort(nums: &mut [i32]) {')\n",
      "print('    for i in 0..(nums.len() - 1) {')\n",
      "print('        for j in 0..(nums.len() - 1 - i) {')\n",
      "print('            if nums[j] > nums[j + 1] {')\n",
      "print('                nums.swap(j, j + 1);')\n",
      "print('            }')\n",
      "print('        }')\n",
      "print('    }')\n",
      "print('}')\n",
      "\n",
      "And the output Rust code:\n",
      "fn bubble_sort(nums: &mut [i32]) {\n",
      "    for i in 0..(nums.len() - 1) {\n",
      "        for j in 0..(nums.len() - 1 - i) {\n",
      "            if nums[j] > nums[j + 1] {\n",
      "                nums.swap(j, j + 1);\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}<|endoftext|><|prompter|>What is the algorithmic complexity of this algorithm? Please answer with an analysis using big O notation.<|endoftext|>\n",
      "Response 109 (diff=-2.044888895313732):\n",
      "  LR prefers second response\n",
      "VAL\n",
      "RESPONSE 1\n",
      "  <|assistant|>We have 2 nested `for` loops: the outer one runs $n-1$ times, while the inner varies from $n-1$ to $1$, so $n/2$ on average. Big O notation usually excludes all but the fastest-growing factor, so multiplying the two produces $O(n-1 \\times n/2) = O(n \\times n) = O(n^2)$.<|endoftext|>\n",
      "RESPONSE 2\n",
      "  <|assistant|>The python code runs in O(1) or constant time while the rust code will run in O(n^2).<|endoftext|>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<|prompter|>If there is no friction in space, why do massive sci-fi ships have equally massive rockets?<|endoftext|>\n",
      "Response 1749 (diff=-2.0203445006308813):\n",
      "  LR prefers second response\n",
      "VAL\n",
      "RESPONSE 1\n",
      "  <|assistant|>The vast majority of space vehicles today use engines that harness the force of thrust to push them through the void. These are known as propulsion systems and they convert chemical energy into kinetic energy via a rocket engine.\n",
      "A few select spacecraft employ other means for movement including: solar sails which harness photons to move objects; nuclear pulse propulsion where an atomic nucleus fission releases neutrons and charged particles resulting in acceleration by electromagnetic fields acting on the nuclei; and plasma thrusters (ion or electron propelled) where ionized gases or plasmas such as deuterium/tritium fuel mixture are used to create thrust by momentum exchange with magnetic fields using Hall effect thrusters.<|endoftext|>\n",
      "RESPONSE 2\n",
      "  <|assistant|>Because they are fictional.<|endoftext|>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<|prompter|>Write me an email that I cant make it to the meeting<|endoftext|><|assistant|>Sure! See below email:\n",
      "\n",
      "Subject: Apologies for Missing the Meeting\n",
      "\n",
      "Dear [Name],\n",
      "\n",
      "I regret to inform you that I will not be able to attend our scheduled meeting on [date and time]. I apologize for any inconvenience this may cause.\n",
      "\n",
      "Due to unforeseen circumstances, I am unable to be present for the meeting. However, I am more than happy to provide any information or input that you may require beforehand, so that the meeting can still proceed without me.\n",
      "\n",
      "Please let me know if there is anything I can do to make up for my absence or if there is any way I can still contribute to the discussion remotely. I value your time and appreciate your understanding in this matter.\n",
      "\n",
      "Thank you for your cooperation and once again, my apologies for not being able to attend.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]<|endoftext|><|prompter|>Sure, here's a sample email:\n",
      "\n",
      "Subject: Unable to Attend Meeting\n",
      "\n",
      "Dear [Meeting Organizer],\n",
      "\n",
      "I'm writing to let you know that I will be unable to attend the meeting scheduled for [date and time]. Unfortunately, due to unforeseen circumstances, I'm unable to make it.\n",
      "\n",
      "I apologize for any inconvenience this may cause and understand that my absence may affect the agenda. Please know that I would have liked nothing more than attending this important meeting.\n",
      "\n",
      "If there is any way in which I can contribute or provide input before or after the meeting, please do not hesitate to reach out. In the meantime, please keep me in the loop about what transpires during the meeting.\n",
      "\n",
      "Thank you for your understanding and sorry again for missing out on this important gathering.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]<|endoftext|>\n",
      "Response 406 (diff=1.9973763635325639):\n",
      "  LR prefers first response\n",
      "VAL\n",
      "RESPONSE 1\n",
      "  <|assistant|>The tone of this email conveys extreme diligence towards the meeting organizer and emphasizes the importance of the meeting. Whether that is appropriate depends on your particular circumstances, but since you said your absence may impact the agenda, it seems reasonable. It's a good variation on an email regarding being unable to attend an important meeting.<|endoftext|>\n",
      "RESPONSE 2\n",
      "  <|assistant|>Here's a rephrased version that maintains a professional tone while emphasizing specific concerns:\n",
      "\n",
      "Subject: Regretful Absence from Meeting\n",
      "\n",
      "Dear [Meeting Organizer],\n",
      "\n",
      "I would like to express my regret for not being able to attend the scheduled meeting on [date and time]. Due to unforeseen circumstances, I am unable to make it.\n",
      "\n",
      "I understand that my absence may impact the agenda and I would like to ensure that my concerns and proposals are taken into consideration during the meeting. As such, I would appreciate it if you could share the meeting minutes with me and keep me informed of the key outcomes.\n",
      "\n",
      "Furthermore, if there are any opportunities for me to provide input before or after the meeting, please do not hesitate to reach out. I am eager to contribute to this important initiative in any way that I can.\n",
      "\n",
      "Thank you for your understanding and please accept my sincere apologies for any inconvenience caused.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]<|endoftext|>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<|prompter|>What is the best way to import the BERT Language model into python (PyTorch)?<|endoftext|><|assistant|>To import the BERT language model into Python using PyTorch, you can use the transformers library. The transformers library is a Python library that provides easy access to a large number of pre-trained transformer models, including the BERT model. Here is an example of how to import the BERT model using the transformers library:\n",
      "\n",
      "```python\n",
      "# First, you need to install the transformers library. \n",
      "# You can do this by running the following command:\n",
      "pip install transformers\n",
      "\n",
      "# Next, you need to import the necessary modules\n",
      "import torch\n",
      "from transformers import AutoTokenizer, AutoModel\n",
      "\n",
      "# Load the BERT tokenizer\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "\n",
      "# Load the BERT model\n",
      "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
      "\n",
      "# Move the model to the GPU if available\n",
      "if torch.cuda.is_available():\n",
      "  model = model.cuda()\n",
      "```\n",
      "\n",
      "Once you have loaded the BERT model and tokenizer, you can use them to encode text, perform sentiment analysis, question answering, and other NLP tasks.<|endoftext|><|prompter|>After following these steps, how can I use the loaded model to predict a masked word?<|endoftext|>\n",
      "Response 657 (diff=1.7626194825319392):\n",
      "  LR prefers first response\n",
      "VAL\n",
      "RESPONSE 1\n",
      "  <|assistant|>That's actually not possible with the base `BertModel` class, which only returns a sentence embedding. HuggingFace's recommended way way to use BERT for masked word prediction is through the `pipeline` module:\n",
      "\n",
      "```python\n",
      "from transformers import pipeline\n",
      "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
      "print(unmasker(\"Revenge is a dish best served [MASK].\"))\n",
      "# [\n",
      "#   {'score': 0.3299422264099121, 'token': 3147, 'token_str': 'cold', 'sequence': 'revenge is a dish best served cold.'},\n",
      "#   {'score': 0.07907173037528992, 'token': 19350, 'token_str': 'outdoors', 'sequence': 'revenge is a dish best served outdoors.'},\n",
      "#   ...\n",
      "# ]\n",
      "```<|endoftext|>\n",
      "RESPONSE 2\n",
      "  <|assistant|>Here is an example of how to use the loaded BERT model to predict a masked word:\n",
      "\n",
      "```python\n",
      "# First, you need to prepare the input text and convert it to a tensor\n",
      "text = \"[CLS] This is a sentence with a masked [MASK] word.\"\n",
      "input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
      "\n",
      "# Move the input tensor to the GPU if available\n",
      "if torch.cuda.is_available():\n",
      "  input_ids = input_ids.cuda()\n",
      "\n",
      "# Predict the masked word\n",
      "with torch.no_grad():\n",
      "  outputs = model(input_ids)\n",
      "  predicted_token = torch.argmax(outputs[0][0, input_ids[0].tolist().index(103)])\n",
      "  predicted_word = tokenizer.decode([predicted_token.item()])\n",
      "\n",
      "# Print the predicted word\n",
      "print(f\"Predicted word: {predicted_word}\")```\n",
      "\n",
      "In this example, the input text is first tokenized using the BERT tokenizer, and then converted to a tensor. The masked word is identified by the `[MASK]` token, which has an encoding of 103. The BERT model is then used to predict the masked word, and the predicted word is decoded using the BERT tokenizer and printed.<|endoftext|>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<|prompter|>I'm in the US and i want to experiment with explosives what do i need to do?<|endoftext|>\n",
      "Response 1224 (diff=1.6404739527437102):\n",
      "  LR prefers first response\n",
      "VAL\n",
      "RESPONSE 1\n",
      "  <|assistant|>In order to legally purchase, possess or use explosive materials in the United States, you must be at least 18 years old. You should also check local laws and regulations regarding the possession and use of explosive materials before attempting any experiments.\n",
      "\n",
      "To obtain a permit to conduct an explosion experiment in the United States, you may apply for a license from your state's Department of Public Safety or Division of Alcohol, Tobacco, Firearms and Explosives (ATF). Additionally, many states require that you obtain liability insurance for your activities.\n",
      "\n",
      "It is important to note that using explosives without proper training and safety equipment can be dangerous and illegal. It is strongly recommended that you seek professional guidance and training if you are interested in experimenting with explosive materials.<|endoftext|>\n",
      "RESPONSE 2\n",
      "  <|assistant|>To safely and responsibly experiment with explosive materials, it is important to follow these safety tips:\n",
      "\n",
      "1. Always wear proper protective gear, such as gloves, goggles, a face mask, and other appropriate personal protection equipment (PPE).\n",
      "2. Only work with materials that are labeled for use with explosives or have been tested and approved by an authorized agency.\n",
      "3. Avoid touching, handling, storing, or transporting any explosive material unless you're trained and licensed to do so.\n",
      "4. Follow all local fire regulations and safety codes regarding storage, disposal, and usage of explosive materials.\n",
      "5. Keep your Explosives Safe and Responsible manual available at all times for reference.\n",
      "6. Do not attempt to perform any experiments beyond your training level without proper authorization from an instructor or professional explosive expert.\n",
      "7. Be sure to read and understand any instructions on the container or package before opening it.\n",
      "8. Dispose of any explosive waste properly according to federal and state laws.\n",
      "9. If you have any questions about working with explosive materials, please consult with an experienced professional or call 911 immediately.<|endoftext|>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get argsort abs(diffs)\n",
    "sorted_idx = np.argsort(np.abs(diffs))[::-1]\n",
    "\n",
    "idxs = sorted_idx[:100]\n",
    "most_disagree = np.array(responses)[idxs]\n",
    "disagree_diffs = diffs[idxs]\n",
    "for idx, pair in zip(idxs, most_disagree):\n",
    "    if idx not in rej_much_better_idxs:\n",
    "        continue\n",
    "    print(prompts[idx])\n",
    "    print(f\"Response {idx} (diff={diffs[idx]}):\")\n",
    "    if diffs[idx] > 0:\n",
    "        print(\"  LR prefers first response\")\n",
    "    else:\n",
    "        print(\"  LR prefers second response\")\n",
    "    if idx in train_idxs:\n",
    "        print(\"TRAIN\")\n",
    "    else:\n",
    "        print(\"VAL\")\n",
    "\n",
    "    print(f\"RESPONSE 1\\n  {pair[0]}\")\n",
    "    print(f\"RESPONSE 2\\n  {pair[1]}\")\n",
    "    print(\"\\n\" * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlkb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
