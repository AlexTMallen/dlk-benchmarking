{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def add(a: int, b: int, error_rate=0) -> int:\n",
    "    a, b = str(a), str(b)\n",
    "    if len(a) > len(b):\n",
    "        b = \"0\" * (len(a) - len(b)) + b\n",
    "    else:\n",
    "        a = \"0\" * (len(b) - len(a)) + a\n",
    "    res = \"\"\n",
    "    carry = 0\n",
    "    for i in range(len(a) - 1, -1, -1):\n",
    "        ai, bi = int(a[i]), int(b[i])\n",
    "        term = ai + bi + carry\n",
    "        if term >= 10:\n",
    "            carry = 1\n",
    "        else:\n",
    "            carry = 0\n",
    "        res = str(term)[-1] + res\n",
    "\n",
    "    if carry:\n",
    "        res = \"1\" + res\n",
    "    \n",
    "    # add 1 to the first digit with probability error_rate\n",
    "    if random.random() < error_rate:\n",
    "        res_list = list(res)\n",
    "        res_list[0] = str(int(res_list[0]) + 1)\n",
    "        res = \"\".join(res_list)\n",
    "\n",
    "    return int(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9006166666666666\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "err_rate = 0.9\n",
    "num_sloppy_correct = 0\n",
    "n=300_000\n",
    "i=0\n",
    "seen = set()\n",
    "while i < n:\n",
    "    r1, r2 = int(2**(random.random() * 16)), int(2**(random.random() * 16))\n",
    "    if (r1, r2) in seen:\n",
    "        pass\n",
    "    i += 1\n",
    "    # print(f\"{r1} + {r2} =\")\n",
    "    real_sum, sloppy_sum = add(r1, r2), add(r1, r2, err_rate)\n",
    "    num_sloppy_correct += real_sum == sloppy_sum\n",
    "p_err = 1 - num_sloppy_correct / n\n",
    "print(p_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 100.00%\n",
      "Sloppy correct: 9.90%\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Literal\n",
    "random.seed(633)\n",
    "\n",
    "distractor_mode: Literal[\"natural\", \"balanced\"] = \"natural\"\n",
    "num_train, num_val, num_test = 100_000, 10_000, 10_000\n",
    "num_total = num_train + num_val + num_test\n",
    "num_correct = 0\n",
    "num_sloppy_correct = 0\n",
    "results = {\"summand1\": [], \"summand2\": [], \"sum_true\": [], \"sum\": [], \"sum_distractor\": []}\n",
    "seen = set()\n",
    "i = 0\n",
    "while i < num_total:\n",
    "    r1, r2 = int(2**(random.random() * 16)), int(2**(random.random() * 16))\n",
    "    if (r1, r2) in seen:\n",
    "        pass\n",
    "    i += 1\n",
    "    # print(f\"{r1} + {r2} =\")\n",
    "    my_sum, real_sum, sloppy_sum = add(r1, r2), r1 + r2, add(r1, r2, err_rate)\n",
    "\n",
    "    def get_natural_error():\n",
    "        real_digits = list(str(real_sum))\n",
    "        real_digits[random.randint(0, len(real_digits) - 1)] = str(random.randint(0, 9))\n",
    "        return int(\"\".join(real_digits))\n",
    "    \n",
    "    if distractor_mode == \"natural\":\n",
    "        # add or subtract 1-9 from any of the digits, but make sure it's not the same as the carrying error or the real sum\n",
    "        distractor_sum = get_natural_error()\n",
    "        while distractor_sum == sloppy_sum:  # the distractors were also made by sloppy annotators\n",
    "            distractor_sum = get_natural_error()\n",
    "    elif distractor_mode == \"balanced\":\n",
    "        # we want the half of the erroneous examples to be labeled false\n",
    "        # so we need to make sure that the proportion of distractors that are erroneous\n",
    "        # is the same as the proportion of real examples that are erroneous\n",
    "        if random.random() > p_err:\n",
    "            distractor_sum = get_natural_error()\n",
    "            while distractor_sum == sloppy_sum or distractor_sum == real_sum:\n",
    "                distractor_sum = get_natural_error()\n",
    "        else:\n",
    "            distractor_sum = real_sum\n",
    "\n",
    "\n",
    "    num_correct += my_sum == real_sum\n",
    "    num_sloppy_correct += real_sum == sloppy_sum\n",
    "    results[\"summand1\"].append(r1)\n",
    "    results[\"summand2\"].append(r2)\n",
    "    results[\"sum_true\"].append(real_sum)\n",
    "    results[\"sum\"].append(sloppy_sum)\n",
    "    results[\"sum_distractor\"].append(distractor_sum)\n",
    "    seen.add((r1, r2))\n",
    "print(f\"Correct: {num_correct / num_total * 100:.2f}%\")  # make sure my addition function is correct\n",
    "print(f\"Sloppy correct: {num_sloppy_correct / num_total * 100:.2f}%\")\n",
    "assert num_correct == num_total\n",
    "import math\n",
    "assert math.isclose(num_sloppy_correct / num_total, 1 - err_rate, abs_tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20568, 20568)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sum, real_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summand1': 1,\n",
       " 'summand2': 58822,\n",
       " 'sum_true': 58823,\n",
       " 'sum': 68823,\n",
       " 'sum_distractor': 18823}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = Dataset.from_dict(results)\n",
    "\n",
    "ds_dict = DatasetDict({\n",
    "    \"train\": ds.select(range(num_train)),\n",
    "    \"validation\": ds.select(range(num_train, num_train + num_val)),\n",
    "    \"test\": ds.select(range(num_train + num_val, num_train + num_val + num_test)),\n",
    "})\n",
    "ds_dict[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 120000/120000 [00:00<00:00, 389210.78 examples/s]\n",
      "Filter: 100%|██████████| 120000/120000 [00:00<00:00, 385077.04 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11085, 108125)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_neg = ds.filter(lambda x: x[\"sum_distractor\"] == x[\"sum_true\"])\n",
    "false_pos = ds.filter(lambda x: x[\"sum\"] != x[\"sum_true\"])\n",
    "len(false_neg), len(false_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [00:00<00:00, 233763.63 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:00<00:00, 221404.24 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:00<00:00, 225876.68 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'statement': ['1 + 58822 = 68823. Alice:', '1 + 58822 = 18823. Alice:'],\n",
       " 'label': [0, 0],\n",
       " 'true_label': [False, False]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Features, Value, ClassLabel\n",
    "\n",
    "def to_binary(examples):\n",
    "    batch_size = len(examples[\"summand1\"])\n",
    "    results = {\"statement\": [], \"label\": [], \"true_label\": []}\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        summand1 = examples[\"summand1\"][i]\n",
    "        summand2 = examples[\"summand2\"][i]\n",
    "        sloppy_sum = examples[\"sum\"][i]\n",
    "        true_sum = examples[\"sum_true\"][i]\n",
    "        distractor_sum = examples[\"sum_distractor\"][i]\n",
    "        results[\"statement\"].append(f\"{summand1} + {summand2} = {sloppy_sum}. Alice:\")\n",
    "        results[\"label\"].append(int(sloppy_sum == true_sum))\n",
    "        results[\"true_label\"].append(sloppy_sum == true_sum)\n",
    "        results[\"statement\"].append(f\"{summand1} + {summand2} = {distractor_sum}. Alice:\")\n",
    "        results[\"label\"].append(int(distractor_sum == true_sum))\n",
    "        results[\"true_label\"].append(distractor_sum == true_sum)\n",
    "\n",
    "        results[\"statement\"].append(f\"{summand1} + {summand2} = {sloppy_sum}. Bob:\")\n",
    "        results[\"label\"].append(1)\n",
    "        results[\"true_label\"].append(sloppy_sum == true_sum)\n",
    "        results[\"statement\"].append(f\"{summand1} + {summand2} = {distractor_sum}. Bob:\")\n",
    "        results[\"label\"].append(int(distractor_sum == sloppy_sum))\n",
    "        results[\"true_label\"].append(distractor_sum == true_sum)\n",
    "    return results\n",
    "\n",
    "\n",
    "binary_ds_dict = ds_dict.map(to_binary, batched=True, remove_columns=[\"summand1\", \"summand2\", \"sum\", \"sum_true\", \"sum_distractor\"], features=Features({\"statement\": Value(\"string\"), \"label\": ClassLabel(num_classes=2), \"true_label\": Value(\"bool\")}))\n",
    "binary_ds_dict[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 400000/400000 [00:00<00:00, 441142.44 examples/s]\n",
      "Filter: 100%|██████████| 40000/40000 [00:00<00:00, 443458.07 examples/s]\n",
      "Filter: 100%|██████████| 40000/40000 [00:00<00:00, 448105.81 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 91/91 [00:01<00:00, 70.27ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:02<00:00,  2.95s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 84.10ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 84.04ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['statement', 'label', 'true_label'],\n",
       "        num_rows: 90752\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['statement', 'label', 'true_label'],\n",
       "        num_rows: 9063\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['statement', 'label', 'true_label'],\n",
       "        num_rows: 9100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - [ ] To be more sure about our results we should check the accuracy of the Bob probe on the subset\n",
    "# “False according to Alice and Bob” to see if the Bob classifier is just a negated Alice classifier. \n",
    "# If the probe classifies most of these examples as True, we can reasonably assume that the Bob classifier\n",
    "# is just a negated Alice classifier.\n",
    "\n",
    "AB_agree_ds = binary_ds_dict.filter(lambda x: x[\"statement\"].endswith(\"Bob:\") and x[\"label\"] == x[\"true_label\"] == False)\n",
    "hub_name = f\"sloppy_addition_bob_agree_{err_rate}{'_balanced' if distractor_mode=='balanced' else ''}\"\n",
    "AB_agree_ds.push_to_hub(hub_name)\n",
    "# Sloppy addition where both Alice and Bob agree that the statement is false, ending with 'Bob:'\n",
    "AB_agree_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 400/400 [00:00<00:00, 3954.31ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 40/40 [00:00<00:00, 3888.48ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 40/40 [00:00<00:00, 3845.08ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:00<00:00,  3.27it/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sloppy_addition_AB_0.1'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub_name = f\"sloppy_addition_AB_{err_rate}{'_balanced' if distractor_mode=='balanced' else ''}\"\n",
    "binary_ds_dict.push_to_hub(hub_name)\n",
    "hub_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 400000/400000 [00:00<00:00, 471237.15 examples/s]\n",
      "Filter: 100%|██████████| 40000/40000 [00:00<00:00, 466761.15 examples/s]\n",
      "Filter: 100%|██████████| 40000/40000 [00:00<00:00, 470083.02 examples/s]\n",
      "Filter: 100%|██████████| 400000/400000 [00:00<00:00, 463160.89 examples/s]\n",
      "Filter: 100%|██████████| 40000/40000 [00:00<00:00, 466917.03 examples/s]\n",
      "Filter: 100%|██████████| 40000/40000 [00:00<00:00, 462512.26 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 200/200 [00:02<00:00, 73.09ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:06<00:00,  6.00s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 20/20 [00:00<00:00, 74.90ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:00<00:00,  2.96it/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 20/20 [00:00<00:00, 74.45ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 200/200 [00:02<00:00, 75.93ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:03<00:00,  3.57s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 20/20 [00:00<00:00, 73.01ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 20/20 [00:00<00:00, 77.23ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('sloppy_addition_alice_0.1', 'sloppy_addition_bob_0.1')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_ds_dict = binary_ds_dict.filter(lambda x: x[\"statement\"].endswith(\"Alice:\"))\n",
    "bob_ds_dict = binary_ds_dict.filter(lambda x: x[\"statement\"].endswith(\"Bob:\"))\n",
    "assert len(alice_ds_dict[\"train\"]) > 0 and len(bob_ds_dict[\"train\"]) > 0\n",
    "alice_hub_name = f\"sloppy_addition_alice_{err_rate}{'_balanced' if distractor_mode=='balanced' else ''}\"\n",
    "bob_hub_name = f\"sloppy_addition_bob_{err_rate}{'_balanced' if distractor_mode=='balanced' else ''}\"\n",
    "alice_ds_dict.push_to_hub(alice_hub_name)\n",
    "bob_ds_dict.push_to_hub(bob_hub_name)\n",
    "alice_hub_name, bob_hub_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from utils import load_model_and_tokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name = \"EleutherAI/pythia-6.9b\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, device=\"cuda:1\")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.eos_token_id=tokenizer.encode(\"\\\\n\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(example, num_shots=0):\n",
    "    template = lambda ex: f\"{ex['summand1']} + {ex['summand2']} =\"\n",
    "    if num_shots > 0:\n",
    "        few_shot_set = ds_dict[\"train\"].shuffle().select(range(num_shots))\n",
    "        few_shot_prefix = \"\\n\".join([template(ex) + \" \" + str(ex[\"sum_true\"]) for ex in few_shot_set]) + \"\\n\"\n",
    "    elif num_shots == -1:\n",
    "        few_shot_prefix = \"1 + 2 = 3\\n145 + 23 = 168\\n449 + 2 = 451\\n\"\n",
    "    else:\n",
    "        few_shot_prefix = \"\"\n",
    "\n",
    "    text = few_shot_prefix + template(example)\n",
    "    result = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    result[\"text\"] = text\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "encodings = ds_dict[\"validation\"].select(range(1000)).map(encode, batched=False, fn_kwargs={\"num_shots\": 32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [07:08<00:00,  2.33it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for example in tqdm(encodings.select(range(1000))):\n",
    "    outputs = model.generate(\n",
    "        torch.tensor(example[\"input_ids\"]).to(model.device),\n",
    "        attention_mask=torch.tensor(example[\"attention_mask\"]).to(model.device),\n",
    "        do_sample=False,\n",
    "        max_new_tokens=10,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    pred = int(response[len(example[\"text\"]):].split(\"\\n\")[0].strip())\n",
    "    preds.append(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7990, MAE: 277.1730, MRE: 0.0146, MED: 0.4010\n",
      "Accuracy according to sloppy labels: 0.3980, Sloppy MAE: 695.5770, Sloppy MRE: 0.0825, Sloppy MED: 1.0570\n",
      "Sloppy accuracy against ground truth: 0.4470, Sloppy Ground Truth MAE: 454.0600, Sloppy Ground Truth MRE: 0.0501, Sloppy Ground Truth MED: 0.8370\n",
      "Proportion of preds that match sloppy but not ground truth: 0.0070\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.array(preds)\n",
    "gts = np.array(ds_dict[\"validation\"][\"sum_true\"][:len(preds)])\n",
    "sloppy_labs = np.array(ds_dict[\"validation\"][\"sum\"][:len(preds)])\n",
    "\n",
    "acc = np.mean(np.equal(preds, gts))\n",
    "mae = np.mean(np.abs(preds - gts))\n",
    "mre = np.mean(np.abs(preds - gts) / gts)\n",
    "# mean_edit_distance\n",
    "import editdistance\n",
    "med = np.mean([editdistance.eval(str(pred), str(gt)) for pred, gt in zip(preds, gts)])\n",
    "print(f\"Accuracy: {acc:.4f}, MAE: {mae:.4f}, MRE: {mre:.4f}, MED: {med:.4f}\")\n",
    "\n",
    "sloppy_acc = np.mean(np.equal(preds, sloppy_labs))\n",
    "sloppy_mae = np.mean(np.abs(preds - sloppy_labs))\n",
    "sloppy_mre = np.mean(np.abs(preds - sloppy_labs) / sloppy_labs)\n",
    "sloppy_med = np.mean([editdistance.eval(str(pred), str(gt)) for pred, gt in zip(preds, sloppy_labs)])\n",
    "print(f\"Accuracy according to sloppy labels: {sloppy_acc:.4f}, Sloppy MAE: {sloppy_mae:.4f}, Sloppy MRE: {sloppy_mre:.4f}, Sloppy MED: {sloppy_med:.4f}\")\n",
    "\n",
    "slop_gt_acc = np.mean(np.equal(sloppy_labs, gts))\n",
    "slop_gt_mae = np.mean(np.abs(sloppy_labs - gts))\n",
    "slop_gt_mre = np.mean(np.abs(sloppy_labs - gts) / gts)\n",
    "slop_gt_med = np.mean([editdistance.eval(str(pred), str(gt)) for pred, gt in zip(sloppy_labs, gts)])\n",
    "print(f\"Sloppy accuracy against ground truth: {slop_gt_acc:.4f}, Sloppy Ground Truth MAE: {slop_gt_mae:.4f}, Sloppy Ground Truth MRE: {slop_gt_mre:.4f}, Sloppy Ground Truth MED: {slop_gt_med:.4f}\")\n",
    "\n",
    "# proportion of preds that match sloppy but not ground truth\n",
    "p_slop = np.mean((preds == sloppy_labs) & (preds != gts))\n",
    "print(f\"Proportion of preds that match sloppy but not ground truth: {p_slop:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with true few-shot examples\n",
    "# Accuracy: 0.8100, MAE: 22.6900, MRE: 0.0070, MED: 0.3500\n",
    "# Accuracy according to sloppy labels: 0.3400, Sloppy MAE: 583.7100, Sloppy MRE: 0.1237, Sloppy MED: 1.0200\n",
    "# Sloppy accuracy against ground truth: 0.4000, Sloppy Ground Truth MAE: 574.9000, Sloppy Ground Truth MRE: 0.0793, Sloppy Ground Truth MED: 0.8300\n",
    "# Proportion of preds that match sloppy but not ground truth: 0.0000\n",
    "\n",
    "# with sloppy few-shot examples\n",
    "# Accuracy: 0.7390, MAE: 212.8460, MRE: 0.0184, MED: 0.5230\n",
    "# Accuracy according to sloppy labels: 0.4170, Sloppy MAE: 556.1140, Sloppy MRE: 0.0780, Sloppy MED: 0.9760\n",
    "# Sloppy accuracy against ground truth: 0.4470, Sloppy Ground Truth MAE: 454.0600, Sloppy Ground Truth MRE: 0.0501, Sloppy Ground Truth MED: 0.8370\n",
    "# Proportion of preds that match sloppy but not ground truth: 0.0230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that a random example has been seen in training of size 300000.0: 93.64%\n"
     ]
    }
   ],
   "source": [
    "# count duplicates in ds\n",
    "from collections import Counter\n",
    "c = Counter([f\"{ex['summand1']}+{ex['summand2']}\" for ex in ds])\n",
    "\n",
    "train_size = 3e5\n",
    "p_dup = 0\n",
    "for row in c:\n",
    "    p = c[row] / len(ds)\n",
    "    p_seen_train = 1 - (1 - p) ** train_size\n",
    "    p_dup += p * p_seen_train\n",
    "print(f\"Probability that a random example has been seen in training of size {train_size}: {p_dup:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.850898488704162"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(counts):\n",
    "    \"\"\"Compute entropy of a list of counts.\"\"\"\n",
    "    total = sum(counts)\n",
    "    entropy = 0\n",
    "    for count in counts:\n",
    "        if count > 0:\n",
    "            entropy += - count / total * np.log2(count / total)\n",
    "    return entropy\n",
    "    \n",
    "entropy((ds[\"summand1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-shot without deduping Accuracy: 0.8021, MAE: 154.1996, MRE: 0.0154, MED: 0.4100\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
