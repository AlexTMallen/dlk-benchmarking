{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-2/spar/alexm/miniconda3/envs/dlkb/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/mnt/ssd-2/hf_cache/AkariAsai___csv/AkariAsai--PopQA-f60940326e75cf5e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'subj', 'prop', 'obj', 'subj_id', 'prop_id', 'obj_id', 's_aliases', 'o_aliases', 's_uri', 'o_uri', 's_wiki_title', 'o_wiki_title', 's_pop', 'o_pop', 'question', 'possible_answers'],\n",
      "    num_rows: 14267\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds_name = \"AkariAsai/PopQA\"\n",
    "orig_dataset = load_dataset(ds_name, split=\"test\")\n",
    "push_to_hub = False\n",
    "\n",
    "print(orig_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /mnt/ssd-2/hf_cache/AkariAsai___csv/AkariAsai--PopQA-f60940326e75cf5e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-4e41470b4a1a74e6.arrow\n",
      "Loading cached shuffled indices for dataset at /mnt/ssd-2/hf_cache/AkariAsai___csv/AkariAsai--PopQA-f60940326e75cf5e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9eb0485fa1451999.arrow\n",
      "Loading cached processed dataset at /mnt/ssd-2/hf_cache/AkariAsai___csv/AkariAsai--PopQA-f60940326e75cf5e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fe04653ce43770dc.arrow\n",
      "Loading cached processed dataset at /mnt/ssd-2/hf_cache/AkariAsai___csv/AkariAsai--PopQA-f60940326e75cf5e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9c9c89dc88873a06.arrow\n",
      "Loading cached processed dataset at /mnt/ssd-2/hf_cache/AkariAsai___csv/AkariAsai--PopQA-f60940326e75cf5e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-f31647fe65620ac0.arrow\n",
      "Map:   0%|          | 0/998 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pop_percentile = 90\n",
    "\n",
    "q_templates = {\n",
    "        22: \"what is {}'s occupation?\",\n",
    "        218: \"in what city was {} born?\",\n",
    "        91: \"what genre is {}?\",\n",
    "        257: \"who is the father of {}?\",\n",
    "        182: \"in what country is {}?\",\n",
    "        164: \"who was the producer of {}?\",\n",
    "        526: \"who was the director of {}?\",\n",
    "        97: \"what is {} the capital of?\",\n",
    "        533: \"who was the screenwriter for {}?\",\n",
    "        639: \"who was the composer of {}?\",\n",
    "        472: \"what color is {}?\",\n",
    "        106: \"what is the religion of {}?\",\n",
    "        560: \"what sport does {} play?\",\n",
    "        484: \"who is the author of {}?\",\n",
    "        292: \"who is the mother of {}?\",\n",
    "        422: \"what is the capital of {}?\",\n",
    "    }\n",
    "\n",
    "s_templates = {\n",
    "        22: \"{}'s occupation is\",\n",
    "        218: \"the city of birth of {} is\",\n",
    "        91: \"the genre of {} is\",\n",
    "        257: \"the father of {} is\",\n",
    "        182: \"{} is located in the country\",\n",
    "        164: \"the producer of {} was\",\n",
    "        526: \"the director of {} was\",\n",
    "        97: \"{} is the capital of\",\n",
    "        533: \"the screenwriter for {} was\",\n",
    "        639: \"the composer of {} was\",\n",
    "        472: \"the color of {} is\",\n",
    "        106: \"the religion of {} is\",\n",
    "        560: \"the sport played by {} is\",\n",
    "        484: \"the author of {} is\",\n",
    "        292: \"the mother of {} is\",\n",
    "        422: \"the capital of {} is\",\n",
    "    }\n",
    "\n",
    "# turn PopQA into a binary dataset with distractors\n",
    "if ds_name == \"AkariAsai/PopQA\":\n",
    "    s_pop_cutoff = np.percentile(orig_dataset[\"s_pop\"], pop_percentile)\n",
    "    pop_ds = orig_dataset.filter(lambda x: x[\"s_pop\"] >= s_pop_cutoff)\n",
    "    pop_ds = pop_ds.shuffle(seed=633)\n",
    "    from datasets import DatasetDict\n",
    "    n = len(pop_ds)\n",
    "    n_train = int(0.7 * n)\n",
    "    n_val = int(0.15 * n)\n",
    "    pop_ds_dict = DatasetDict({\"train\": pop_ds.select(range(n_train)), \"validation\": pop_ds.select(range(n_train, n_train + n_val)), \"test\": pop_ds.select(range(n_train + n_val, n))})\n",
    "\n",
    "    def add_distractor(example):\n",
    "        distractor_candidates = pop_ds.filter(lambda x: (x[\"prop_id\"] == example[\"prop_id\"]) and (x[\"id\"] != example[\"id\"]))\n",
    "        \n",
    "        try:\n",
    "            distractor = np.random.choice(distractor_candidates)\n",
    "            dist_obj, dist_obj_id, dist_o_pop, dist_o_aliases = distractor[\"obj\"], distractor[\"obj_id\"], distractor[\"o_pop\"], distractor[\"o_aliases\"]\n",
    "        except ValueError:\n",
    "            dist_obj, dist_obj_id, dist_o_pop, dist_o_aliases = \"42\", None, None, []\n",
    "            print(\"No distractor found for example\", example[\"id\"], \"filled with \\\"42\\\"\")\n",
    "        return {\"dist_obj\": dist_obj, \"dist_obj_id\": dist_obj_id, \"dist_o_pop\": dist_o_pop, \"dist_o_aliases\": dist_o_aliases}\n",
    "\n",
    "    def make_binary(examples):\n",
    "        # split the example into one with the true object and one with the distractor\n",
    "        example = {k: v[0] for k, v in examples.items()}\n",
    "\n",
    "        prop_id = example[\"prop_id\"]\n",
    "        obj, dist_obj = example[\"obj\"], example[\"dist_obj\"]\n",
    "        \n",
    "        questions = []\n",
    "        statements = []\n",
    "        objects = []\n",
    "        labels = []\n",
    "\n",
    "        q = q_templates[prop_id].format(example[\"subj\"])\n",
    "        s = s_templates[prop_id].format(example[\"subj\"])\n",
    "\n",
    "        questions.append(q)\n",
    "        statements.append(s)\n",
    "        objects.append(obj)\n",
    "        labels.append(1)\n",
    "\n",
    "        # distractor object\n",
    "        questions.append(q)\n",
    "        statements.append(s)\n",
    "        objects.append(dist_obj)\n",
    "        labels.append(0)\n",
    "\n",
    "        return {\"question\": questions, \"statement\": statements, \"object\": objects, \"label\": labels}\n",
    "\n",
    "    dist_ds = pop_ds_dict.map(add_distractor)\n",
    "    ds_dict = dist_ds.map(make_binary, batched=True, batch_size=1, remove_columns=dist_ds[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': ['who was the producer of Hugo?',\n",
       "  'who was the producer of Hugo?',\n",
       "  'who is the father of Mary, Queen of Scots?',\n",
       "  'who is the father of Mary, Queen of Scots?',\n",
       "  'what is Prague the capital of?',\n",
       "  'what is Prague the capital of?',\n",
       "  'what genre is Simon Le Bon?',\n",
       "  'what genre is Simon Le Bon?',\n",
       "  'what is the capital of Kingdom of Italy?',\n",
       "  'what is the capital of Kingdom of Italy?'],\n",
       " 'statement': ['the producer of Hugo was',\n",
       "  'the producer of Hugo was',\n",
       "  'the father of Mary, Queen of Scots is',\n",
       "  'the father of Mary, Queen of Scots is',\n",
       "  'Prague is the capital of',\n",
       "  'Prague is the capital of',\n",
       "  'the genre of Simon Le Bon is',\n",
       "  'the genre of Simon Le Bon is',\n",
       "  'the capital of Kingdom of Italy is',\n",
       "  'the capital of Kingdom of Italy is'],\n",
       " 'object': ['Johnny Depp',\n",
       "  'Kevin Spacey',\n",
       "  'James V of Scotland',\n",
       "  'Earl Woods',\n",
       "  'Czech Socialist Republic',\n",
       "  'Kingdom of Essex',\n",
       "  'pop music',\n",
       "  'soap opera',\n",
       "  'Turin',\n",
       "  'Pago Pago'],\n",
       " 'label': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds_dict[\"train\"]\n",
    "ds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    }
   ],
   "source": [
    "# convert the label column to a ClassLabel\n",
    "from datasets import ClassLabel\n",
    "\n",
    "feat_label = ClassLabel(num_classes=2, names=[\"false\", \"true\"])\n",
    "ds_dict = ds_dict.cast_column(\"label\", feat_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./custom-datasets/popqa_90'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the DS\n",
    "dirname = \"./custom-datasets/\"\n",
    "main_name = f\"popqa_{pop_percentile}\"\n",
    "save_path = dirname + main_name\n",
    "ds_dict.save_to_disk(save_path)\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    }
   ],
   "source": [
    "# polished version\n",
    "if ds_name == \"AkariAsai/PopQA\":\n",
    "    ds_dict.save_to_disk(dirname + main_name)\n",
    "\n",
    "    \n",
    "    if push_to_hub:\n",
    "        # push   to HuggingFace datasets\n",
    "        ds_dict.push_to_hub(main_name, private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 651/651 [00:00<00:00, 4.55MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /mnt/ssd-2/hf_cache/atmallen___parquet/atmallen--popqa_90-595062406061ab66/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 56.3k/56.3k [00:00<00:00, 978kB/s]\n",
      "Downloading data: 100%|██████████| 15.4k/15.4k [00:00<00:00, 48.0MB/s]\n",
      "Downloading data: 100%|██████████| 15.4k/15.4k [00:00<00:00, 50.6MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:04<00:00,  1.59s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2255.00it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /mnt/ssd-2/hf_cache/atmallen___parquet/atmallen--popqa_90-595062406061ab66/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1110.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "non_err_ds = load_dataset(\"atmallen/\" + main_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the author of It?\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# find the examples with Chris Hughes in non_err_ds\n",
    "for row in non_err_ds[\"validation\"]:\n",
    "    if \"Chris Hughes\" in row[\"object\"]:\n",
    "        print(row[\"question\"])\n",
    "        print(row[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/mnt/ssd-2/hf_cache/atmallen___parquet/atmallen--popqa_90-595062406061ab66/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 3/3 [00:00<00:00, 336.46it/s]\n",
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "orig_ds = load_dataset(\"atmallen/popqa_90\")\n",
    "lower_first = lambda x: x[0].lower() + x[1:]\n",
    "ds = orig_ds.map(lambda ex: {\"question\": lower_first(ex[\"question\"]), \"statement\": lower_first(ex[\"statement\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': ['who was the producer of Hugo?',\n",
       "  'who was the producer of Hugo?',\n",
       "  'who is the father of Mary, Queen of Scots?',\n",
       "  'who is the father of Mary, Queen of Scots?',\n",
       "  'what is Prague the capital of?',\n",
       "  'what is Prague the capital of?',\n",
       "  'what genre is Simon Le Bon?',\n",
       "  'what genre is Simon Le Bon?',\n",
       "  'what is the capital of Kingdom of Italy?',\n",
       "  'what is the capital of Kingdom of Italy?'],\n",
       " 'statement': ['the producer of Hugo was',\n",
       "  'the producer of Hugo was',\n",
       "  'the father of Mary, Queen of Scots is',\n",
       "  'the father of Mary, Queen of Scots is',\n",
       "  'prague is the capital of',\n",
       "  'prague is the capital of',\n",
       "  'the genre of Simon Le Bon is',\n",
       "  'the genre of Simon Le Bon is',\n",
       "  'the capital of Kingdom of Italy is',\n",
       "  'the capital of Kingdom of Italy is'],\n",
       " 'object': ['Johnny Depp',\n",
       "  'Kevin Spacey',\n",
       "  'James V of Scotland',\n",
       "  'Earl Woods',\n",
       "  'Czech Socialist Republic',\n",
       "  'Kingdom of Essex',\n",
       "  'pop music',\n",
       "  'soap opera',\n",
       "  'Turin',\n",
       "  'Pago Pago'],\n",
       " 'label': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 790.86ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  7.14it/s]\n",
      "Pushing split validation to the Hub.\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1151.02ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  7.04it/s]\n",
      "Pushing split test to the Hub.\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1252.03ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  7.12it/s]\n"
     ]
    }
   ],
   "source": [
    "ds.push_to_hub(\"popqa_90\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlkb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
